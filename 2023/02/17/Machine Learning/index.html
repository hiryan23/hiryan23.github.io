<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>机器学习-课程笔记 | Hiryan的世界</title>
  <meta name="description" content="本笔记整理自安泰经济与管理学院2022年春季学期课程BUSS2505-02机器学习，授课人是李成璋老师。  Lecture1_IntroductionApplicationsMachine Learning is all around us Game AI Deep blue, IBM AlphaGo, Deep Mind Deepstack, CMU &amp; Facebook AI   R">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-课程笔记">
<meta property="og:url" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/index.html">
<meta property="og:site_name" content="Hiryan的世界">
<meta property="og:description" content="本笔记整理自安泰经济与管理学院2022年春季学期课程BUSS2505-02机器学习，授课人是李成璋老师。  Lecture1_IntroductionApplicationsMachine Learning is all around us Game AI Deep blue, IBM AlphaGo, Deep Mind Deepstack, CMU &amp; Facebook AI   R">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hiryan23.github.io/pic/9538373a69fb3836595f01495607834.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/bfec802ec5619a7f6b5ef92fd832d31.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/6cf69ccc0b5e18ab05c080df1515205.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/110a43955525c0ad9a62c039eb3876a.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/dce728e8f2d0819688a148ebaf686f9.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/11af398aea2369112afe0fe46f5d255.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/379ac1b116f3c5612b0e6a1f3df25ea.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/b885ffb38cf90c077ad79abb079e005.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/57c1cdf60aceade4f79afcdcf5781c5.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/b9432d6edc99c82146c5ca340b6fd65.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/8c43075b2094ec2db8c526f1ea28d14.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/a99b5dbe4e510c276edb458034cea64.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/f70b694bdd89dce57b1ed3575e59c22.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/8918735b6e8c752e2d4ae0af01c4e03.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/7ba8a7a66050778027f6549fa41cb2a.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/10adcb662406252426c360458bf636b.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/8181efdc993f5c23481a08ae4ac524b.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/e4f39df5e0cf02295ed0adf94236c82.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/4010e1c4642202ac977ba16ffc9a0ee.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/b0f3730daa3d994524a12413137e863.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/1a58a26150be5c680563a0eb25d7b8a.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/30dc5adbbd599316a0cb17da5726c8c.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/f4eec853b3f60729c5dd34dc7b4acc9.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/0ad30f160f2e7ecab62f7354a1c4616.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/a28e0bebc2515f58f224263e5cbedf5.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/dd55e1c530692fa6810d9b314c15f84.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/6c170feb2750e49af1c23e63965b608.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/132f7b5bbcbb9caecf14d6a7f6bf2f0.png">
<meta property="og:image" content="http://hiryan23.github.io/8fce1beb900d9682237c079542b44f5.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/5192779dc145eba035b997cf1a76fcc.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/876fcd4c99289d7d14c2494c352de58.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/4cda0b999d6f3752153a5f48f5b45fa.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/bfd5b6d0bc87a6d76c69c391e921870.png">
<meta property="article:published_time" content="2023-02-17T15:12:03.271Z">
<meta property="article:modified_time" content="2023-03-11T08:51:49.911Z">
<meta property="article:author" content="Ruiyang He">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hiryan23.github.io/pic/9538373a69fb3836595f01495607834.png">
  <!-- Canonical links -->
  <link rel="canonical" href="http://hiryan23.github.io/2023/02/17/Machine%20Learning/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Hiryan的世界" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 6.3.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Ruiyang He</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">上海交通大学 本科生</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shanghai, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">友链</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">关于</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/hiryan23" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>你永远不该评判自己不了解的事物。</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E5%8D%9A%E5%BC%88%E8%AE%BA/">2022S博弈论</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E5%8F%91%E5%B1%95%E7%BB%8F%E6%B5%8E%E5%AD%A6/">2022S发展经济学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">2022S机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/">2022S计量经济学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E8%B4%A2%E5%8A%A1%E7%AE%A1%E7%90%86/">2022S财务管理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E8%B4%A7%E5%B8%81%E9%87%91%E8%9E%8D%E5%AD%A6/">2022S货币金融学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E9%87%91%E8%9E%8D%E5%AD%A6%E5%8E%9F%E7%90%86/">2022S金融学原理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2023S%E9%A9%AC%E5%85%8B%E6%80%9D%E4%B8%BB%E4%B9%89%E5%8E%9F%E7%90%86/">2023S马克思主义原理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0/">分享文章</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%97%A5%E8%AE%B0%E3%80%81%E6%9D%82%E6%84%9F%E4%B8%8E%E8%87%AA%E6%88%91%E6%89%B9%E5%88%A4/">日记、杂感与自我批判</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82/">杂</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E2%80%9C%E6%84%8F%E8%AF%86%E6%B5%81%E2%80%9D/" rel="tag">“意识流”</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E5%B1%95%E7%BB%8F%E6%B5%8E%E5%AD%A6/" rel="tag">发展经济学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E7%8E%B0%E4%BB%A3%E4%B8%BB%E4%B9%89%E5%93%B2%E5%AD%A6/" rel="tag">后现代主义哲学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%8F%E8%A7%82%E9%87%91%E8%9E%8D%E5%AD%A6/" rel="tag">宏观金融学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B0%8F%E8%AF%B4-%E6%95%A3%E6%96%87%E7%AD%89/" rel="tag">小说\散文等</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/" rel="tag">微观经济学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BE%AE%E8%A7%82%E9%87%91%E8%9E%8D%E5%AD%A6/" rel="tag">微观金融学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%BD%E8%BD%A6%E6%8E%A7%E5%88%B6/" rel="tag">汽车控制</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A4%BE%E4%BC%9A%E5%88%86%E6%9E%90/" rel="tag">社会分析</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/" rel="tag">计量经济学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%87%91%E8%9E%8D%E5%AD%A6/" rel="tag">金融学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A9%AC%E5%8E%9F/" rel="tag">马原</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/%E2%80%9C%E6%84%8F%E8%AF%86%E6%B5%81%E2%80%9D/" style="font-size: 13px;">“意识流”</a> <a href="/tags/%E5%8F%91%E5%B1%95%E7%BB%8F%E6%B5%8E%E5%AD%A6/" style="font-size: 13px;">发展经济学</a> <a href="/tags/%E5%90%8E%E7%8E%B0%E4%BB%A3%E4%B8%BB%E4%B9%89%E5%93%B2%E5%AD%A6/" style="font-size: 13px;">后现代主义哲学</a> <a href="/tags/%E5%AE%8F%E8%A7%82%E9%87%91%E8%9E%8D%E5%AD%A6/" style="font-size: 13px;">宏观金融学</a> <a href="/tags/%E5%B0%8F%E8%AF%B4-%E6%95%A3%E6%96%87%E7%AD%89/" style="font-size: 13px;">小说\散文等</a> <a href="/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/" style="font-size: 13px;">微观经济学</a> <a href="/tags/%E5%BE%AE%E8%A7%82%E9%87%91%E8%9E%8D%E5%AD%A6/" style="font-size: 13px;">微观金融学</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 13px;">机器学习</a> <a href="/tags/%E6%B1%BD%E8%BD%A6%E6%8E%A7%E5%88%B6/" style="font-size: 13px;">汽车控制</a> <a href="/tags/%E7%A4%BE%E4%BC%9A%E5%88%86%E6%9E%90/" style="font-size: 13px;">社会分析</a> <a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 14px;">笔记</a> <a href="/tags/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/" style="font-size: 13px;">计量经济学</a> <a href="/tags/%E9%87%91%E8%9E%8D%E5%AD%A6/" style="font-size: 13px;">金融学</a> <a href="/tags/%E9%A9%AC%E5%8E%9F/" style="font-size: 13px;">马原</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a><span class="archive-list-count">13</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0/">分享文章</a>
              </p>
              <p class="item-title">
                <a href="/2023/02/28/%E6%9C%80%E5%90%8E%E4%B8%80%E5%90%8D/" class="title">最后一名 | 马塞尔·埃梅</a>
              </p>
              <p class="item-date">
                <time datetime="2023-02-28T13:04:05.126Z" itemprop="datePublished">2023-02-28</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/2023S%E9%A9%AC%E5%85%8B%E6%80%9D%E4%B8%BB%E4%B9%89%E5%8E%9F%E7%90%86/">2023S马克思主义原理</a>
              </p>
              <p class="item-title">
                <a href="/2023/02/27/%E9%A9%AC%E5%85%8B%E6%80%9D%E4%B8%BB%E4%B9%89%E5%8E%9F%E7%90%86/" class="title">马克思主义原理-重点梳理</a>
              </p>
              <p class="item-date">
                <time datetime="2023-02-27T06:20:15.125Z" itemprop="datePublished">2023-02-27</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%9D%82/">杂</a>
              </p>
              <p class="item-title">
                <a href="/2023/02/22/%E4%B8%80%E9%81%93can%E6%8A%A5%E6%96%87%E8%AF%BB%E5%8F%96%E7%9A%84%E9%97%AE%E9%A2%98/" class="title">一道can报文读取的问题</a>
              </p>
              <p class="item-date">
                <time datetime="2023-02-22T05:44:19.256Z" itemprop="datePublished">2023-02-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%97%A5%E8%AE%B0%E3%80%81%E6%9D%82%E6%84%9F%E4%B8%8E%E8%87%AA%E6%88%91%E6%89%B9%E5%88%A4/">日记、杂感与自我批判</a>
              </p>
              <p class="item-title">
                <a href="/2023/02/22/2023.02.22/" class="title">2023.02.22</a>
              </p>
              <p class="item-date">
                <time datetime="2023-02-22T00:14:36.308Z" itemprop="datePublished">2023-02-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0/">分享文章</a>
              </p>
              <p class="item-title">
                <a href="/2023/02/19/%E5%AF%B9chatgpt%E7%9A%84%E6%AC%A2%E5%91%BC%E6%98%AF%E4%BA%BA%E5%AF%B9%E5%BC%82%E5%8C%96%E7%9A%84%E5%8F%8D%E6%8A%97%E2%80%94%E2%80%94%E8%AE%BAai%E4%BB%A3%E5%86%99%E8%AE%BA%E6%96%87%E7%9A%84%E8%A7%A3%E6%94%BE%E6%80%A7%E4%B8%8E%E5%BC%80%E6%94%BE%E6%80%A7/" class="title">对chatgpt的欢呼是人对异化的反抗——论ai代写论文的解放性与开放性</a>
              </p>
              <p class="item-date">
                <time datetime="2023-02-19T12:27:55.196Z" itemprop="datePublished">2023-02-19</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
  <aside class="sidebar sidebar-toc   in  " id="collapseToc" itemscope
    itemtype="http://schema.org/WPSideBar">
    <div class="slimContent">
      <nav id="toc" class="article-toc">
        <h3 class="toc-title">
          文章目录
        </h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture1-Introduction"><span class="toc-text">Lecture1_Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Applications"><span class="toc-text">Applications</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Machine-Learning-is-all-around-us"><span class="toc-text">Machine Learning is all around us</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Economical-impact-of-Machine-Learning"><span class="toc-text">Economical impact of Machine Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Trends-of-Machine-Learning"><span class="toc-text">Trends of Machine Learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition"><span class="toc-text">Definition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#What-is-Machine-Learning"><span class="toc-text">What is Machine Learning?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Programming-vs-Machine-Learning"><span class="toc-text">Programming vs Machine Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Brief-History-of-Machine-Learning"><span class="toc-text">Brief History of Machine Learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Categories"><span class="toc-text">Categories</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Supervised-Learning"><span class="toc-text">Supervised Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Process-of-Supervised-Learning"><span class="toc-text">Process of Supervised Learning</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Unsupervised-Learning"><span class="toc-text">Unsupervised Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Some-Machine-Learning-algorithms"><span class="toc-text">Some Machine Learning algorithms</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture2-MathForML"><span class="toc-text">Lecture2_MathForML</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Notations"><span class="toc-text">Notations</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Probability-amp-Statistics"><span class="toc-text">Probability &amp; Statistics</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sample-space-Omega"><span class="toc-text">Sample space($\Omega$)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Event-E"><span class="toc-text">Event($E$)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Event-space-mathcal-F"><span class="toc-text">Event space($\mathcal F$)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Axioms-of-probability"><span class="toc-text">Axioms of probability</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Random-variable-RV"><span class="toc-text">Random variable (RV)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Probability-distribution"><span class="toc-text">Probability distribution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Joint-distribution"><span class="toc-text">Joint distribution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conditional-probability"><span class="toc-text">Conditional probability</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Product-rule"><span class="toc-text">Product rule</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bayes%E2%80%99-rule"><span class="toc-text">Bayes’ rule</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Marginal-probability"><span class="toc-text">Marginal probability</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Independence"><span class="toc-text">Independence</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Expectation"><span class="toc-text">Expectation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conditional-expectation"><span class="toc-text">Conditional expectation</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Law-of-total-expectation"><span class="toc-text">Law of total expectation</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Variance"><span class="toc-text">Variance</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Covariance"><span class="toc-text">Covariance</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Estimation-of-Parameters"><span class="toc-text">Estimation of Parameters</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Maximum-Likelihood-Estimation-MLE"><span class="toc-text">Maximum Likelihood Estimation (MLE)</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Example-of-MLE"><span class="toc-text">Example of MLE</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Maximum-A-Posteriori-Estimation-MAP"><span class="toc-text">Maximum A Posteriori Estimation (MAP)</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Example-of-MAP"><span class="toc-text">Example of MAP</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-Algebra"><span class="toc-text">Linear Algebra</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Vector"><span class="toc-text">Vector</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Matrix"><span class="toc-text">Matrix</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Vector-algorithmic"><span class="toc-text">Vector algorithmic</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Vector-norm"><span class="toc-text">Vector norm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Matrix-arithmetric"><span class="toc-text">Matrix arithmetric</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transpose"><span class="toc-text">Transpose</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Symmetric-matrix"><span class="toc-text">Symmetric matrix</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inverse-of-a-matrix"><span class="toc-text">Inverse of a matrix</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Semidefinite-matrices"><span class="toc-text">Semidefinite matrices</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Back-to-Probability-amp-Statistics"><span class="toc-text">Back to Probability &amp; Statistics</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Random-vector"><span class="toc-text">Random vector</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Covariance-matrix"><span class="toc-text">Covariance matrix</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Matrix-calculus"><span class="toc-text">Matrix calculus</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimization"><span class="toc-text">Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#What-is-optimization"><span class="toc-text">What is optimization?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Local-minima-and-global-minima"><span class="toc-text">Local minima and global minima</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Convex-set"><span class="toc-text">Convex set</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Convex-Concave-function"><span class="toc-text">Convex (Concave) function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#First-order-convexity-condition"><span class="toc-text">First-order convexity condition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Second-order-convexity-condition"><span class="toc-text">Second-order convexity condition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Examples-of-convex-functions"><span class="toc-text">Examples of convex functions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Convex-optimization-problem"><span class="toc-text">Convex optimization problem</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#It%E2%80%99s-nice-to-be-convex"><span class="toc-text">It’s nice to be convex!</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Optimization-methods"><span class="toc-text">Optimization methods</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture3-BasicInML"><span class="toc-text">Lecture3_BasicInML</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Generalization-ability-%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="toc-text">Generalization ability (泛化能力)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Underfitting-and-overfitting"><span class="toc-text">Underfitting and overfitting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Basic-terms"><span class="toc-text">Basic terms</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Loss-function"><span class="toc-text">Loss function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Generalization-error"><span class="toc-text">Generalization error</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Generalization-error-%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-text">Generalization error (泛化误差)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Test-error-%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE"><span class="toc-text">Test error (测试误差)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Training-error-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE"><span class="toc-text">Training error (训练误差)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Learning-objective"><span class="toc-text">Learning objective</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Training-error-vs-test-error"><span class="toc-text">Training error vs test error</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Performance-metrics"><span class="toc-text">Performance metrics</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Classification"><span class="toc-text">Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Performance-metrics-for-classification"><span class="toc-text">Performance metrics for classification</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Confusion-matrix"><span class="toc-text">Confusion matrix</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Accuracy"><span class="toc-text">Accuracy</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Limitation-of-Accuracy"><span class="toc-text">Limitation of Accuracy</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Precision-%E6%9F%A5%E5%87%86%E7%8E%87"><span class="toc-text">Precision(查准率)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Recall-%E6%9F%A5%E5%85%A8%E7%8E%87"><span class="toc-text">Recall(查全率)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#F-1-score"><span class="toc-text">$F_1$ score</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Regression"><span class="toc-text">Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Performance-metrics-for-regression"><span class="toc-text">Performance metrics for regression</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bias-variance-decomposition"><span class="toc-text">Bias-variance decomposition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-selection"><span class="toc-text">Model selection</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Principle-of-Occam%E2%80%99s-razor-%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80"><span class="toc-text">Principle of Occam’s razor (奥卡姆剃刀)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Regularization"><span class="toc-text">Regularization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hyperparameter-%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-text">Hyperparameter(超参数)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Validation-strategy"><span class="toc-text">Validation strategy</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hold-out-validation"><span class="toc-text">Hold-out validation</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Cross-validation"><span class="toc-text">Cross validation</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Leave-one-out-cross-validation"><span class="toc-text">Leave-one-out cross validation</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Which-validation-strategy-to-use"><span class="toc-text">Which validation strategy to use?</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture4-LinearModels"><span class="toc-text">Lecture4_LinearModels</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-Regression"><span class="toc-text">Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Motivating-example"><span class="toc-text">Motivating example</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Simple-linear-regression"><span class="toc-text">Simple linear regression</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Estimating-the-parameters"><span class="toc-text">Estimating the parameters</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multivariate-linear-regression"><span class="toc-text">Multivariate linear regression</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Ridge-regression"><span class="toc-text">Ridge regression</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Regularized-linear-regression"><span class="toc-text">Regularized linear regression</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Regularization-1"><span class="toc-text">Regularization</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Linear-regression-Probabilistic-View"><span class="toc-text">Linear regression: Probabilistic View</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Recap-Probability-amp-Statistics"><span class="toc-text">Recap: Probability &amp; Statistics</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#The-multivariate-Gaussian-distribution"><span class="toc-text">The multivariate Gaussian distribution</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Linear-regression-Probabilistic-View-1"><span class="toc-text">Linear regression: Probabilistic View</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Ridge-regression-Probabilistic-View"><span class="toc-text">Ridge regression: Probabilistic View</span></a></li></ol></li></ol></li></ol></li></ol>
      </nav>
    </div>
  </aside>
  
<main class="main" role="main">
  <div class="content">
  <article id="post-Machine Learning" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
    
          <h1 class="article-title" itemprop="name">
            机器学习-课程笔记
          </h1>
          
            
      
      <div class="article-meta">
        <span class="article-date">
	<i class="icon icon-calendar"></i>
	<a href="/2023/02/17/Machine%20Learning/" class="article-date">
		<time datetime="2023-02-17T15:12:03.271Z" itemprop="datePublished">
			2023-02-17
		</time>
	</a>
</span>

<span class="article-date">
	<i class="icon icon-calendar-check"></i>
	<a href="/2023/02/17/Machine%20Learning/" class="article-date">
		<time datetime="2023-03-11T08:51:49.911Z" itemprop="dateUpdated">
			2023-03-11
		</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/2022S%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">2022S机器学习</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a>, <a class="article-tag-link-link" href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a>
  </span>


        
	<span class="article-read hidden-xs">
		<i class="icon icon-eye-fill" aria-hidden="true"></i>
		<span id="busuanzi_container_page_pv">
			<span id="busuanzi_value_page_pv"></span>
		</span>
	</span>

	
		
        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2023/02/17/Machine%20Learning/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <blockquote>
<p><em>本笔记整理自安泰经济与管理学院2022年春季学期课程BUSS2505-02机器学习，授课人是李成璋老师。</em></p>
</blockquote>
<h2 id="Lecture1-Introduction"><a href="#Lecture1-Introduction" class="headerlink" title="Lecture1_Introduction"></a>Lecture1_Introduction</h2><h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><h4 id="Machine-Learning-is-all-around-us"><a href="#Machine-Learning-is-all-around-us" class="headerlink" title="Machine Learning is all around us"></a>Machine Learning is all around us</h4><ul>
<li>Game AI<ul>
<li>Deep blue, IBM</li>
<li>AlphaGo, Deep Mind</li>
<li>Deepstack, CMU &amp; Facebook AI</li>
</ul>
</li>
<li>Robot<ul>
<li>SpotMini, BostonDynamics</li>
<li>Big Dog</li>
</ul>
</li>
<li>Image recognition</li>
<li>Self-driving car</li>
<li>Medical Diagnosis</li>
<li>Applications of ML in business settings</li>
<li>Customer segmentation</li>
<li>Applications in Finance</li>
<li>Credit lending &amp; Fraud detection</li>
<li>Personalized recommendation</li>
<li>Dynamic pricing<ul>
<li>Rue La La</li>
</ul>
</li>
<li>Order dispatch for ride-sharing platforms<ul>
<li>DiDi</li>
</ul>
</li>
</ul>
<h4 id="Economical-impact-of-Machine-Learning"><a href="#Economical-impact-of-Machine-Learning" class="headerlink" title="Economical impact of Machine Learning"></a>Economical impact of Machine Learning</h4><ul>
<li>By 2035, AI could double annual global economic growth rates (Accenture)</li>
<li>Global GDP may increase by up to 14% (the equivalent of US$15.7 trillion) by 2030 as a result of the accelerating development and take-up of AI (PwC)</li>
</ul>
<h4 id="Trends-of-Machine-Learning"><a href="#Trends-of-Machine-Learning" class="headerlink" title="Trends of Machine Learning"></a>Trends of Machine Learning</h4><p><img src="/pic/9538373a69fb3836595f01495607834.png" alt="Trends of ML"></p>
<h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><h4 id="What-is-Machine-Learning"><a href="#What-is-Machine-Learning" class="headerlink" title="What is Machine Learning?"></a>What is Machine Learning?</h4><ul>
<li>Field of study that gives computers the ability to learn without being explicitly programmed. - Arthur Samuel, 1959</li>
<li>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. - Tom Mitchell, 1997</li>
</ul>
<h4 id="Programming-vs-Machine-Learning"><a href="#Programming-vs-Machine-Learning" class="headerlink" title="Programming vs Machine Learning"></a>Programming vs Machine Learning</h4><p><img src="/pic/bfec802ec5619a7f6b5ef92fd832d31.png" alt="Programming vs ML"></p>
<h4 id="Brief-History-of-Machine-Learning"><a href="#Brief-History-of-Machine-Learning" class="headerlink" title="Brief History of Machine Learning"></a>Brief History of Machine Learning</h4><ol>
<li>Connectionism, 1950s<ul>
<li>Perception, F. Rosenblatt</li>
</ul>
</li>
<li>Checker game, Arthur Samuel, 1959<ul>
<li>The term Machine Learning is coined.</li>
</ul>
</li>
<li>Symbolism, 1970-1980s<ul>
<li>Decision tree</li>
<li>ID3, Quinlan</li>
<li>Classification and Regression Tree (CART)</li>
</ul>
</li>
<li>Connectionism, 1980-1990s<ul>
<li>Back-Propagation</li>
<li>for Multi-layer Neural Networks</li>
</ul>
</li>
<li>Statistical Learning, 1990s<ul>
<li>Support vector machine</li>
<li>Kernel methods</li>
</ul>
</li>
<li>Connectionism, 2000s<ul>
<li>Deep Learning</li>
</ul>
</li>
</ol>
<h3 id="Categories"><a href="#Categories" class="headerlink" title="Categories"></a>Categories</h3><p>Categories of Machine Learning<br><img src="/pic/6cf69ccc0b5e18ab05c080df1515205.png" alt="Categories of ML"></p>
<h4 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h4><p><img src="/pic/110a43955525c0ad9a62c039eb3876a.png" alt="SL"></p>
<ul>
<li>Aims to <strong>predict on unknown data</strong> using models trained by labeled data</li>
<li>Learning a function that maps the <strong>feature (attribute)</strong> to <strong>label (response)</strong></li>
<li>Classification vs Regression<ul>
<li>Both utilize the training set (known data) to make predictions</li>
<li>The output of classification is categorical (<strong>discrete</strong>) while the output of regression is numerical (<strong>continuous</strong>)</li>
</ul>
</li>
</ul>
<h5 id="Process-of-Supervised-Learning"><a href="#Process-of-Supervised-Learning" class="headerlink" title="Process of Supervised Learning"></a>Process of Supervised Learning</h5><ol>
<li>Split data into training &amp; test sets</li>
<li>Train a model</li>
<li>Make predictions on testing set</li>
<li>Compare predicted and true labels</li>
</ol>
<h4 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h4><p><img src="/pic/dce728e8f2d0819688a148ebaf686f9.png" alt="UL"></p>
<p>Discover the structure and pattern within the unlabeled data.</p>
<ul>
<li>Market Segmentation</li>
<li>Social Network Analysis</li>
</ul>
<h4 id="Some-Machine-Learning-algorithms"><a href="#Some-Machine-Learning-algorithms" class="headerlink" title="Some Machine Learning algorithms"></a>Some Machine Learning algorithms</h4><p><img src="/pic/11af398aea2369112afe0fe46f5d255.png" alt="ML algorithms"></p>
<h2 id="Lecture2-MathForML"><a href="#Lecture2-MathForML" class="headerlink" title="Lecture2_MathForML"></a>Lecture2_MathForML</h2><h4 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h4><ul>
<li>$a \in A$ : $a$ is a member of set A</li>
<li>$||\pmb{v}||$ : the norm of vector $\pmb{v}$</li>
<li>$\pmb{x},\pmb{y},\pmb{z}$ : vector (lower case, bold)</li>
<li>$\pmb{A},\pmb{B}$ : matrix (upper case, bold)</li>
<li>$X$ : random variable (upper case)</li>
<li>$x$ : realizaton of random variable (lower case)</li>
<li>$y&#x3D; f(\pmb{x})$ : function with muitiple inputs</li>
</ul>
<h3 id="Probability-amp-Statistics"><a href="#Probability-amp-Statistics" class="headerlink" title="Probability &amp; Statistics"></a>Probability &amp; Statistics</h3><h4 id="Sample-space-Omega"><a href="#Sample-space-Omega" class="headerlink" title="Sample space($\Omega$)"></a>Sample space($\Omega$)</h4><p>Set of all possible outcomes of an experiment</p>
<h4 id="Event-E"><a href="#Event-E" class="headerlink" title="Event($E$)"></a>Event($E$)</h4><p>Any subset of outcomes contained in the sample space</p>
<h4 id="Event-space-mathcal-F"><a href="#Event-space-mathcal-F" class="headerlink" title="Event space($\mathcal F$)"></a>Event space($\mathcal F$)</h4><p>The set of all possible events</p>
<h4 id="Axioms-of-probability"><a href="#Axioms-of-probability" class="headerlink" title="Axioms of probability"></a>Axioms of probability</h4><p>The <em>probabililty distribution</em> P is a function that satisfies the following</p>
<ol>
<li>$0 \leq P(E) \leq 1$ for any $E \in \mathcal F$ (Non-negativity)</li>
<li>$P(\Omega)&#x3D;1$ </li>
<li>$P(E_1 \cup E_2)&#x3D;P(E_1)+P(E_2)$ if $E_1$ and $E_2$ mutually exclusive events (Additivity)</li>
</ol>
<h4 id="Random-variable-RV"><a href="#Random-variable-RV" class="headerlink" title="Random variable (RV)"></a>Random variable (RV)</h4><p>mapping from sample space to real numbers</p>
<ul>
<li>Probability distribution specifies the probability of observing every possible value of a random variable</li>
<li>Discrete RV has a countable set of possible values: Bernoulli, Poisson, …</li>
<li>Continuous RV can take infinitely many possible values: Uniform, Normal, Exponential, …</li>
</ul>
<h4 id="Probability-distribution"><a href="#Probability-distribution" class="headerlink" title="Probability distribution"></a>Probability distribution</h4><p>Cumulative distribution function (CDF)<br>$$ F_X(x)&#x3D;P(X \leq x)$$<br>Discrete random variable: probability mass function $p_X(x)$<br>$$p_X(x)&#x3D;P(X&#x3D;x)$$<br>Continuous random variable: probability density function $f_X(x)$<br>$$f_X(x) &#x3D; \cfrac{\text{d}F_X(x)}{\text{d}x}$$</p>
<h4 id="Joint-distribution"><a href="#Joint-distribution" class="headerlink" title="Joint distribution"></a>Joint distribution</h4><p>Consider two random variables $X$ and $Y$ , the <em>joint cumulative distribution function</em> is defined as<br>$$F_{XY}(x,y)&#x3D;P(X \leq x,Y \leq y)$$<br>The joint probability mass function of two discrete variables $X$ , $Y$<br>$$p_{X,Y}(x,y)&#x3D;P(X&#x3D;x,Y&#x3D;y)$$<br>The joint probability density function of two continuous variables $X$ , $Y$<br>$$f_{X,Y}(x,y)&#x3D;\cfrac{\partial^2 F_{XY}(x,y)}{\partial x \partial y}$$</p>
<h4 id="Conditional-probability"><a href="#Conditional-probability" class="headerlink" title="Conditional probability"></a>Conditional probability</h4><p>The conditional probability of $X$ given $Y&#x3D;y$ is defined as,<br>$$P(X&#x3D;x|Y&#x3D;y)&#x3D;\cfrac{P(X&#x3D;x,Y&#x3D;y)}{P(Y&#x3D;y)}$$</p>
<h4 id="Product-rule"><a href="#Product-rule" class="headerlink" title="Product rule"></a>Product rule</h4><p>$$P(X&#x3D;x,Y&#x3D;y)&#x3D;P(X&#x3D;x|Y&#x3D;y)P(Y&#x3D;y)&#x3D;P(Y&#x3D;y|X&#x3D;x)P(X&#x3D;x)$$</p>
<h4 id="Bayes’-rule"><a href="#Bayes’-rule" class="headerlink" title="Bayes’ rule"></a>Bayes’ rule</h4><p>$$P(Y&#x3D;y|X&#x3D;x)&#x3D;\cfrac{P(X&#x3D;x|Y&#x3D;y)\cdot P(Y&#x3D;y)}{P(X&#x3D;x)}$$</p>
<ul>
<li>Application: SPAM email case<ul>
<li>y: labels (SPAM&#x2F;normal)</li>
<li>x: frequency of keywords</li>
</ul>
</li>
</ul>
<p>	</p>
<h4 id="Marginal-probability"><a href="#Marginal-probability" class="headerlink" title="Marginal probability"></a>Marginal probability</h4><p>The probability of event that will occur regardless of conditional events<br>$$<br>\begin{align}<br>P(X&#x3D;x) &amp;&#x3D; \sum_{y \in \mathcal{y}}P(X&#x3D;x,Y&#x3D;y) \<br>       &amp;&#x3D; \sum_{y \in \mathcal{y}}P(X&#x3D;x|Y&#x3D;y)P(Y&#x3D;y)<br>\end{align}<br>$$</p>
<h4 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h4><p>Consider two events $A$ and $B$ , they are <em>independent</em>  if<br>$$<br>P(X&#x3D;x,Y&#x3D;y)&#x3D;P(X&#x3D;x) \cdot P(Y&#x3D;y)<br>$$<br>In addition, (if they are independent: )<br>$$<br>P(X&#x3D;x)&#x3D; \cfrac{P(X&#x3D;x,Y&#x3D;y)}{P(Y&#x3D;y)}&#x3D;P(X&#x3D;x | Y&#x3D;y)<br>$$</p>
<h4 id="Expectation"><a href="#Expectation" class="headerlink" title="Expectation"></a>Expectation</h4><p>Expectation (expected value) of a random variable $X$ is computed as</p>
<ul>
<li><u>Discrete RV</u> $$ \mathbb{E} [x] &#x3D; \sum_x x \cdot p(x)$$</li>
<li><u>Continuous RV</u> $$ \mathbb{E}[X]&#x3D;\int_x x \cdot f(x) \cdot \text{d} x$$</li>
<li>Expectation of functions $$\mathbb{E}[h(X)] &#x3D; \int_x h(x) \cdot  f(x) \cdot \text{d}x$$</li>
<li>Other properties: $$\begin{align}<br>\mathbb{E}[aX+b]&amp;&#x3D;a\mathbb{E}[X]+b \<br>\mathbb{E}[X+Y]&amp;&#x3D;\mathbb{E}[X]+\mathbb{E}[Y]\<br>\mathbb E[XY]&amp;&#x3D;\mathbb E[X]\mathbb E[Y],\ \text{if\ X\ and\ Y\ are uncoorrelated}<br>\end{align}$$</li>
</ul>
<h4 id="Conditional-expectation"><a href="#Conditional-expectation" class="headerlink" title="Conditional expectation"></a>Conditional expectation</h4><p>The conditional expectation of $X$ with respect to $Y$ is the function<br>$$\mathbb{E}[X|Y&#x3D;y]$$<br>Discrete random variable<br>$$\mathbb{E}[X|Y&#x3D;y]&#x3D; \sum_x x\cdot P(X&#x3D;x|Y&#x3D;y)$$<br>Continuous random variable<br>$$ \mathbb{E}[X|Y&#x3D;y]&#x3D;\int_x x f_{X|Y}(x|y) \text{d}x$$</p>
<h5 id="Law-of-total-expectation"><a href="#Law-of-total-expectation" class="headerlink" title="Law of total expectation"></a>Law of total expectation</h5><p>$$\mathbb{E}[X] &#x3D; \mathbb{E}[\mathbb{E}[X|Y]]$$</p>
<h4 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h4><p>The squared deviation of $X$ from its mean<br>$$ \text{Var}[X]&#x3D;\mathbb{E}[(X-\mathbb{E}[X])^2]&#x3D;\mathbb{E}[X^2]&#x3D;\mathbb{E}[X]^2$$</p>
<ul>
<li>Standard variation $$ \sigma &#x3D; \sqrt{\text{Var}[X]}$$</li>
<li>Other properties $$\begin{align}<br>\text{Var}[aX+b]&amp;&#x3D;a^2 \cdot \text{Var}[x] \<br>\text{Var}[X+Y]&amp;&#x3D;\text{Var}[X]+\text{Var}[Y]\ \text{if X,Y are uncorrelated}<br>\end{align}$$</li>
</ul>
<h4 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h4><p>$$ \text{Cov}(X_1,X_2)&#x3D;\mathbb{E}[(X_1-\mathbb{E}[X_1])(X_2-\mathbb{E}[X_2])]$$</p>
<h4 id="Estimation-of-Parameters"><a href="#Estimation-of-Parameters" class="headerlink" title="Estimation of Parameters"></a>Estimation of Parameters</h4><ul>
<li>Suppose we have random variables $X_1, X_2, \cdots, X_n$ and corresponding observations $x_1, x_2, \cdots, x_n$</li>
<li>We select a parametric model and fit the parameters of the model to the data.</li>
<li>How do we choose the values of the parameters $\theta$ ?</li>
</ul>
<h5 id="Maximum-Likelihood-Estimation-MLE"><a href="#Maximum-Likelihood-Estimation-MLE" class="headerlink" title="Maximum Likelihood Estimation (MLE)"></a>Maximum Likelihood Estimation (MLE)</h5><p>Which $\theta$ makes the observations $x_1,x_2,\cdots,x_n$ most likely?</p>
<ul>
<li>Maximize the likelihood of the observed data $$ \hat{\theta}_{MLE}&#x3D;\text{arg}\max_\theta \mathcal{L}(\theta)&#x3D;\text{arg}\max_\theta p(x_1,x_2,\cdots,x_n|\theta)$$</li>
<li>Assume that $x_1,x_2, \cdots,x_n$ are i.i.d., we have $$\mathcal{L}(\theta)&#x3D;\prod_{i&#x3D;1}^n p(x_i|\theta)$$</li>
<li>Take the logarithmic on both sides, we obtain the log-likelihood $$\log \mathcal{L}(\theta)&#x3D; \sum_{i&#x3D;1}^n \log p(x_i|\theta)$$</li>
</ul>
<h6 id="Example-of-MLE"><a href="#Example-of-MLE" class="headerlink" title="Example of MLE"></a>Example of MLE</h6><ul>
<li>Imagine a bowl contains a large number of red and white balls. The proportion of the red balls, denoted by $\theta$ , is unknown.</li>
<li>Now we sample balls from this bowl with replacement for $n$ times and observe $x$ red balls out of $n$ balls.</li>
<li>Likelihood function:$$\begin{align}<br>L(\theta)&#x3D;L(x;\theta)&#x3D;\binom{n}{x}\theta^x(1-\theta)^{n-x} \<br>\log L(\theta) &#x3D; x\log \theta+(n-x)\log(1-\theta) \<br>\cfrac{\partial \log L(\theta)}{\partial \theta} &#x3D; 0 \Rightarrow \hat\theta_{MLE} &#x3D; \cfrac{x}{n}<br>\end{align}$$</li>
</ul>
<h5 id="Maximum-A-Posteriori-Estimation-MAP"><a href="#Maximum-A-Posteriori-Estimation-MAP" class="headerlink" title="Maximum A Posteriori Estimation (MAP)"></a>Maximum A Posteriori Estimation (MAP)</h5><p>Which $\theta$ maximizes the posterior $p(\theta | x_1,x_2,\cdots,x_n)$ given the prior $p(\theta)$ ?</p>
<ul>
<li>We assume that the parameter is a random variable, and we specify a prior distribution $p(\theta)$</li>
<li>By Bayes’ rule, we compute the posterior of the parameter $$p(\theta | x_1,x_2,\cdots,x_n) \propto p(\theta)p(x_1,x_2,\cdots,x_n|\theta)$$</li>
<li>Estimate parameter $\theta$ by maximizing the posterior $$\hat\theta_{MAP}&#x3D;\text{arg}\max_\theta p(\theta)p(x_1,x_2,\cdots,x_n|\theta)$$</li>
<li>We take the logarithmic of the posterior, $$\hat\theta_{MAP}&#x3D;\text{arg}\max_\theta \log p(\theta)+\sum_{i&#x3D;1}^n \log p(x_i|\theta)$$<br><em>: MAP: balance MLE and prior knowledge</em></li>
</ul>
<h6 id="Example-of-MAP"><a href="#Example-of-MAP" class="headerlink" title="Example of MAP"></a>Example of MAP</h6><ul>
<li>Imagine a bowl contains a large number of red and white balls. The proportion of the red balls, denoted by $\theta$ , is unknown, but with a Beta prior, $P(\theta) &#x3D;\cfrac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}$</li>
<li>Now we sample balls from this bowl with replacement for $n$ times and observe $x$ red balls out of $n$ balls.</li>
<li>The posterior function: $$\begin{align}<br>p(x|\theta)p(\theta) &amp;&#x3D; \binom{n}{x}\theta^x(1-\theta)^{n-x}\cfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1} \<br>\log p(x|\theta)p(\theta) &amp;&#x3D; (x+\alpha-1)\log \theta +(n-x+\beta -1)\log(1-\theta) \<br>\hat\theta_{MAP}&amp;&#x3D;\cfrac{x+\alpha-1}{n+\alpha+\beta-2}<br>\end{align}$$</li>
</ul>
<h3 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a>Linear Algebra</h3><h4 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h4><ul>
<li>A one-dimension array of $n$ values, denoted by $\pmb{x}$ (lower case, bold)</li>
<li>$x_i$ is the $i$-th element of $\pmb{x}$ $$ \pmb{x}&#x3D;(x_1,x_2,\cdots,x_n)^T&#x3D; \begin{bmatrix}x_1\ x_2\ \vdots \ x_n \end{bmatrix}$$</li>
</ul>
<h4 id="Matrix"><a href="#Matrix" class="headerlink" title="Matrix"></a>Matrix</h4><ul>
<li>A two-dimension array of $m \times n$ values, denoted by $\pmb{A}$ (upper case, bold)</li>
<li>$m$ is the number of row vector, $n$ is the number of column vectors</li>
<li>$a_{ij}$ is the entry in $i$-th row and $j$-th column $$\pmb{A}&#x3D;\begin{bmatrix} a_{11} &amp;\cdots &amp;a_{1n} \ \vdots &amp;\ddots &amp;\vdots \ a_{m1} &amp;\cdots &amp;a_{mn} \end{bmatrix} $$</li>
</ul>
<h4 id="Vector-algorithmic"><a href="#Vector-algorithmic" class="headerlink" title="Vector algorithmic"></a>Vector algorithmic</h4><ul>
<li>Scalar multiplication of a vector $$ \pmb{y}&#x3D;a\pmb{x}&#x3D;(ax_1,ax_2,\cdots,ax_n)^T$$</li>
<li>Dot product of the vectors $\pmb{x},\pmb{y} \in \mathbb{R}^n$ $$\pmb{x}^T\pmb{y} &#x3D; [x_1,x_2,\cdots, x_n]\begin{bmatrix}y_1\ y_2\ \vdots\ y_n\end{bmatrix}&#x3D;\sum_{i&#x3D;1}^n x_iy_i \in \mathbb{R}$$</li>
<li>Outer product of the vectors $\pmb{x} \in \mathbb{R}^m, \pmb{y} \in \mathbb{R}^n$ $$\pmb{x}\pmb{y}^T&#x3D;\begin{bmatrix}x_1\ x_2\ \vdots x_m\end{bmatrix}[y_1,y_2,\cdots,y_n]&#x3D;\begin{bmatrix}x_1y_1 &amp;\cdots &amp;x_1y_n\ \vdots &amp;\ddots &amp;\vdots\ x_my_1 &amp;\cdots &amp;x_my_n\end{bmatrix}$$</li>
</ul>
<h4 id="Vector-norm"><a href="#Vector-norm" class="headerlink" title="Vector norm"></a>Vector norm</h4><p>A norm $||\cdot||$ is a function that satisfies</p>
<ul>
<li>$||\pmb{x}|| \geq 0$ with equality if and only if $\pmb{x}&#x3D;\pmb{0}$</li>
<li>$||\pmb{x}+\pmb{y}|| \leq ||\pmb{x}|| + ||\pmb{y}||$</li>
<li>$||a\pmb{x}||&#x3D;|a|||\pmb{x}||$</li>
<li>$\pmb{x}^T\pmb{y}&#x3D;||\pmb{x}||_2||\pmb{y}||_2\cos(\theta)$</li>
<li>$l_1$ norm $||\pmb{x}||<em>1&#x3D;\displaystyle\sum</em>{i&#x3D;1}^n|x_i|$</li>
<li>$l_2$ norm $||\pmb{x}||<em>2&#x3D;\left(\displaystyle\sum</em>{i&#x3D;1}^n|x_i|^2\right)^{\frac{1}{2}}$</li>
</ul>
<h4 id="Matrix-arithmetric"><a href="#Matrix-arithmetric" class="headerlink" title="Matrix arithmetric"></a>Matrix arithmetric</h4><ul>
<li>Addition of matrices $\pmb{A},\pmb{B} \in \mathbb{R}^{m\times n}$ $$\pmb{C}&#x3D;\pmb{A}+\textcolor{red}{\pmb{B}}&#x3D;\begin{bmatrix} a_{11}+\textcolor{red}{b_{11}} &amp;\cdots &amp;a_{1n}+\textcolor{red}{b_{1n}}\ \vdots &amp;\ddots &amp;\vdots\ a_{m1}+\textcolor{red}{b_{m1}} &amp;\cdots &amp;a_{mn}+\textcolor{red}{b_{mn}} \end{bmatrix}$$</li>
<li>Scalar multiplication of a matrix $$\pmb{B}&#x3D;\textcolor{red}{d}\cdot \pmb{A}&#x3D;\begin{bmatrix}\textcolor{red}{d}\cdot a_{11} &amp;\cdots &amp;\textcolor{red}{d}\cdot a_{1n}\ \vdots &amp;\ddots &amp;\vdots\ \textcolor{red}{d}\cdot a_{m1} &amp;\cdots &amp;\textcolor{red}{d}\cdot a_{mn}\end{bmatrix}$$</li>
<li>Multiplication of matrices $\pmb{A} \in \mathbb{R}^{m \times n}$ and $\pmb{B} \in \mathbb{R}^{n\times p}$ $$ \pmb{A}\pmb{B}&#x3D;\pmb{C} \in \mathbb{R}^{m\times p},\ \ \ \ c_{ij}&#x3D;\sum_{k&#x3D;1}^na_{ik}b_{kj}$$</li>
<li>Matrix multiplication is <em>associative</em>: $\pmb{A}(\pmb{B}\pmb{C})&#x3D;(\pmb{AB})\pmb{C}$ </li>
<li>Matrix multiplication is <em>distributive</em>: $\pmb A (\pmb B+\pmb C)&#x3D;\pmb{AB}+\pmb{AC}$</li>
<li>Matrix multiplication is <em>NOT communicative</em>: $\pmb{AB} \neq \pmb{BA}$</li>
</ul>
<h4 id="Transpose"><a href="#Transpose" class="headerlink" title="Transpose"></a>Transpose</h4><p>Given a matrix $\pmb A \in \mathbb R^{m\times n}$ , its transpose, written by $\pmb A^T \in \mathbb R^{n\times m}$ , is given by $$(\pmb A^T)<em>{ij}&#x3D;(\pmb A)</em>{ji}$$<br>Some properties</p>
<ul>
<li>$(\pmb{AB})^T&#x3D;\pmb B^T\pmb A^T$</li>
<li>$(\pmb A^T)^T&#x3D;\pmb A$</li>
<li>$(\pmb A+\pmb B)^T&#x3D;\pmb A^T+\pmb B^T$</li>
</ul>
<h4 id="Symmetric-matrix"><a href="#Symmetric-matrix" class="headerlink" title="Symmetric matrix"></a>Symmetric matrix</h4><p>A square matrix is <em>symmetric</em> if $\pmb A &#x3D;\pmb A^T$.</p>
<h4 id="Inverse-of-a-matrix"><a href="#Inverse-of-a-matrix" class="headerlink" title="Inverse of a matrix"></a>Inverse of a matrix</h4><p>For a matrix $\pmb A\in \mathbb R^{n\times n}$ , if there exists a square matrix $\pmb B \in \mathbb R^{n\times n}$ such that $$\pmb{BA}&#x3D;\pmb{AB}&#x3D;\pmb I$$<br>where $\pmb I$ is the $n$-by-$n$ <em>identity matrix</em>, then $\pmb B$ is the <em>inverse</em> of $\pmb A$.</p>
<ul>
<li>The inverse of $\pmb A$ is denoted by $\pmb A^{-1}$ </li>
<li>A matrix is <em>invertible</em> if it is not <em>singular</em>.</li>
</ul>
<p><u>Solving a linear system</u><br>If $\pmb A$ is square nonsingular matrix, then the solution to the linear system $\pmb{AX}&#x3D;\pmb b$ is given by $$\pmb x&#x3D;\pmb A^{-1}\pmb b$$</p>
<h4 id="Semidefinite-matrices"><a href="#Semidefinite-matrices" class="headerlink" title="Semidefinite matrices"></a>Semidefinite matrices</h4><p>A symmetric matrix $\pmb A \in \mathbb S^{n\times n}$ is </p>
<ul>
<li><em>positive semidefinite</em> if $\pmb x^T \pmb{Ax} \geq 0$ for any $\pmb x \in \mathbb R^n$ and $\pmb x \neq \pmb 0$ , denoted by $\pmb A \succcurlyeq 0$ ;</li>
<li><em>positive definite</em> if $\pmb x^T \pmb{Ax} \textgreater 0$ for any $\pmb x \in \mathbb R^n$ and $\pmb x \neq 0$ , denoted by $\pmb A \succ 0$ ;</li>
<li>negative semidefinite if $-\pmb A$ is positive semidefinite;</li>
<li>negative definite if $-\pmb A$ is positive definite;</li>
<li>indefinite if it is neither positive nor negative definite.</li>
</ul>
<h4 id="Back-to-Probability-amp-Statistics"><a href="#Back-to-Probability-amp-Statistics" class="headerlink" title="Back to Probability &amp; Statistics"></a>Back to Probability &amp; Statistics</h4><h5 id="Random-vector"><a href="#Random-vector" class="headerlink" title="Random vector"></a>Random vector</h5><p>A vector of random variables $X_1,X_2,\cdots,X_n$, denoted by $\pmb X&#x3D;[X_1,X_2,\cdots,X_n]^T$ </p>
<ul>
<li>$\mathbb E[X]&#x3D;[\mathbb E[X_1],\mathbb E[X_2],\cdots,\mathbb E[X_n]]^T$</li>
</ul>
<h5 id="Covariance-matrix"><a href="#Covariance-matrix" class="headerlink" title="Covariance matrix"></a>Covariance matrix</h5><p>$$\Sigma &#x3D; \begin{bmatrix} \text{Cov}[X_1,X_1] &amp;\cdots &amp;\text{Cov}[X_1,X_n]\ \vdots &amp;\ddots &amp;\vdots\ \text{Cov}[X_n,X_1] &amp;\cdots &amp;\text{Cov}[X_n,X_n]\end{bmatrix}&#x3D;\mathbb E[(\pmb X -\mathbb E[X])(\pmb X-\mathbb E[\pmb X])^T]$$</p>
<h4 id="Matrix-calculus"><a href="#Matrix-calculus" class="headerlink" title="Matrix calculus"></a>Matrix calculus</h4><p>Consider a function $f:\mathbb R^n \rightarrow \mathbb R$ , the <em>gradient</em> of $f$ is defined as a vector of partial derivatives $$\nabla f(\pmb x)&#x3D;\begin{bmatrix}\cfrac{\partial f(\pmb x)}{\partial x_1}\ \cfrac{\partial f(\pmb x)}{\partial x_2}\ \vdots\ \cfrac{\partial f(\pmb x)}{\partial x_n}\end{bmatrix}$$<br>“direction and rate of fastest <strong>increase</strong>“</p>
<ul>
<li>The direction of fastest increase of the function</li>
<li>The magnitude is the rate of increase</li>
</ul>
<p>Consider a function $f: \mathbb R^n \rightarrow \mathbb R$, the <em>Hessian</em> of $f$ is defined as $$\nabla^2f(\pmb x)&#x3D;\begin{bmatrix}\cfrac{\partial^2f(\pmb x)}{\partial x_1^2} &amp;\cdots &amp;\cfrac{\partial^2f(\pmb x)}{\partial x_1\partial x_n}\ \vdots &amp;\ddots &amp;\vdots\ \cfrac{\partial^2f(\pmb x)}{\partial x_n\partial x_1} &amp;\cdots &amp;\cfrac{\partial^2f(\pmb x)}{\partial x_n^2}\end{bmatrix}$$</p>
<ul>
<li>Hessian is symmetric when $f$ is twice differentiable. $$\cfrac{\partial^2 f(\pmb x)}{\partial x_i\partial x_j}&#x3D;\cfrac{\partial^2f(\pmb x)}{\partial x_j\partial x_i}$$</li>
</ul>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><h4 id="What-is-optimization"><a href="#What-is-optimization" class="headerlink" title="What is optimization?"></a>What is optimization?</h4><p>Finding the minimizer of a function subject to constraints:<br>$$\begin{align}<br>\text{minimize}_{\pmb x} \ \ \ \ &amp;f(\pmb x) \<br>s.t.\ \ \ \ &amp;f_i(\pmb x)\leq 0,i&#x3D;1,2,\cdots, k;\<br>&amp;h_j(\pmb x)&#x3D;0,j&#x3D;1,2,\cdots,l.<br>\end{align}$$</p>
<ul>
<li>Mean-variance analysis</li>
<li>Transportation problems</li>
<li>Facility location</li>
<li>Linear regression</li>
<li>Logistic regression</li>
<li>Support vector machine</li>
<li>Neural networks</li>
</ul>
<h4 id="Local-minima-and-global-minima"><a href="#Local-minima-and-global-minima" class="headerlink" title="Local minima and global minima"></a>Local minima and global minima</h4><ul>
<li>Local minima is the solution that optimal within a neighboring set</li>
<li>Global minima is the optimal solution among all possible solutions<br><img src="/pic/379ac1b116f3c5612b0e6a1f3df25ea.png"></li>
</ul>
<h4 id="Convex-set"><a href="#Convex-set" class="headerlink" title="Convex set"></a>Convex set</h4><p>A set $C\in \mathbb R^n$ is <em>convex</em> if for $\pmb x,\pmb y\in C$ and any $\alpha\in [0,1]$, $$\alpha \pmb x+(1-\alpha)\pmb y \in C$$<img src="/pic/b885ffb38cf90c077ad79abb079e005.png"></p>
<p>Examples: $\mathbb R^n$ , norm ball {$\pmb x: ||\pmb x|| \leq r$} for a given $r$ , intersection of convex sets</p>
<h4 id="Convex-Concave-function"><a href="#Convex-Concave-function" class="headerlink" title="Convex (Concave) function"></a>Convex (Concave) function</h4><p>A function $f:\mathbb R^n \rightarrow \mathbb R$ is convex (concave) if for $\pmb x,\pmb y \in \text{dom}(f)$ and any $\alpha \in [0,1]$ , $$f(\alpha \pmb x+(1-\alpha)\pmb y)\leq(\geq) \alpha f(\pmb x)+(1-\alpha)f(\pmb y)$$<img src="/pic/57c1cdf60aceade4f79afcdcf5781c5.png"></p>
<h4 id="First-order-convexity-condition"><a href="#First-order-convexity-condition" class="headerlink" title="First-order convexity condition"></a>First-order convexity condition</h4><p>Suppose a function $f:\mathbb R^n \rightarrow \mathbb R$ is differentiable. Then $f$ is convex if and only if for all $\pmb x,\pmb y \in \text{dom}(f)$ $$f(\pmb y) \geq f(\pmb x) + \nabla f(\pmb x)^T(\pmb y-\pmb x)$$<img src="/pic/b9432d6edc99c82146c5ca340b6fd65.png"></p>
<h4 id="Second-order-convexity-condition"><a href="#Second-order-convexity-condition" class="headerlink" title="Second-order convexity condition"></a>Second-order convexity condition</h4><p>Suppose a function $f:\mathbb R^n \rightarrow \mathbb R$ is twice differentiable. Then $f$ is convex if and only if for all $\pmb x \in \text{dom}(f)$ $$\nabla^2f(\pmb x)\succcurlyeq 0$$<img src="/pic/8c43075b2094ec2db8c526f1ea28d14.png"></p>
<h4 id="Examples-of-convex-functions"><a href="#Examples-of-convex-functions" class="headerlink" title="Examples of convex functions"></a>Examples of convex functions</h4><ul>
<li>Exponential function: $e^{ax}$</li>
<li>Logarithmic function: $\log(x)$ is concave</li>
<li>Affine function: $\pmb a^T\pmb x+b$ is a convex and concave</li>
<li>Least square loss: $||\pmb y-\pmb{X\beta}||_2^2$</li>
<li>$f_1(x)$ is convex for $x\in \text{dom}(f_1)$ and $f_2(x)$ is convex for $x\in \text{dom}(f_2)$ , then $f_1+f_2$ is convex for $x\in \text{dom}(f_1) \cap \text{dom}(f_2)$</li>
</ul>
<h4 id="Convex-optimization-problem"><a href="#Convex-optimization-problem" class="headerlink" title="Convex optimization problem"></a>Convex optimization problem</h4><p>An optimization problem is convex if its objective is a convex function, the inequality constraints $f_j$ are convex, and the equality constraints $h_j$ are affine.<br>$$\begin{align}<br>\text{minimize}_{\pmb x} \ \ \ \ &amp;f(\pmb x)\<br>s.t. \ \ \ \ &amp;f_i(\pmb x)\leq 0,i&#x3D;1,2,\cdots,k; \<br>&amp;h_j(\pmb x)&#x3D;0,j&#x3D;1,2,\cdots,l.<br>\end{align}$$</p>
<h4 id="It’s-nice-to-be-convex"><a href="#It’s-nice-to-be-convex" class="headerlink" title="It’s nice to be convex!"></a>It’s nice to be convex!</h4><ul>
<li>$\nabla f(\pmb x)&#x3D;0$ if and only if $\pmb x$ is a global minimizer of $f(\pmb x)$ .</li>
<li>If $\pmb x$ is a local minimizer of a convex optimization problem, it is a global minimizer.</li>
</ul>
<h4 id="Optimization-methods"><a href="#Optimization-methods" class="headerlink" title="Optimization methods"></a>Optimization methods</h4><ul>
<li>Gradient descent</li>
<li>Newton’s method</li>
<li>Coordinate descent</li>
<li>Lagrangian method</li>
</ul>
<h2 id="Lecture3-BasicInML"><a href="#Lecture3-BasicInML" class="headerlink" title="Lecture3_BasicInML"></a>Lecture3_BasicInML</h2><h3 id="Generalization-ability-泛化能力"><a href="#Generalization-ability-泛化能力" class="headerlink" title="Generalization ability (泛化能力)"></a>Generalization ability (泛化能力)</h3><ul>
<li>A model’s ability to generalize to new data</li>
<li>If the model is trained too well, it can fit perfectly the random fluctuatioins or noise in the training data but it will fail to predict accurately on new data<br><img src="/pic/a99b5dbe4e510c276edb458034cea64.png"></li>
</ul>
<h4 id="Underfitting-and-overfitting"><a href="#Underfitting-and-overfitting" class="headerlink" title="Underfitting and overfitting"></a>Underfitting and overfitting</h4><ul>
<li><p>Underfitting (欠拟合) occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data.<br><img src="/pic/f70b694bdd89dce57b1ed3575e59c22.png"></p>
</li>
<li><p>Overfitting (过拟合) occurs when a statistical model describes random error or noise instead of the underlying relationship.<br><img src="/pic/8918735b6e8c752e2d4ae0af01c4e03.png"></p>
</li>
</ul>
<h4 id="Basic-terms"><a href="#Basic-terms" class="headerlink" title="Basic terms"></a>Basic terms</h4><ul>
<li>A sample is denoted by $(\pmb x_i,y_i)$ where $\pmb x_i$ is the attribute (feature) vector and $y_i$ is the label (response).</li>
<li>A list of $m$ samples is a dataset, denoted by $D$ .$$D&#x3D;{(\pmb x_i,y_i):i&#x3D;1,2,\cdots,m}$$<ul>
<li>Training set, denoted by $S$ , is used to train the model</li>
<li>Test set, denoted by $T$ , is used to evaluate the performance of the model</li>
</ul>
</li>
<li>A mapping from the attribute space (特征空间) $\mathcal X$ to the label space (标签空间) $\mathcal Y$ , denoted by $f$ , is called a hypothesis (假设).$$f:\mathcal X \rightarrow \mathcal Y$$</li>
<li>The set of all possible hypotheses is called hypothesis space (假设空间), denoted by $F$ .$$F&#x3D;{f_1,f_2,\cdots}$$</li>
</ul>
<h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><ul>
<li>Suppose $\hat f(\pmb x)$ is obtained using the training set $S$ </li>
<li>For an unknown sample $(\pmb x_0,y_0)$ , the error between the prediction $\hat f(\pmb x_0)$ and the observed value $y_0$ is measured by the loss function (损失函数) $$ L(\hat f(\pmb x_0),y_0)$$</li>
<li>Examples<ul>
<li>$L(\hat f(\pmb x),y)&#x3D;\mathbb I(\hat f(\pmb x)\neq y)$ (binary classification)</li>
<li>$L(\hat f(\pmb x),y)&#x3D;(\hat f(\pmb x)-y)^2$ (regression)</li>
</ul>
</li>
</ul>
<h4 id="Generalization-error"><a href="#Generalization-error" class="headerlink" title="Generalization error"></a>Generalization error</h4><h5 id="Generalization-error-泛化误差"><a href="#Generalization-error-泛化误差" class="headerlink" title="Generalization error (泛化误差)"></a>Generalization error (泛化误差)</h5><p>$$R(f)&#x3D;\mathbb E [L(f(\pmb x),y)]$$</p>
<ul>
<li>The expectation is taken over the joint distribution of $(\pmb x ,y)$</li>
</ul>
<h5 id="Test-error-测试误差"><a href="#Test-error-测试误差" class="headerlink" title="Test error (测试误差)"></a>Test error (测试误差)</h5><ul>
<li>Given a set of test samples $T&#x3D;{(\pmb x_i,y_i):i&#x3D;1,2,\cdots,n}$ , the test error is given by $$\hat R_T(f)&#x3D;\cfrac{1}{n}\sum_{i&#x3D;1}^n L(f(\pmb x_i),y_i)$$</li>
</ul>
<h5 id="Training-error-训练误差"><a href="#Training-error-训练误差" class="headerlink" title="Training error (训练误差)"></a>Training error (训练误差)</h5><ul>
<li>Given a set of training samples $S&#x3D;{(\pmb x_i,y_i):i&#x3D;1,2,\cdots,m}$ , the training error is given by $$\hat R_S(f)&#x3D;\cfrac{1}{m}\sum_{i&#x3D;1}^mL(f(\pmb x_i),y_i)$$</li>
</ul>
<h5 id="Learning-objective"><a href="#Learning-objective" class="headerlink" title="Learning objective"></a>Learning objective</h5><ul>
<li>Select a hypothesis $f\in F$ with the smallest <em>generalization error</em>. $$\min_{f\in F}R(f)&#x3D;\min_{f\in F}\mathbb E[L(f(\pmb x),y)]$$</li>
<li>However, the true distribution of $(\pmb x,y)$ is usually unknown in practice.</li>
<li>We obtain the hypothesis by minimizing the <em>training error</em> with the training set $S&#x3D;{(\pmb x_i,y_i):i&#x3D;1,2,\cdots,m}$ ,$$\min_{f\in F}\hat R_S(f)&#x3D;\min_{f\in F}\cfrac{1}{m}\sum_{i&#x3D;1}^m L(f(\pmb x_i),y_i)$$</li>
</ul>
<h5 id="Training-error-vs-test-error"><a href="#Training-error-vs-test-error" class="headerlink" title="Training error vs test error"></a>Training error vs test error</h5><p><img src="/pic/7ba8a7a66050778027f6549fa41cb2a.png"></p>
<h3 id="Performance-metrics"><a href="#Performance-metrics" class="headerlink" title="Performance metrics"></a>Performance metrics</h3><h4 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h4><h5 id="Performance-metrics-for-classification"><a href="#Performance-metrics-for-classification" class="headerlink" title="Performance metrics for classification"></a>Performance metrics for classification</h5><ul>
<li>Consider a binary classifier $\mathcal Y &#x3D;{-,+}$<br><img src="/pic/10adcb662406252426c360458bf636b.png"></li>
</ul>
<h5 id="Confusion-matrix"><a href="#Confusion-matrix" class="headerlink" title="Confusion matrix"></a>Confusion matrix</h5><p><img src="/pic/8181efdc993f5c23481a08ae4ac524b.png"></p>
<ul>
<li>True Positive (真正例) - we predicted “+” and the true class is “+”</li>
<li>True Negative (真反例) - we predicted “-“ and the true class is “-“</li>
<li>False Positive (假正例) - we predicted “+” and the true class is “-“</li>
<li>False Negative (假反例) - we predicted “-“ and the true class is “+”</li>
</ul>
<h5 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h5><p>$$\text{Accuracy} &#x3D; \cfrac{TP+TN}{TP+FN+FP+TN}&#x3D;\cfrac{\text{Correct predictions}}{\text{Total data points}}$$</p>
<ul>
<li>The relationship with the misclassification error rate (分类错误率), $$\text{Accuracy}&#x3D;1-\cfrac{1}{m}\sum_{i&#x3D;1}^m\mathbb I (f(\pmb x_i\neq y_i))$$</li>
</ul>
<h6 id="Limitation-of-Accuracy"><a href="#Limitation-of-Accuracy" class="headerlink" title="Limitation of Accuracy"></a>Limitation of Accuracy</h6><ul>
<li>A predictive model may have high accuracy, but be useless.<ul>
<li>Suppose the positive class is only a tiny portion of the observed data. For example, only 1% of patients has true cancer while other 99% of patients don’t have any cancers.</li>
<li>Consider a “stupid” model that always predicts “no cancer”, what is the accuracy?</li>
</ul>
</li>
</ul>
<h5 id="Precision-查准率"><a href="#Precision-查准率" class="headerlink" title="Precision(查准率)"></a>Precision(查准率)</h5><p>$$\text{Precision}&#x3D;\cfrac{TP}{TP+FP}&#x3D;\cfrac{\text{Correctly predicted positive}}{\text{All predicted positive}}$$</p>
<h5 id="Recall-查全率"><a href="#Recall-查全率" class="headerlink" title="Recall(查全率)"></a>Recall(查全率)</h5><p>$$\text{Recall}&#x3D;\cfrac{TP}{TP+FN}&#x3D;\cfrac{\text{Correctly predicted positive}}{\text{All real positive}}$$</p>
<ul>
<li>Which one is worse, False Positive or False Negative?</li>
<li>It depends!<ul>
<li>Medical Diagnosis - False Negative</li>
<li>Span Detection - False Positive</li>
</ul>
</li>
</ul>
<h5 id="F-1-score"><a href="#F-1-score" class="headerlink" title="$F_1$ score"></a>$F_1$ score</h5><ul>
<li><p>How to compare precision&#x2F;recall and decide which algorithm is better?</p>
</li>
<li><p>$F_1$ score: a combined measure $$F_1&#x3D;2\cfrac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}$$</p>
</li>
<li><p>Which metric to use?</p>
</li>
<li><p>Accuracy</p>
<ul>
<li>The class distribution is balanced</li>
<li>FP and FN costs are similar</li>
</ul>
</li>
<li><p>$F_1$ score</p>
<ul>
<li>The class distribution is unbalanced</li>
<li>FP and FN costs may be different</li>
</ul>
</li>
<li><p>Recall</p>
<ul>
<li>The cost of FN is much higher than that of FP</li>
<li>e.g. rather get healthy people labeled as sick over leaving a infected person labeled healthy</li>
</ul>
</li>
<li><p>Precision</p>
<ul>
<li>The cost of FP is much higher than that of FN</li>
<li>e.g. rather have some span emails in inbox than some regular emails in your spam box</li>
</ul>
</li>
</ul>
<h4 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h4><h5 id="Performance-metrics-for-regression"><a href="#Performance-metrics-for-regression" class="headerlink" title="Performance metrics for regression"></a>Performance metrics for regression</h5><ul>
<li>The common performance measure for regression is <em>mean squared error (MSE)</em> .</li>
<li>For a dateset $D&#x3D;{(\pmb x_i,y_i):i&#x3D;1,2,\cdots,m}$ , $$\hat R_D(\hat f)&#x3D;\cfrac{1}{m}\sum_{i&#x3D;1}^m(\hat f(\pmb x_i)-y_i)^2$$</li>
<li>Some other performance measures<ul>
<li>Root Mean Square Error (RMSE)</li>
<li>Mean Absolute Error (MAE)</li>
<li>$R^2$</li>
</ul>
</li>
</ul>
<h3 id="Bias-variance-decomposition"><a href="#Bias-variance-decomposition" class="headerlink" title="Bias-variance decomposition"></a>Bias-variance decomposition</h3><ul>
<li><p>Given a training set $S&#x3D;{(\pmb x_i,y_i),i&#x3D;1,2,\cdots,m}$ such that each sample $(\pmb x_i,y_i)$ satisfies the following relationship $$y&#x3D;f(\pmb x)+\epsilon$$</p>
<ul>
<li>$\epsilon$ is the noise with mean zero and variance $\sigma^2$ .</li>
</ul>
</li>
<li><p>Let $\hat f(\pmb x;S)$ denote the estimated function that is trained with the set $S$ </p>
</li>
<li><p>For an unseen sample $(\pmb x_0,y_0)$ ,</p>
<ul>
<li>the predicted value using the function trained with $S$ is $\hat f(\pmb x_0;S)$ </li>
<li>the expected predicted value is $\mathbb E_S[\hat f(\pmb x_0;S)]$ </li>
<li>the true value is $f(\pmb x_0)$ </li>
<li>the bias (偏差) of the predicted value is $$\text{Bias}[\hat f(\pmb x_0;S)]&#x3D;\mathbb E[\hat f(\pmb x_0;S)]-f(\pmb x_0)$$</li>
<li>the variance (方差) of the predicted value is $$\text{Var}[\hat f(\pmb x_0;S)]&#x3D;\mathbb E\left[(\hat f(\pmb x_0;S)-\mathbb E[\hat f(\pmb x_0;S)])^2\right]$$</li>
<li>the expected squared error, where the expectation is over the random noise and the training set, is given by $$E\left[(y_0-\hat f(\pmb x_0;S))^2\right]&#x3D;(\text{Bias}[\hat f(\pmb x_0;S)])^2+\text{Var}[\hat f(\pmb x_0;S)]+\sigma^2$$</li>
<li>proof: $$\begin{align}<br>  &amp;\mathbb E\left[(y_0-\hat f(\pmb x_0;S))^2\right]\<br>  &#x3D;&amp;\mathbb E\left[(y_0-\textcolor{red}{\mathbb E[\hat f(\pmb x_0;S)]+\mathbb E[\hat f(\pmb x_0;S)]}-\hat f(\pmb x_0;S))^2\right]\<br>  &#x3D;&amp;\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))^2+(y_0-\mathbb E[[\hat f(\pmb x_0;S)])^2+2(y_0-\mathbb E[\hat f(\pmb x_0;S)])(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))\right]\<br>  &#x3D;&amp;\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))^2\right]+\mathbb E[(y_0-\mathbb E[\hat f(\pmb x_0;S)])^2]+\mathbb E[2(y_0-\mathbb E[\hat f(\pmb x_0;S)])(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))]\<br>  &#x3D;&amp;\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))^2\right]+\mathbb E [(\textcolor{red}{f(\pmb x_0)+\epsilon}-\mathbb E[\hat f(\pmb x_0;S)])^2]\<br>  &#x3D;&amp;\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))^2\right]+\mathbb E[\hat f(\pmb x_0)-\mathbb E[\hat f(\pmb x_0;S)])^2]+\mathbb E[\epsilon^2]+\mathbb E[2\epsilon(f(\pmb x_0)-\mathbb E[\hat f(\pmb x_0;S)])]\<br>  &#x3D;&amp;\text{Var}[\hat f(\pmb x_0;S)]+(\text{Bias}[\hat f(\pmb x_0;S)])^2+\sigma^2<br>  \end{align}$$</li>
</ul>
</li>
<li><p>The <em>variance</em> represents how much the trained model move about its mean.</p>
</li>
<li><p>The <em>bias</em> represents the difference between the expected prediction of our model and the true value.<br><img src="/pic/e4f39df5e0cf02295ed0adf94236c82.png"><br><img src="/pic/4010e1c4642202ac977ba16ffc9a0ee.png"></p>
</li>
<li><p>The ideal case is that we reduce both the bias and variance to reduce the total error.</p>
</li>
<li><p>However, there is a trade-off between the bias and variance.</p>
<ul>
<li>High bias: more features, more complex models, better optimization, boosting, …</li>
<li>High variance: more data, regularization, less features, less complex models, bagging, …<br><img src="/pic/b0f3730daa3d994524a12413137e863.png"></li>
</ul>
</li>
</ul>
<h3 id="Model-selection"><a href="#Model-selection" class="headerlink" title="Model selection"></a>Model selection</h3><p>Fit the data (blue dots) using polynomials with different degrees of freedom.</p>
<ul>
<li>How to select the appropriate model with good fit?<br><img src="/pic/1a58a26150be5c680563a0eb25d7b8a.png"></li>
</ul>
<h4 id="Principle-of-Occam’s-razor-奥卡姆剃刀"><a href="#Principle-of-Occam’s-razor-奥卡姆剃刀" class="headerlink" title="Principle of Occam’s razor (奥卡姆剃刀)"></a>Principle of Occam’s razor (奥卡姆剃刀)</h4><ul>
<li>Select the hypothesis with the fewest assumptions among all competing hypotheses that explain known observations equally well.<br><img src="/pic/30dc5adbbd599316a0cb17da5726c8c.png"></li>
</ul>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><ul>
<li><p>Regularization(正则化) refers to the process of adding additional terms to our loss function, often to introduce a <u>preference for simpler models</u></p>
</li>
<li><p>It aims to reduce the generalization error but not its training error.</p>
</li>
<li><p>Recall the training error $\hat R_S(f)&#x3D;\cfrac{1}{m}\displaystyle\sum_{i&#x3D;1}^mL(f(\pmb x_i),y_i)$ , we search for the hypothesis that leads to the smallest training error $$\min_{f\in F}\hat R_S(f)$$</p>
</li>
<li><p>Minimization problem with regularized loss function $$\min_{f\in F}\hat R_S(f)+\lambda E(f)$$<br><img src="/pic/f4eec853b3f60729c5dd34dc7b4acc9.png"></p>
</li>
<li><p>$L_2$ regularization $$E(f&#x3D;\pmb w^T\pmb x)&#x3D;||\pmb w||<em>2^2&#x3D;\sum</em>{i&#x3D;1}^nw_i^2$$<br><img src="/pic/0ad30f160f2e7ecab62f7354a1c4616.png"></p>
</li>
<li><p>$L_1$ regularization $$E(f&#x3D;\pmb w^T\pmb x)&#x3D;||\pmb w||<em>1&#x3D;\sum</em>{i&#x3D;1}^n|w_i|$$<br><img src="/pic/a28e0bebc2515f58f224263e5cbedf5.png"></p>
</li>
</ul>
<h4 id="Hyperparameter-超参数"><a href="#Hyperparameter-超参数" class="headerlink" title="Hyperparameter(超参数)"></a>Hyperparameter(超参数)</h4><ul>
<li>The parameter is determined before the learning process<ul>
<li>Example: the degree of the polynomial, the regularization coefficient.</li>
</ul>
</li>
<li>It can not be adopted by the learning algorithm from the training data</li>
<li>How to find the optimal hyperparameter?<ul>
<li>Set it to different values</li>
<li>Evaluate the corresponding models</li>
<li>Choose the one that results in the best performance</li>
</ul>
</li>
</ul>
<h4 id="Validation-strategy"><a href="#Validation-strategy" class="headerlink" title="Validation strategy"></a>Validation strategy</h4><h5 id="Hold-out-validation"><a href="#Hold-out-validation" class="headerlink" title="Hold-out validation"></a>Hold-out validation</h5><ul>
<li>Split the training set into two parts: a training set and a validation set<br><img src="/pic/dd55e1c530692fa6810d9b314c15f84.png"></li>
</ul>
<h5 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross validation"></a>Cross validation</h5><ul>
<li>K-fold cross validation: divide the training set into k equal size subsets<br><img src="/pic/6c170feb2750e49af1c23e63965b608.png"></li>
</ul>
<h5 id="Leave-one-out-cross-validation"><a href="#Leave-one-out-cross-validation" class="headerlink" title="Leave-one-out cross validation"></a>Leave-one-out cross validation</h5><ul>
<li><p>K-fold cross validation where $k&#x3D;m$<br><img src="/pic/132f7b5bbcbb9caecf14d6a7f6bf2f0.png"></p>
</li>
<li><p>Use one observation as the validation set</p>
</li>
<li><p>Each sample is used once for validation</p>
</li>
<li><p>It could be vary computationally intensive!</p>
</li>
</ul>
<h4 id="Which-validation-strategy-to-use"><a href="#Which-validation-strategy-to-use" class="headerlink" title="Which validation strategy to use?"></a>Which validation strategy to use?</h4><ul>
<li>Large data set<ul>
<li>Hold-out validation is simpler testing and computationally cheaper.</li>
<li>Hold-out strategy is suitable when the amount of data is huge.</li>
</ul>
</li>
<li>Small data set<ul>
<li>Cross-validation is useful when the dataset is small.</li>
<li>10-fold cross validation is common, but smaller values of k are often used when learning takes a lot of time</li>
</ul>
</li>
</ul>
<h2 id="Lecture4-LinearModels"><a href="#Lecture4-LinearModels" class="headerlink" title="Lecture4_LinearModels"></a>Lecture4_LinearModels</h2><h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><h4 id="Motivating-example"><a href="#Motivating-example" class="headerlink" title="Motivating example"></a>Motivating example</h4><ul>
<li>The Advertising data set consists of the <em>sales</em> of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: <em>TV, radio and newspaper</em>.<br><img src="/8fce1beb900d9682237c079542b44f5.png"></li>
</ul>
<h4 id="Simple-linear-regression"><a href="#Simple-linear-regression" class="headerlink" title="Simple linear regression"></a>Simple linear regression</h4><ul>
<li>Assuming approximately a linear relationship between $X$ and $Y$ $$y\approx b+w\cdot x$$</li>
<li>Predicting $\hat y$ based on $x$ $$\hat y&#x3D;\hat b+\hat w\cdot x$$<br><img src="/pic/5192779dc145eba035b997cf1a76fcc.png"></li>
</ul>
<h5 id="Estimating-the-parameters"><a href="#Estimating-the-parameters" class="headerlink" title="Estimating the parameters"></a>Estimating the parameters</h5><ul>
<li><p>Mean square error (MSE) $$\begin{align}<br>MSE&amp;&#x3D;\cfrac{1}{m}((y_1-b-w\cdot x_1)^2+(y_2-b-w\cdot x_2)^2+\cdots+(y_m-b-w\cdot x_m)^2)\<br>&amp;&#x3D;\cfrac{1}{m}\sum_{i&#x3D;1}^m(y_i-b-w\cdot x_i)^2<br>\end{align}$$<br><img src="/pic/876fcd4c99289d7d14c2494c352de58.png"></p>
</li>
<li><p>FInd the linear function with the smallest MSE $$(\hat w,\hat b)&#x3D;\text{arg}\min_{w,b}&#x3D;\cfrac{1}{m}\sum_{i&#x3D;1}^m(y_i-b-w\cdot x_i)^2$$</p>
</li>
<li><p>FOCs: $$\begin{align}<br>\cfrac{\partial L(w,b)}{\partial w}&amp;&#x3D;\cfrac{1}{m}\sum_{i&#x3D;1}^m2(y_i-b-w\cdot x_i)(-x_i)&#x3D;0\<br>\cfrac{\partial L(w,b)}{\partial b}&amp;&#x3D;\cfrac{1}{m}\sum_{i&#x3D;1}^m2(y_i-b-w\cdot x_i)(-1)&#x3D;0<br>\end{align}$$<br>$$\begin{align}<br>\hat w&amp;&#x3D;\cfrac{\displaystyle\sum_{i&#x3D;1}^m(x_i-\bar x)(y_i-\bar y)}{\displaystyle\sum_{i&#x3D;1}^m(x_i-\bar x)^2},\ \ \ \ \hat b&#x3D;\bar y-\hat w\bar x,\<br>\bar x&amp;&#x3D;\cfrac{1}{m}\sum_{i&#x3D;1}^mx_i, \ \ \ \ \bar y&#x3D;\cfrac{1}{m}\sum_{i&#x3D;1}^my_i<br>\end{align}$$</p>
</li>
</ul>
<p><img src="/pic/4cda0b999d6f3752153a5f48f5b45fa.png"></p>
<h4 id="Multivariate-linear-regression"><a href="#Multivariate-linear-regression" class="headerlink" title="Multivariate linear regression"></a>Multivariate linear regression</h4><ul>
<li>We ignore the other two factors when estimating the coefficients</li>
<li>How to make predictions given the levels of the three advertising media?$$sales&#x3D;b+w_1\cdot TV+w_2\cdot radio+w_3\cdot newspaper+\epsilon$$<br>In general, with $n$-dimension features, $$\begin{align}<br>Y&amp;\approx b+w_1\cdot x_1+w_2\cdot x_2+\cdots+w_n\cdot x_n\<br>\hat y &amp;&#x3D;\hat b+\hat w_1\cdot x_1+\hat w_2\cdot x_2+\cdots+\hat w_n\cdot x_n<br>\end{align}$$<br>Choose $b,w_1,\cdots,w_n$ to minimize the following $$MSE&#x3D;\cfrac{1}{m}\sum_{i&#x3D;1}^m(y_i-\hat y_i)^2&#x3D;\cfrac{1}{m}\sum_{i&#x3D;1}^m(y_i-b-\hat w_1\cdot x_{i,1}-\hat w_2\cdot x_{i,2}-\cdots-\hat w_n\cdot x_{i,n})^2$$<br>$$\begin{align}<br>\pmb X&amp;&#x3D;\begin{bmatrix}<br>1 &amp;x_{1,1} &amp;\cdots &amp;x_{i,n}\<br>\vdots &amp;\vdots &amp;\ddots &amp;\vdots\<br>1 &amp;x_{m,1} &amp;\cdots &amp;x_{m,n}<br>\end{bmatrix}\ \ \ \ m\times (n+1) \text{ matrix with each row a feature vector}\<br>\pmb y&amp;&#x3D;[y_1,y_2,\cdots,y_m]^T\ \ \ \ m\times 1\text{ vector of outputs in the training set}\<br>\pmb\beta &amp;&#x3D;[b,w_1,w_2,\cdots,w_n]^T\ \ \ \ (n+1)\times 1\text{ vector of parameters}<br>\end{align}$$</li>
</ul>
<p>Choose $\beta$ to minimize the following objective $$MSE&#x3D;\cfrac{1}{m}(\pmb y-\pmb X\pmb \beta)^T(\pmb y-\pmb X\pmb\beta)$$</p>
<p>$$\min_{\pmb\beta} L(\pmb\beta)&#x3D;\cfrac{1}{2}(\pmb y-\pmb X\pmb\beta)^T(\pmb y-\pmb X\pmb\beta)$$<br>$$\cfrac{\partial L(\pmb\beta)}{\partial \pmb\beta}&#x3D;-\pmb X^T(\pmb y-\pmb X\pmb\beta)$$<br>To minimize $L(\pmb\beta)$ , we set its derivatives to zero and obtain the normal equations $$\cfrac{\partial L(\pmb\beta)}{\partial \pmb\beta}&#x3D;-\pmb X^T(\pmb y-\pmb X\pmb\beta)&#x3D;0\Rightarrow \pmb X^T\pmb X\hat{\pmb\beta}&#x3D;\pmb X^T\pmb y$$<br>Suppose $\pmb X^T\pmb X$ is invertible, $$\begin{align}<br>\hat{\pmb\beta}&amp;&#x3D;(\pmb X^T\pmb X)^{-1}\pmb X^T\pmb y\<br>\hat y_i&amp;&#x3D;\pmb x_i^T\hat{\pmb\beta}&#x3D;\pmb x_i^T(\pmb X^T\pmb X)^{-1}\pmb X^T\pmb y\<br>\hat {\pmb y}&amp;&#x3D;\pmb X\hat{\pmb\beta}&#x3D;\pmb X(\pmb X^T\pmb X)^{-1}\pmb X^T\pmb y<br>\end{align}$$</p>
<h5 id="Ridge-regression"><a href="#Ridge-regression" class="headerlink" title="Ridge regression"></a>Ridge regression</h5><ul>
<li>What if $\pmb X^T\pmb X$ is not invertible? $$\min_{\pmb\beta}L(\pmb\beta)&#x3D;(\pmb y-\pmb X\pmb\beta)^T(\pmb y-\pmb X\pmb\beta)+\lambda||\pmb\beta||_2^2$$ $$\cfrac{\partial L(\pmb\beta)}{\partial \pmb\beta}&#x3D;-2\pmb X^T(\pmb y-\pmb X\pmb\beta)+2\lambda\pmb\beta$$</li>
<li>To minimize $L(\pmb\beta)$ , we set its derivatives to zero: $$(\lambda\pmb I+\pmb X^T\pmb X)\pmb\beta&#x3D;\pmb X^T\pmb y$$</li>
<li>The estimator from Ridge regression is computed as: $$\hat{\pmb\beta}_\text{ridge}&#x3D;(\lambda\pmb I+\pmb X^T\pmb X)^{-1}\pmb X^T\pmb y\Leftarrow (\lambda\pmb I+\pmb X^T\pmb X)\text{ is invertible for }\lambda\textgreater 0$$</li>
</ul>
<h4 id="Regularized-linear-regression"><a href="#Regularized-linear-regression" class="headerlink" title="Regularized linear regression"></a>Regularized linear regression</h4><p>$$(\pmb y-\pmb X\pmb\beta)^T(\pmb y-\pmb X\pmb\beta)+\lambda E(\pmb\beta^T\pmb x)$$<br>$\lambda$ : Regularization coefficient;<br>$E(\pmb\beta^T\pmb x)$ : Regularization term</p>
<ul>
<li>L2 regularization (Ridge regression)<ul>
<li>$E(f&#x3D;\pmb\beta^T\pmb x)&#x3D;||\pmb\beta||_2^2$</li>
<li>$\lambda\rightarrow 0, \hat{\pmb\beta}<em>\text{ridge}\rightarrow\hat{\pmb\beta}</em>{OLS}$</li>
<li>$\lambda\rightarrow\infty,\hat{\pmb\beta}_\text{ridge}\rightarrow 0$</li>
</ul>
</li>
<li>L1 regularization (Lasso regression)<ul>
<li>$E(f&#x3D;\pmb\beta^T\pmb x)&#x3D;||\pmb\beta||_1$</li>
<li>Some of coefficient estimates tend to zeros</li>
<li>Variable selection (sparse models)</li>
</ul>
</li>
</ul>
<h5 id="Regularization-1"><a href="#Regularization-1" class="headerlink" title="Regularization"></a>Regularization</h5><p>Ridge vs. Lasso: which one is better? </p>
<ul>
<li>Case 1: A relatively small number of features have substantial coefficients and the remaining features have parameters that are very small or equal to zero.<ul>
<li>Lasso regression</li>
</ul>
</li>
<li>Case 2: The response is a function of many features, all with parameters of roughly equal size.<ul>
<li>Ridge regression</li>
</ul>
</li>
<li>Neither ridge nor lasso regression would universally dominate the other</li>
<li>Which approach to use? How to determine $\lambda$ ?<ul>
<li>Cross validation!</li>
</ul>
</li>
</ul>
<h4 id="Linear-regression-Probabilistic-View"><a href="#Linear-regression-Probabilistic-View" class="headerlink" title="Linear regression: Probabilistic View"></a>Linear regression: Probabilistic View</h4><h5 id="Recap-Probability-amp-Statistics"><a href="#Recap-Probability-amp-Statistics" class="headerlink" title="Recap: Probability &amp; Statistics"></a>Recap: Probability &amp; Statistics</h5><h6 id="The-multivariate-Gaussian-distribution"><a href="#The-multivariate-Gaussian-distribution" class="headerlink" title="The multivariate Gaussian distribution"></a>The multivariate Gaussian distribution</h6><p>A random vector $X$ is said to have a multivariate normal (Gaussian) distribution with mean $\mu\in\mathbb R^n$ and covariance matrix $\Sigma \in S_{++}^n$<br>$$f_{X_1,X_2,\cdots,X_n}(x_1,x_2,\cdots,x_n)&#x3D;\cfrac{1}{(2\pi)^\frac{n}{2}|\Sigma|^\frac{1}{2}}e^{-\frac{1}{2}(\pmb x-\mu)^T\Sigma^{-1}(\pmb x-\mu)}$$ We write this as $X\sim\mathcal N(\mu,\Sigma)$. </p>
<p><img src="/pic/bfd5b6d0bc87a6d76c69c391e921870.png"></p>
<p>When $n&#x3D;1$ , it is a normal (Gaussian) distribution $\mathcal N(\mu_1,\Sigma_{11})$ .<br>$$f_{X_1}(x_1)&#x3D;\cfrac{1}{(2\pi)^\frac{1}{2}|\Sigma_{11}|^\frac{1}{2}}e^{-\frac{1}{2}(x_1-\mu_1)^T\Sigma_{11}^{-1}(x_1-\mu_1)}$$</p>
<h5 id="Linear-regression-Probabilistic-View-1"><a href="#Linear-regression-Probabilistic-View-1" class="headerlink" title="Linear regression: Probabilistic View"></a>Linear regression: Probabilistic View</h5><p>Assume the response $Y$ is given by a deterministic function and an additive Gaussian noise. $$y&#x3D;\pmb\beta^T\pmb x+\epsilon,\text{ where }\epsilon\sim N(0,\sigma^2)$$</p>
<ul>
<li>The linear regression estimator is the maximum likelihood estimator of the data.</li>
<li>The likelihood function $$P(\pmb y|\pmb X;\pmb\beta)&#x3D;P(y_1|\pmb x_1;\pmb\beta)P(y_2|\pmb x_2;\pmb\beta)\cdots P(y_m|\pmb x_m;\pmb\beta)&#x3D;\prod_{i&#x3D;1}^mP(y_i|\pmb x_i;\pmb\beta)$$</li>
<li>The log-likelihood function $$\log P(\pmb y|\pmb X;\pmb\beta)&#x3D;\log\prod_{i&#x3D;1}^m P(y_i|\pmb x_i;\pmb\beta)&#x3D;\sum_{i&#x3D;1}^m\log P(y_i|\pmb x_i;\pmb\beta)$$</li>
<li>Given dataset $(\pmb X,\pmb y)$ , find $\pmb\beta$ that can maximize the log-likelihood of $\pmb y$ . $$\hat{\pmb\beta}&#x3D;\text{arg}\max_{\pmb\beta}\log P(\pmb y|\pmb X;\pmb\beta)&#x3D;\text{arg}\max_{\pmb\beta}\sum_{i&#x3D;1}^m\log P(y_i|\pmb x_i;\pmb\beta)$$</li>
</ul>
<p>$$\begin{align}<br>\hat{\pmb\beta}&amp;&#x3D;\text{arg}\max_{\pmb\beta}\log P(\pmb y|\pmb X;\pmb\beta)\<br>&amp;&#x3D;\text{arg}\max_{\pmb\beta}\sum_{i&#x3D;1}^m\log P(y_i|\pmb x_i;\pmb\beta)\<br>&amp;&#x3D;\text{arg}\max_{\pmb\beta}\sum_{i&#x3D;1}^m\log\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}}<br>\Leftarrow P(y_i|\pmb x_i;\pmb\beta)&#x3D;\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}}\<br>&amp;&#x3D;\text{arg}\max_{\pmb\beta}-\sum_{i&#x3D;1}^m\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}\<br>&amp;&#x3D;\text{arg}\min_{\pmb\beta}\sum_{i&#x3D;1}^m\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}\<br>&amp;&#x3D;\text{arg}\min_{\pmb\beta}\sum_{i&#x3D;1}^m(y_i-\pmb\beta^T\pmb x_i)^2<br>\Leftrightarrow MSE&#x3D;\frac{1}{m}(\pmb y-\pmb X\pmb\beta)^T(\pmb  y-\pmb X\pmb\beta)<br>\end{align}$$</p>
<h5 id="Ridge-regression-Probabilistic-View"><a href="#Ridge-regression-Probabilistic-View" class="headerlink" title="Ridge regression: Probabilistic View"></a>Ridge regression: Probabilistic View</h5><ul>
<li>Assume the response $Y$ is given by a deterministic function and an additive Gaussian noise. $$y&#x3D;\pmb\beta^T\pmb x+\epsilon,\text{ where }\epsilon\sim N(0,\sigma^2)$$</li>
<li>Suppose $\pmb\beta$ has the prior $p(\pmb\beta)&#x3D;\mathcal N(0,\tau^2\pmb I)$</li>
<li>Ridge regression estimator is a MAP estimator with Gaussian prior $$\begin{align}<br>\hat{\pmb\beta}<em>{MAP}&amp;&#x3D;\text{arg}\max\sum</em>{i&#x3D;1}^m\log p(y_i|\pmb x_i;\pmb\beta)+\log p(\pmb\beta)\<br>P(y_i|\pmb x_i;\pmb\beta)&#x3D;\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}}<br>\Rightarrow &amp;&#x3D;\text{arg}\max\sum_{i&#x3D;1}^m-\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}-\cfrac{1}{2\tau^2}||\pmb\beta||<em>2^2<br>\Leftarrow p(\pmb\beta)&#x3D;\cfrac{e^{-\cfrac{1}{2\tau^2}\pmb\beta^T\pmb\beta}}{\sqrt{(2\pi\tau^2)^n}}\<br>&amp;&#x3D;\text{arg}\max</em>{\pmb\beta}\sum_{i&#x3D;1}^m-(y_i-\pmb\beta^T\pmb x_i)^2-\lambda||\pmb\beta||<em>2^2<br>\Leftarrow \lambda&#x3D;\cfrac{\sigma^2}{\tau^2}\<br>\text{Ridge regression}\Leftrightarrow &amp;&#x3D;\text{arg}\min</em>{\pmb\beta}\sum_{i&#x3D;1}^m(y_i-\pmb\beta^T\pmb x_i)^2+\lambda||\pmb\beta||_2^2<br>\end{align}$$</li>
</ul>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://hiryan23.github.io/2023/02/17/Machine%20Learning/" title="机器学习-课程笔记" target="_blank" rel="external">http://hiryan23.github.io/2023/02/17/Machine Learning/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="" target="_blank"><span class="text-dark">Ruiyang He</span><small class="ml-1x">上海交通大学 本科生</small></a></h3>
        <div>生活不止眼前的苟且。</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
    
      <ul class="pager pull-left">
        
          <li class="prev">
            <a href="/2023/02/19/%E5%AF%B9chatgpt%E7%9A%84%E6%AC%A2%E5%91%BC%E6%98%AF%E4%BA%BA%E5%AF%B9%E5%BC%82%E5%8C%96%E7%9A%84%E5%8F%8D%E6%8A%97%E2%80%94%E2%80%94%E8%AE%BAai%E4%BB%A3%E5%86%99%E8%AE%BA%E6%96%87%E7%9A%84%E8%A7%A3%E6%94%BE%E6%80%A7%E4%B8%8E%E5%BC%80%E6%94%BE%E6%80%A7/" title="对chatgpt的欢呼是人对异化的反抗——论ai代写论文的解放性与开放性"><i
                class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
          </li>
          
            
              <li class="next">
                <a href="/2023/02/17/%E5%8D%9A%E5%BC%88%E8%AE%BA/" title="博弈论-课程笔记"><span>
                    下一篇&nbsp;&nbsp;
                  </span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
              </li>
              
                
                  <li class="toggle-toc">
                    <a class="toggle-btn " data-toggle="collapse"
                      href="#collapseToc" aria-expanded="false" title="文章目录" role="button">
                      <span>[&nbsp;</span><span>
                        文章目录
                      </span>
                      <i class="text-collapsed icon icon-anchor"></i>
                      <i class="text-in icon icon-close"></i>
                      <span>]</span>
                    </a>
                  </li>
                  
      </ul>
      
        

            <div class="bar-right">
              
                <div class="share-component" data-sites="qq,wechat"
                  data-mobile-sites="qq,wechat"></div>
                
            </div>
  </div>
</nav>
  


</main>

  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
    
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/hiryan23" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

        <div class="copyright">
            
                &copy; 2023
                    Ruiyang He
                        

                            <div class="publishby">
                                <span id="busuanzi_container_site_pv">
                                    <!--点击<span id="busuanzi_value_site_pv" style="font-family:Courier"></span>次，
                                    -->
                                    访客<span id="busuanzi_value_site_uv" style="font-family:Courier"></span>人
                                </span>
                            </div>

                            <!--
        <div class="publishby">
            Theme by
            <a href="https://github.com/cofess" target="_blank"> cofess </a>
            base on
            <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
        -->
        </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
   window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>

   
<script src="/js/application.js"></script>

      
    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>



         
                  
                           
                              
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



                                 
                                    
                                       
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'kuf03HfUPLpIZjgOwQwJ4d7f-9Nh9j0Va',
    appKey: 'dIdmyF1QYnqtOiB5b9grGYyf',
    placeholder: '不怀疑不能见真理',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script>

     

                                          
                                             

                                                



                                                   <div id="go-top"></div>
                                                   <style type="text/css">
                                                      #go-top {
                                                         width: 40px;
                                                         height: 36px;
                                                         background-color: #5a7db1;
                                                         position: relative;
                                                         border-radius: 2px;
                                                         position: fixed;
                                                         right: 10px;
                                                         bottom: 60px;
                                                         cursor: pointer;
                                                         display: none;
                                                      }

                                                      #go-top:after {
                                                         content: " ";
                                                         position: absolute;
                                                         left: 14px;
                                                         top: 14px;
                                                         border-top: 2px solid #fff;
                                                         border-right: 2px solid #fff;
                                                         width: 12px;
                                                         height: 12px;
                                                         transform: rotate(-45deg);
                                                      }

                                                      #go-top:hover {
                                                         background-color: #1A2433;
                                                      }
                                                   </style>
                                                   <script>
                                                      $(function () {
                                                         var top = $("#go-top");
                                                         $(window).scroll(function () {
                                                            ($(window).scrollTop() > 300) ? top.show(300) : top.hide(200);
                                                            $("#go-top").click(function () {
                                                               $('body,html').animate({ scrollTop: 0 });
                                                               return false();
                                                            })
                                                         });
                                                      });
                                                   </script>
</body>
</html>