

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Ruiyang He">
  <meta name="keywords" content="">
  
    <meta name="description" content="本笔记整理自安泰经济与管理学院2022年春季学期课程BUSS2505-02机器学习，授课人是李成璋老师。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-课程笔记 (TBC)">
<meta property="og:url" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/index.html">
<meta property="og:site_name" content="Hiryan&#39;s Blog">
<meta property="og:description" content="本笔记整理自安泰经济与管理学院2022年春季学期课程BUSS2505-02机器学习，授课人是李成璋老师。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/9538373a69fb3836595f01495607834.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/bfec802ec5619a7f6b5ef92fd832d31.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/6cf69ccc0b5e18ab05c080df1515205.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/110a43955525c0ad9a62c039eb3876a.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/dce728e8f2d0819688a148ebaf686f9.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/11af398aea2369112afe0fe46f5d255.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/379ac1b116f3c5612b0e6a1f3df25ea.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/b885ffb38cf90c077ad79abb079e005.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/57c1cdf60aceade4f79afcdcf5781c5.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/b9432d6edc99c82146c5ca340b6fd65.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/8c43075b2094ec2db8c526f1ea28d14.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/a99b5dbe4e510c276edb458034cea64.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/f70b694bdd89dce57b1ed3575e59c22.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/8918735b6e8c752e2d4ae0af01c4e03.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/7ba8a7a66050778027f6549fa41cb2a.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/10adcb662406252426c360458bf636b.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/8181efdc993f5c23481a08ae4ac524b.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/e4f39df5e0cf02295ed0adf94236c82.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/4010e1c4642202ac977ba16ffc9a0ee.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/b0f3730daa3d994524a12413137e863.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/1a58a26150be5c680563a0eb25d7b8a.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/30dc5adbbd599316a0cb17da5726c8c.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/f4eec853b3f60729c5dd34dc7b4acc9.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/0ad30f160f2e7ecab62f7354a1c4616.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/a28e0bebc2515f58f224263e5cbedf5.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/dd55e1c530692fa6810d9b314c15f84.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/6c170feb2750e49af1c23e63965b608.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/132f7b5bbcbb9caecf14d6a7f6bf2f0.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/8fce1beb900d9682237c079542b44f5.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/5192779dc145eba035b997cf1a76fcc.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/876fcd4c99289d7d14c2494c352de58.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/4cda0b999d6f3752153a5f48f5b45fa.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/bfd5b6d0bc87a6d76c69c391e921870.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/8238a887080cb52105e2da7c8321fab.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/6360a7faf1d39affd8e0985f07daa4a.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/a07605c556f05131fcb0eed75e8fe92.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/c04750dc47e5320cec59791c2897602.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/4e3d1c319b8d2121651b340bb9e0379.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/28249e9b76fddb1d028bad646f5b33e.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/34a695cff223faf29380cb4cd036595.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/6a2e08f093e043bef56e9524d47d15f.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/2beb86993fa4ae74f0081ec91c2c703.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/ab20f3bcdac435adab7fbe44748781b.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/0dbd9f34a7dd01a3cf8fdff2267228b.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/3b345e6e23b44d137f2eef677b086a0.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/163ee69801c5fd90f0db8434f12c0a0.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/d6ac10786053cf27bd83808148d4005.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/99505909a8ce803cd2585640c29ea03.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/31005d52795dd6bf9d6ccb279d59ceb.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/f7eb0da81ed0871c9b951828e1eef00.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/44f6ee7c2003c9bfdc6952ad6a82bb8.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/7932e2084b642ada22ad4a78f912adf.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/cf8051b1925bef0c1100e19ec7ecdc2.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/46bc95f08fe1f0d64f8f9a4733f00b0.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/60f7ff4efd734ff604959c09c02b128.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/9bc96b20dd2d9a83b34e6770feefa63.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/cba0fc4f531383fa7eee44fe02e9de0.png">
<meta property="article:published_time" content="2023-02-17T15:12:03.271Z">
<meta property="article:modified_time" content="2023-05-31T16:00:00.000Z">
<meta property="article:author" content="Ruiyang He">
<meta property="article:tag" content="课程笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/9538373a69fb3836595f01495607834.png">
  
  
  
  <title>机器学习-课程笔记 (TBC) - Hiryan&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"hiryan23.github.io","root":"/","version":"1.9.5-a","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":3},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Hiryan&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/lungmen.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.7)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="机器学习-课程笔记 (TBC)"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Ruiyang He
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-02-17 23:12" pubdate>
          2023年2月17日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          46k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          387 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar category-bar" style="margin-right: -1rem">
    





<div class="category-list">
  
  
    
    
    
    <div class="category row nomargin-x">
      <a class="category-item 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="2022S"
        id="heading-7c619f2a944876ca8a0b05cc8240e4b9" role="tab" data-toggle="collapse" href="#collapse-7c619f2a944876ca8a0b05cc8240e4b9"
        aria-expanded="true"
      >
        2022S
        <span class="list-group-count">(9)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-7c619f2a944876ca8a0b05cc8240e4b9"
           role="tabpanel" aria-labelledby="heading-7c619f2a944876ca8a0b05cc8240e4b9">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/2023/02/17/%E5%8D%9A%E5%BC%88%E8%AE%BA/" title="博弈论-课程笔记 (TBC)"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">博弈论-课程笔记 (TBC)</span>
        </a>
      
    
      
      
        <a href="/2023/02/16/%E5%8F%91%E5%B1%95%E7%BB%8F%E6%B5%8E%E5%AD%A6/" title="发展经济学-课程笔记 (TBC)"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">发展经济学-课程笔记 (TBC)</span>
        </a>
      
    
      
      
        <a href="/2023/02/17/Machine%20Learning/" title="机器学习-课程笔记 (TBC)"
           class="list-group-item list-group-item-action
           active">
          <span class="category-post">机器学习-课程笔记 (TBC)</span>
        </a>
      
    
      
      
        <a href="/2023/02/16/Econometrics/" title="计量经济学-课程笔记 (TBC)"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">计量经济学-课程笔记 (TBC)</span>
        </a>
      
    
      
      
        <a href="/2023/04/06/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%87%AA%E6%95%91/" title="计量经济学自救"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">计量经济学自救</span>
        </a>
      
    
      
      
        <a href="/2023/02/13/%E8%B4%A2%E5%8A%A1%E7%AE%A1%E7%90%86/" title="财务管理-课程笔记 (TBC)"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">财务管理-课程笔记 (TBC)</span>
        </a>
      
    
      
      
        <a href="/2023/02/14/%E8%B4%A7%E5%B8%81%E9%87%91%E8%9E%8D%E5%AD%A6/" title="货币金融学-课程笔记 (TBC)"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">货币金融学-课程笔记 (TBC)</span>
        </a>
      
    
      
      
        <a href="/2023/02/15/%E9%87%91%E8%9E%8D%E5%AD%A6%E5%8E%9F%E7%90%86/" title="金融学原理-课程笔记 (TBC)"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">金融学原理-课程笔记 (TBC)</span>
        </a>
      
    
      
      
        <a href="/2023/02/27/%E9%A9%AC%E5%85%8B%E6%80%9D%E4%B8%BB%E4%B9%89%E5%8E%9F%E7%90%86/" title="马克思主义原理-重点梳理"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">马克思主义原理-重点梳理</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
</div>


  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">机器学习-课程笔记 (TBC)</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：4 个月前
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <blockquote>
<p><em>本笔记整理自安泰经济与管理学院2022年春季学期课程BUSS2505-02机器学习，授课人是李成璋老师。</em></p>
</blockquote>
<span id="more"></span>
<h2 id="lecture1_introduction">Lecture1_Introduction</h2>
<h3 id="applications">Applications</h3>
<h4 id="machine-learning-is-all-around-us">Machine Learning is all
around us</h4>
<ul>
<li>Game AI
<ul>
<li>Deep blue, IBM</li>
<li>AlphaGo, Deep Mind</li>
<li>Deepstack, CMU &amp; Facebook AI</li>
</ul></li>
<li>Robot
<ul>
<li>SpotMini, BostonDynamics</li>
<li>Big Dog</li>
</ul></li>
<li>Image recognition</li>
<li>Self-driving car</li>
<li>Medical Diagnosis</li>
<li>Applications of ML in business settings</li>
<li>Customer segmentation</li>
<li>Applications in Finance</li>
<li>Credit lending &amp; Fraud detection</li>
<li>Personalized recommendation</li>
<li>Dynamic pricing
<ul>
<li>Rue La La</li>
</ul></li>
<li>Order dispatch for ride-sharing platforms
<ul>
<li>DiDi</li>
</ul></li>
</ul>
<h4 id="economical-impact-of-machine-learning">Economical impact of
Machine Learning</h4>
<ul>
<li>By 2035, AI could double annual global economic growth rates
(Accenture)</li>
<li>Global GDP may increase by up to 14% (the equivalent of US$15.7
trillion) by 2030 as a result of the accelerating development and
take-up of AI (PwC)</li>
</ul>
<h4 id="trends-of-machine-learning">Trends of Machine Learning</h4>
<figure>
<img src="/2023/02/17/Machine%20Learning/9538373a69fb3836595f01495607834.png" srcset="/img/loading.gif" lazyload alt="Trends of ML">
<figcaption aria-hidden="true">Trends of ML</figcaption>
</figure>
<h3 id="definition">Definition</h3>
<h4 id="what-is-machine-learning">What is Machine Learning?</h4>
<ul>
<li>Field of study that gives computers the ability to learn without
being explicitly programmed. - Arthur Samuel, 1959</li>
<li>A computer program is said to learn from experience E with respect
to some class of tasks T and performance measure P if its performance at
tasks in T, as measured by P, improves with experience E. - Tom
Mitchell, 1997</li>
</ul>
<h4 id="programming-vs-machine-learning">Programming vs Machine
Learning</h4>
<figure>
<img src="/2023/02/17/Machine%20Learning/bfec802ec5619a7f6b5ef92fd832d31.png" srcset="/img/loading.gif" lazyload alt="Programming vs ML">
<figcaption aria-hidden="true">Programming vs ML</figcaption>
</figure>
<h4 id="brief-history-of-machine-learning">Brief History of Machine
Learning</h4>
<ol type="1">
<li>Connectionism, 1950s
<ul>
<li>Perception, F. Rosenblatt</li>
</ul></li>
<li>Checker game, Arthur Samuel, 1959
<ul>
<li>The term Machine Learning is coined.</li>
</ul></li>
<li>Symbolism, 1970-1980s
<ul>
<li>Decision tree</li>
<li>ID3, Quinlan</li>
<li>Classification and Regression Tree (CART)</li>
</ul></li>
<li>Connectionism, 1980-1990s
<ul>
<li>Back-Propagation</li>
<li>for Multi-layer Neural Networks</li>
</ul></li>
<li>Statistical Learning, 1990s
<ul>
<li>Support vector machine</li>
<li>Kernel methods</li>
</ul></li>
<li>Connectionism, 2000s
<ul>
<li>Deep Learning</li>
</ul></li>
</ol>
<h3 id="categories">Categories</h3>
<p>Categories of Machine Learning <img src="/2023/02/17/Machine%20Learning/6cf69ccc0b5e18ab05c080df1515205.png" srcset="/img/loading.gif" lazyload alt="Categories of ML"></p>
<h4 id="supervised-learning">Supervised Learning</h4>
<p><img src="/2023/02/17/Machine%20Learning/110a43955525c0ad9a62c039eb3876a.png" srcset="/img/loading.gif" lazyload alt="SL"> - Aims to <strong>predict on unknown data</strong> using
models trained by labeled data - Learning a function that maps the
<strong>feature (attribute)</strong> to <strong>label
(response)</strong> - Classification vs Regression - Both utilize the
training set (known data) to make predictions - The output of
classification is categorical (<strong>discrete</strong>) while the
output of regression is numerical (<strong>continuous</strong>)</p>
<h5 id="process-of-supervised-learning">Process of Supervised
Learning</h5>
<ol type="1">
<li>Split data into training &amp; test sets</li>
<li>Train a model</li>
<li>Make predictions on testing set</li>
<li>Compare predicted and true labels</li>
</ol>
<h4 id="unsupervised-learning">Unsupervised Learning</h4>
<figure>
<img src="/2023/02/17/Machine%20Learning/dce728e8f2d0819688a148ebaf686f9.png" srcset="/img/loading.gif" lazyload alt="UL">
<figcaption aria-hidden="true">UL</figcaption>
</figure>
<p>Discover the structure and pattern within the unlabeled data. -
Market Segmentation - Social Network Analysis</p>
<h4 id="some-machine-learning-algorithms">Some Machine Learning
algorithms</h4>
<figure>
<img src="/2023/02/17/Machine%20Learning/11af398aea2369112afe0fe46f5d255.png" srcset="/img/loading.gif" lazyload alt="ML algorithms">
<figcaption aria-hidden="true">ML algorithms</figcaption>
</figure>
<h2 id="lecture2_mathforml">Lecture2_MathForML</h2>
<h4 id="notations">Notations</h4>
<ul>
<li><span class="math inline">\(a \in A\)</span> : <span class="math inline">\(a\)</span> is a member of set A</li>
<li><span class="math inline">\(||\pmb{v}||\)</span> : the norm of
vector <span class="math inline">\(\pmb{v}\)</span></li>
<li><span class="math inline">\(\pmb{x},\pmb{y},\pmb{z}\)</span> :
vector (lower case, bold)</li>
<li><span class="math inline">\(\pmb{A},\pmb{B}\)</span> : matrix (upper
case, bold)</li>
<li><span class="math inline">\(X\)</span> : random variable (upper
case)</li>
<li><span class="math inline">\(x\)</span> : realizaton of random
variable (lower case)</li>
<li><span class="math inline">\(y= f(\pmb{x})\)</span> : function with
muitiple inputs</li>
</ul>
<h3 id="probability-statistics">Probability &amp; Statistics</h3>
<h4 id="sample-spaceomega">Sample space(<span class="math inline">\(\Omega\)</span>)</h4>
<p>Set of all possible outcomes of an experiment</p>
<h4 id="evente">Event(<span class="math inline">\(E\)</span>)</h4>
<p>Any subset of outcomes contained in the sample space</p>
<h4 id="event-spacemathcal-f">Event space(<span class="math inline">\(\mathcal F\)</span>)</h4>
<p>The set of all possible events</p>
<h4 id="axioms-of-probability">Axioms of probability</h4>
<p>The <em>probabililty distribution</em> P is a function that satisfies
the following 1. <span class="math inline">\(0 \leq P(E) \leq 1\)</span>
for any <span class="math inline">\(E \in \mathcal F\)</span>
(Non-negativity) 2. <span class="math inline">\(P(\Omega)=1\)</span> 3.
<span class="math inline">\(P(E_1 \cup E_2)=P(E_1)+P(E_2)\)</span> if
<span class="math inline">\(E_1\)</span> and <span class="math inline">\(E_2\)</span> mutually exclusive events
(Additivity)</p>
<h4 id="random-variable-rv">Random variable (RV)</h4>
<p>mapping from sample space to real numbers - Probability distribution
specifies the probability of observing every possible value of a random
variable - Discrete RV has a countable set of possible values:
Bernoulli, Poisson, ... - Continuous RV can take infinitely many
possible values: Uniform, Normal, Exponential, ...</p>
<h4 id="probability-distribution">Probability distribution</h4>
<p>Cumulative distribution function (CDF) <span class="math display">\[
F_X(x)=P(X \leq x)\]</span> Discrete random variable: probability mass
function <span class="math inline">\(p_X(x)\)</span> <span class="math display">\[p_X(x)=P(X=x)\]</span> Continuous random
variable: probability density function <span class="math inline">\(f_X(x)\)</span> <span class="math display">\[f_X(x) =
\cfrac{\text{d}F_X(x)}{\text{d}x}\]</span> #### Joint distribution
Consider two random variables <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span> , the <em>joint cumulative
distribution function</em> is defined as <span class="math display">\[F_{XY}(x,y)=P(X \leq x,Y \leq y)\]</span> The
joint probability mass function of two discrete variables <span class="math inline">\(X\)</span> , <span class="math inline">\(Y\)</span> <span class="math display">\[p_{X,Y}(x,y)=P(X=x,Y=y)\]</span> The joint
probability density function of two continuous variables <span class="math inline">\(X\)</span> , <span class="math inline">\(Y\)</span> <span class="math display">\[f_{X,Y}(x,y)=\cfrac{\partial^2
F_{XY}(x,y)}{\partial x \partial y}\]</span> #### Conditional
probability The conditional probability of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=y\)</span> is defined as, <span class="math display">\[P(X=x|Y=y)=\cfrac{P(X=x,Y=y)}{P(Y=y)}\]</span></p>
<h4 id="product-rule">Product rule</h4>
<p><span class="math display">\[P(X=x,Y=y)=P(X=x|Y=y)P(Y=y)=P(Y=y|X=x)P(X=x)\]</span></p>
<h4 id="bayes-rule">Bayes' rule</h4>
<p><span class="math display">\[P(Y=y|X=x)=\cfrac{P(X=x|Y=y)\cdot
P(Y=y)}{P(X=x)}\]</span> - Application: SPAM email case - y: labels
(SPAM/normal) - x: frequency of keywords</p>
<h4 id="marginal-probability">Marginal probability</h4>
<p>The probability of event that will occur regardless of conditional
events <span class="math display">\[
\begin{align}
P(X=x) &amp;= \sum_{y \in \mathcal{y}}P(X=x,Y=y) \\
       &amp;= \sum_{y \in \mathcal{y}}P(X=x|Y=y)P(Y=y)
\end{align}
\]</span></p>
<h4 id="independence">Independence</h4>
<p>Consider two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> , they are <em>independent</em> if
<span class="math display">\[
P(X=x,Y=y)=P(X=x) \cdot P(Y=y)
\]</span> In addition, (if they are independent: ) <span class="math display">\[
P(X=x)= \cfrac{P(X=x,Y=y)}{P(Y=y)}=P(X=x | Y=y)
\]</span></p>
<h4 id="expectation">Expectation</h4>
<p>Expectation (expected value) of a random variable <span class="math inline">\(X\)</span> is computed as - <u>Discrete RV</u>
<span class="math display">\[ \mathbb{E} [x] = \sum_x x \cdot
p(x)\]</span> - <u>Continuous RV</u> <span class="math display">\[
\mathbb{E}[X]=\int_x x \cdot f(x) \cdot \text{d} x\]</span> -
Expectation of functions <span class="math display">\[\mathbb{E}[h(X)] =
\int_x h(x) \cdot  f(x) \cdot \text{d}x\]</span> - Other properties:
<span class="math display">\[\begin{align}
\mathbb{E}[aX+b]&amp;=a\mathbb{E}[X]+b \\
\mathbb{E}[X+Y]&amp;=\mathbb{E}[X]+\mathbb{E}[Y]\\
\mathbb E[XY]&amp;=\mathbb E[X]\mathbb E[Y],\ \text{if\ X\ and\ Y\ are
uncoorrelated}
\end{align}\]</span></p>
<h4 id="conditional-expectation">Conditional expectation</h4>
<p>The conditional expectation of <span class="math inline">\(X\)</span>
with respect to <span class="math inline">\(Y\)</span> is the function
<span class="math display">\[\mathbb{E}[X|Y=y]\]</span> Discrete random
variable <span class="math display">\[\mathbb{E}[X|Y=y]= \sum_x x\cdot
P(X=x|Y=y)\]</span> Continuous random variable <span class="math display">\[ \mathbb{E}[X|Y=y]=\int_x x f_{X|Y}(x|y)
\text{d}x\]</span></p>
<h5 id="law-of-total-expectation">Law of total expectation</h5>
<p><span class="math display">\[\mathbb{E}[X] =
\mathbb{E}[\mathbb{E}[X|Y]]\]</span></p>
<h4 id="variance">Variance</h4>
<p>The squared deviation of <span class="math inline">\(X\)</span> from
its mean <span class="math display">\[
\text{Var}[X]=\mathbb{E}[(X-\mathbb{E}[X])^2]=\mathbb{E}[X^2]=\mathbb{E}[X]^2\]</span>
- Standard variation <span class="math display">\[ \sigma =
\sqrt{\text{Var}[X]}\]</span> - Other properties <span class="math display">\[\begin{align}
\text{Var}[aX+b]&amp;=a^2 \cdot \text{Var}[x] \\
\text{Var}[X+Y]&amp;=\text{Var}[X]+\text{Var}[Y]\ \text{if X,Y are
uncorrelated}
\end{align}\]</span></p>
<h4 id="covariance">Covariance</h4>
<p><span class="math display">\[
\text{Cov}(X_1,X_2)=\mathbb{E}[(X_1-\mathbb{E}[X_1])(X_2-\mathbb{E}[X_2])]\]</span></p>
<h4 id="estimation-of-parameters">Estimation of Parameters</h4>
<ul>
<li>Suppose we have random variables <span class="math inline">\(X_1,
X_2, \cdots, X_n\)</span> and corresponding observations <span class="math inline">\(x_1, x_2, \cdots, x_n\)</span></li>
<li>We select a parametric model and fit the parameters of the model to
the data.</li>
<li>How do we choose the values of the parameters <span class="math inline">\(\theta\)</span> ?</li>
</ul>
<h5 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation
(MLE)</h5>
<p>Which <span class="math inline">\(\theta\)</span> makes the
observations <span class="math inline">\(x_1,x_2,\cdots,x_n\)</span>
most likely? - Maximize the likelihood of the observed data <span class="math display">\[ \hat{\theta}_{MLE}=\text{arg}\max_\theta
\mathcal{L}(\theta)=\text{arg}\max_\theta
p(x_1,x_2,\cdots,x_n|\theta)\]</span> - Assume that <span class="math inline">\(x_1,x_2, \cdots,x_n\)</span> are i.i.d., we have
<span class="math display">\[\mathcal{L}(\theta)=\prod_{i=1}^n
p(x_i|\theta)\]</span> - Take the logarithmic on both sides, we obtain
the log-likelihood <span class="math display">\[\log
\mathcal{L}(\theta)= \sum_{i=1}^n \log p(x_i|\theta)\]</span> ######
Example of MLE - Imagine a bowl contains a large number of red and white
balls. The proportion of the red balls, denoted by <span class="math inline">\(\theta\)</span> , is unknown. - Now we sample
balls from this bowl with replacement for <span class="math inline">\(n\)</span> times and observe <span class="math inline">\(x\)</span> red balls out of <span class="math inline">\(n\)</span> balls. - Likelihood function:<span class="math display">\[\begin{align}
L(\theta)=L(x;\theta)=\binom{n}{x}\theta^x(1-\theta)^{n-x} \\
\log L(\theta) = x\log \theta+(n-x)\log(1-\theta) \\
\cfrac{\partial \log L(\theta)}{\partial \theta} = 0 \Rightarrow
\hat\theta_{MLE} = \cfrac{x}{n}
\end{align}\]</span></p>
<h5 id="maximum-a-posteriori-estimation-map">Maximum A Posteriori
Estimation (MAP)</h5>
<p>Which <span class="math inline">\(\theta\)</span> maximizes the
posterior <span class="math inline">\(p(\theta |
x_1,x_2,\cdots,x_n)\)</span> given the prior <span class="math inline">\(p(\theta)\)</span> ? - We assume that the
parameter is a random variable, and we specify a prior distribution
<span class="math inline">\(p(\theta)\)</span> - By Bayes' rule, we
compute the posterior of the parameter <span class="math display">\[p(\theta | x_1,x_2,\cdots,x_n) \propto
p(\theta)p(x_1,x_2,\cdots,x_n|\theta)\]</span> - Estimate parameter
<span class="math inline">\(\theta\)</span> by maximizing the posterior
<span class="math display">\[\hat\theta_{MAP}=\text{arg}\max_\theta
p(\theta)p(x_1,x_2,\cdots,x_n|\theta)\]</span> - We take the logarithmic
of the posterior, <span class="math display">\[\hat\theta_{MAP}=\text{arg}\max_\theta \log
p(\theta)+\sum_{i=1}^n \log p(x_i|\theta)\]</span> <em>: MAP: balance
MLE and prior knowledge</em></p>
<h6 id="example-of-map">Example of MAP</h6>
<ul>
<li>Imagine a bowl contains a large number of red and white balls. The
proportion of the red balls, denoted by <span class="math inline">\(\theta\)</span> , is unknown, but with a Beta
prior, <span class="math inline">\(P(\theta) =\cfrac{\Gamma(\alpha +
\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}\)</span></li>
<li>Now we sample balls from this bowl with replacement for <span class="math inline">\(n\)</span> times and observe <span class="math inline">\(x\)</span> red balls out of <span class="math inline">\(n\)</span> balls.</li>
<li>The posterior function: <span class="math display">\[\begin{align}
p(x|\theta)p(\theta) &amp;=
\binom{n}{x}\theta^x(1-\theta)^{n-x}\cfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}
\\
\log p(x|\theta)p(\theta) &amp;= (x+\alpha-1)\log \theta +(n-x+\beta
-1)\log(1-\theta) \\
\hat\theta_{MAP}&amp;=\cfrac{x+\alpha-1}{n+\alpha+\beta-2}
\end{align}\]</span></li>
</ul>
<h3 id="linear-algebra">Linear Algebra</h3>
<h4 id="vector">Vector</h4>
<ul>
<li>A one-dimension array of <span class="math inline">\(n\)</span>
values, denoted by <span class="math inline">\(\pmb{x}\)</span> (lower
case, bold)</li>
<li><span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span>-th element of <span class="math inline">\(\pmb{x}\)</span> <span class="math display">\[
\pmb{x}=(x_1,x_2,\cdots,x_n)^T= \begin{bmatrix}x_1\\ x_2\\ \vdots \\ x_n
\end{bmatrix}\]</span></li>
</ul>
<h4 id="matrix">Matrix</h4>
<ul>
<li>A two-dimension array of <span class="math inline">\(m \times
n\)</span> values, denoted by <span class="math inline">\(\pmb{A}\)</span> (upper case, bold)</li>
<li><span class="math inline">\(m\)</span> is the number of row vector,
<span class="math inline">\(n\)</span> is the number of column
vectors</li>
<li><span class="math inline">\(a_{ij}\)</span> is the entry in <span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column <span class="math display">\[\pmb{A}=\begin{bmatrix} a_{11} &amp;\cdots
&amp;a_{1n} \\ \vdots &amp;\ddots &amp;\vdots \\ a_{m1} &amp;\cdots
&amp;a_{mn} \end{bmatrix} \]</span></li>
</ul>
<h4 id="vector-algorithmic">Vector algorithmic</h4>
<ul>
<li>Scalar multiplication of a vector <span class="math display">\[
\pmb{y}=a\pmb{x}=(ax_1,ax_2,\cdots,ax_n)^T\]</span></li>
<li>Dot product of the vectors <span class="math inline">\(\pmb{x},\pmb{y} \in \mathbb{R}^n\)</span> <span class="math display">\[\pmb{x}^T\pmb{y} = [x_1,x_2,\cdots,
x_n]\begin{bmatrix}y_1\\ y_2\\ \vdots\\ y_n\end{bmatrix}=\sum_{i=1}^n
x_iy_i \in \mathbb{R}\]</span></li>
<li>Outer product of the vectors <span class="math inline">\(\pmb{x} \in
\mathbb{R}^m, \pmb{y} \in \mathbb{R}^n\)</span> <span class="math display">\[\pmb{x}\pmb{y}^T=\begin{bmatrix}x_1\\ x_2\\
\vdots x_m\end{bmatrix}[y_1,y_2,\cdots,y_n]=\begin{bmatrix}x_1y_1
&amp;\cdots &amp;x_1y_n\\ \vdots &amp;\ddots &amp;\vdots\\ x_my_1
&amp;\cdots &amp;x_my_n\end{bmatrix}\]</span></li>
</ul>
<h4 id="vector-norm">Vector norm</h4>
<p>A norm <span class="math inline">\(||\cdot||\)</span> is a function
that satisfies - <span class="math inline">\(||\pmb{x}|| \geq 0\)</span>
with equality if and only if <span class="math inline">\(\pmb{x}=\pmb{0}\)</span> - <span class="math inline">\(||\pmb{x}+\pmb{y}|| \leq ||\pmb{x}|| +
||\pmb{y}||\)</span> - <span class="math inline">\(||a\pmb{x}||=|a|||\pmb{x}||\)</span> - <span class="math inline">\(\pmb{x}^T\pmb{y}=||\pmb{x}||_2||\pmb{y}||_2\cos(\theta)\)</span>
- <span class="math inline">\(l_1\)</span> norm <span class="math inline">\(||\pmb{x}||_1=\displaystyle\sum_{i=1}^n|x_i|\)</span>
- <span class="math inline">\(l_2\)</span> norm <span class="math inline">\(||\pmb{x}||_2=\left(\displaystyle\sum_{i=1}^n|x_i|^2\right)^{\frac{1}{2}}\)</span></p>
<h4 id="matrix-arithmetric">Matrix arithmetric</h4>
<ul>
<li>Addition of matrices <span class="math inline">\(\pmb{A},\pmb{B} \in
\mathbb{R}^{m\times n}\)</span> <span class="math display">\[\pmb{C}=\pmb{A}+\textcolor{red}{\pmb{B}}=\begin{bmatrix}
a_{11}+\textcolor{red}{b_{11}} &amp;\cdots
&amp;a_{1n}+\textcolor{red}{b_{1n}}\\ \vdots &amp;\ddots &amp;\vdots\\
a_{m1}+\textcolor{red}{b_{m1}} &amp;\cdots
&amp;a_{mn}+\textcolor{red}{b_{mn}} \end{bmatrix}\]</span></li>
<li>Scalar multiplication of a matrix <span class="math display">\[\pmb{B}=\textcolor{red}{d}\cdot
\pmb{A}=\begin{bmatrix}\textcolor{red}{d}\cdot a_{11} &amp;\cdots
&amp;\textcolor{red}{d}\cdot a_{1n}\\ \vdots &amp;\ddots &amp;\vdots\\
\textcolor{red}{d}\cdot a_{m1} &amp;\cdots &amp;\textcolor{red}{d}\cdot
a_{mn}\end{bmatrix}\]</span></li>
<li>Multiplication of matrices <span class="math inline">\(\pmb{A} \in
\mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(\pmb{B}
\in \mathbb{R}^{n\times p}\)</span> <span class="math display">\[
\pmb{A}\pmb{B}=\pmb{C} \in \mathbb{R}^{m\times p},\ \ \ \
c_{ij}=\sum_{k=1}^na_{ik}b_{kj}\]</span></li>
<li>Matrix multiplication is <em>associative</em>: <span class="math inline">\(\pmb{A}(\pmb{B}\pmb{C})=(\pmb{AB})\pmb{C}\)</span></li>
<li>Matrix multiplication is <em>distributive</em>: <span class="math inline">\(\pmb A (\pmb B+\pmb
C)=\pmb{AB}+\pmb{AC}\)</span></li>
<li>Matrix multiplication is <em>NOT communicative</em>: <span class="math inline">\(\pmb{AB} \neq \pmb{BA}\)</span></li>
</ul>
<h4 id="transpose">Transpose</h4>
<p>Given a matrix <span class="math inline">\(\pmb A \in \mathbb
R^{m\times n}\)</span> , its transpose, written by <span class="math inline">\(\pmb A^T \in \mathbb R^{n\times m}\)</span> , is
given by <span class="math display">\[(\pmb A^T)_{ij}=(\pmb
A)_{ji}\]</span> Some properties - <span class="math inline">\((\pmb{AB})^T=\pmb B^T\pmb A^T\)</span> - <span class="math inline">\((\pmb A^T)^T=\pmb A\)</span> - <span class="math inline">\((\pmb A+\pmb B)^T=\pmb A^T+\pmb B^T\)</span></p>
<h4 id="symmetric-matrix">Symmetric matrix</h4>
<p>A square matrix is <em>symmetric</em> if <span class="math inline">\(\pmb A =\pmb A^T\)</span>.</p>
<h4 id="inverse-of-a-matrix">Inverse of a matrix</h4>
<p>For a matrix <span class="math inline">\(\pmb A\in \mathbb R^{n\times
n}\)</span> , if there exists a square matrix <span class="math inline">\(\pmb B \in \mathbb R^{n\times n}\)</span> such
that <span class="math display">\[\pmb{BA}=\pmb{AB}=\pmb I\]</span>
where <span class="math inline">\(\pmb I\)</span> is the <span class="math inline">\(n\)</span>-by-<span class="math inline">\(n\)</span> <em>identity matrix</em>, then <span class="math inline">\(\pmb B\)</span> is the <em>inverse</em> of <span class="math inline">\(\pmb A\)</span>.</p>
<ul>
<li>The inverse of <span class="math inline">\(\pmb A\)</span> is
denoted by <span class="math inline">\(\pmb A^{-1}\)</span></li>
<li>A matrix is <em>invertible</em> if it is not <em>singular</em>.</li>
</ul>
<p><u>Solving a linear system</u> If <span class="math inline">\(\pmb
A\)</span> is square nonsingular matrix, then the solution to the linear
system <span class="math inline">\(\pmb{AX}=\pmb b\)</span> is given by
<span class="math display">\[\pmb x=\pmb A^{-1}\pmb b\]</span></p>
<h4 id="semidefinite-matrices">Semidefinite matrices</h4>
<p>A symmetric matrix <span class="math inline">\(\pmb A \in \mathbb
S^{n\times n}\)</span> is - <em>positive semidefinite</em> if <span class="math inline">\(\pmb x^T \pmb{Ax} \geq 0\)</span> for any <span class="math inline">\(\pmb x \in \mathbb R^n\)</span> and <span class="math inline">\(\pmb x \neq \pmb 0\)</span> , denoted by <span class="math inline">\(\pmb A \succcurlyeq 0\)</span> ; - <em>positive
definite</em> if <span class="math inline">\(\pmb x^T \pmb{Ax} &gt;
0\)</span> for any <span class="math inline">\(\pmb x \in \mathbb
R^n\)</span> and <span class="math inline">\(\pmb x \neq 0\)</span> ,
denoted by <span class="math inline">\(\pmb A \succ 0\)</span> ; -
negative semidefinite if <span class="math inline">\(-\pmb A\)</span> is
positive semidefinite; - negative definite if <span class="math inline">\(-\pmb A\)</span> is positive definite; -
indefinite if it is neither positive nor negative definite.</p>
<h4 id="back-to-probability-statistics">Back to Probability &amp;
Statistics</h4>
<h5 id="random-vector">Random vector</h5>
<p>A vector of random variables <span class="math inline">\(X_1,X_2,\cdots,X_n\)</span>, denoted by <span class="math inline">\(\pmb X=[X_1,X_2,\cdots,X_n]^T\)</span> - <span class="math inline">\(\mathbb E[X]=[\mathbb E[X_1],\mathbb
E[X_2],\cdots,\mathbb E[X_n]]^T\)</span></p>
<h5 id="covariance-matrix">Covariance matrix</h5>
<p><span class="math display">\[\Sigma = \begin{bmatrix}
\text{Cov}[X_1,X_1] &amp;\cdots &amp;\text{Cov}[X_1,X_n]\\ \vdots
&amp;\ddots &amp;\vdots\\ \text{Cov}[X_n,X_1] &amp;\cdots
&amp;\text{Cov}[X_n,X_n]\end{bmatrix}=\mathbb E[(\pmb X -\mathbb
E[X])(\pmb X-\mathbb E[\pmb X])^T]\]</span></p>
<h4 id="matrix-calculus">Matrix calculus</h4>
<p>Consider a function <span class="math inline">\(f:\mathbb R^n
\rightarrow \mathbb R\)</span> , the <em>gradient</em> of <span class="math inline">\(f\)</span> is defined as a vector of partial
derivatives <span class="math display">\[\nabla f(\pmb
x)=\begin{bmatrix}\cfrac{\partial f(\pmb x)}{\partial x_1}\\
\cfrac{\partial f(\pmb x)}{\partial x_2}\\ \vdots\\ \cfrac{\partial
f(\pmb x)}{\partial x_n}\end{bmatrix}\]</span> "direction and rate of
fastest <strong>increase</strong>" - The direction of fastest increase
of the function - The magnitude is the rate of increase</p>
<p>Consider a function <span class="math inline">\(f: \mathbb R^n
\rightarrow \mathbb R\)</span>, the <em>Hessian</em> of <span class="math inline">\(f\)</span> is defined as <span class="math display">\[\nabla^2f(\pmb
x)=\begin{bmatrix}\cfrac{\partial^2f(\pmb x)}{\partial x_1^2}
&amp;\cdots &amp;\cfrac{\partial^2f(\pmb x)}{\partial x_1\partial x_n}\\
\vdots &amp;\ddots &amp;\vdots\\ \cfrac{\partial^2f(\pmb x)}{\partial
x_n\partial x_1} &amp;\cdots &amp;\cfrac{\partial^2f(\pmb x)}{\partial
x_n^2}\end{bmatrix}\]</span> - Hessian is symmetric when <span class="math inline">\(f\)</span> is twice differentiable. <span class="math display">\[\cfrac{\partial^2 f(\pmb x)}{\partial x_i\partial
x_j}=\cfrac{\partial^2f(\pmb x)}{\partial x_j\partial x_i}\]</span> ###
Optimization</p>
<h4 id="what-is-optimization">What is optimization?</h4>
<p>Finding the minimizer of a function subject to constraints: <span class="math display">\[\begin{align}
\text{minimize}_{\pmb x} \ \ \ \ &amp;f(\pmb x) \\
s.t.\ \ \ \ &amp;f_i(\pmb x)\leq 0,i=1,2,\cdots, k;\\
&amp;h_j(\pmb x)=0,j=1,2,\cdots,l.
\end{align}\]</span> - Mean-variance analysis - Transportation problems
- Facility location - Linear regression - Logistic regression - Support
vector machine - Neural networks</p>
<h4 id="local-minima-and-global-minima">Local minima and global
minima</h4>
<ul>
<li>Local minima is the solution that optimal within a neighboring
set</li>
<li>Global minima is the optimal solution among all possible solutions
<img src="/2023/02/17/Machine%20Learning/379ac1b116f3c5612b0e6a1f3df25ea.png" srcset="/img/loading.gif" lazyload></li>
</ul>
<h4 id="convex-set">Convex set</h4>
<p>A set <span class="math inline">\(C\in \mathbb R^n\)</span> is
<em>convex</em> if for <span class="math inline">\(\pmb x,\pmb y\in
C\)</span> and any <span class="math inline">\(\alpha\in [0,1]\)</span>,
<span class="math display">\[\alpha \pmb x+(1-\alpha)\pmb y \in
C\]</span><img src="/2023/02/17/Machine%20Learning/b885ffb38cf90c077ad79abb079e005.png" srcset="/img/loading.gif" lazyload></p>
<p>Examples: <span class="math inline">\(\mathbb R^n\)</span> , norm
ball {<span class="math inline">\(\pmb x: ||\pmb x|| \leq r\)</span>}
for a given <span class="math inline">\(r\)</span> , intersection of
convex sets</p>
<h4 id="convex-concave-function">Convex (Concave) function</h4>
<p>A function <span class="math inline">\(f:\mathbb R^n \rightarrow
\mathbb R\)</span> is convex (concave) if for <span class="math inline">\(\pmb x,\pmb y \in \text{dom}(f)\)</span> and any
<span class="math inline">\(\alpha \in [0,1]\)</span> , <span class="math display">\[f(\alpha \pmb x+(1-\alpha)\pmb y)\leq(\geq)
\alpha f(\pmb x)+(1-\alpha)f(\pmb y)\]</span><img src="/2023/02/17/Machine%20Learning/57c1cdf60aceade4f79afcdcf5781c5.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="first-order-convexity-condition">First-order convexity
condition</h4>
<p>Suppose a function <span class="math inline">\(f:\mathbb R^n
\rightarrow \mathbb R\)</span> is differentiable. Then <span class="math inline">\(f\)</span> is convex if and only if for all <span class="math inline">\(\pmb x,\pmb y \in \text{dom}(f)\)</span> <span class="math display">\[f(\pmb y) \geq f(\pmb x) + \nabla f(\pmb
x)^T(\pmb y-\pmb x)\]</span><img src="/2023/02/17/Machine%20Learning/b9432d6edc99c82146c5ca340b6fd65.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="second-order-convexity-condition">Second-order convexity
condition</h4>
<p>Suppose a function <span class="math inline">\(f:\mathbb R^n
\rightarrow \mathbb R\)</span> is twice differentiable. Then <span class="math inline">\(f\)</span> is convex if and only if for all <span class="math inline">\(\pmb x \in \text{dom}(f)\)</span> <span class="math display">\[\nabla^2f(\pmb x)\succcurlyeq 0\]</span><img src="/2023/02/17/Machine%20Learning/8c43075b2094ec2db8c526f1ea28d14.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="examples-of-convex-functions">Examples of convex functions</h4>
<ul>
<li>Exponential function: <span class="math inline">\(e^{ax}\)</span></li>
<li>Logarithmic function: <span class="math inline">\(\log(x)\)</span>
is concave</li>
<li>Affine function: <span class="math inline">\(\pmb a^T\pmb
x+b\)</span> is a convex and concave</li>
<li>Least square loss: <span class="math inline">\(||\pmb
y-\pmb{X\beta}||_2^2\)</span></li>
<li><span class="math inline">\(f_1(x)\)</span> is convex for <span class="math inline">\(x\in \text{dom}(f_1)\)</span> and <span class="math inline">\(f_2(x)\)</span> is convex for <span class="math inline">\(x\in \text{dom}(f_2)\)</span> , then <span class="math inline">\(f_1+f_2\)</span> is convex for <span class="math inline">\(x\in \text{dom}(f_1) \cap
\text{dom}(f_2)\)</span></li>
</ul>
<h4 id="convex-optimization-problem">Convex optimization problem</h4>
<p>An optimization problem is convex if its objective is a convex
function, the inequality constraints <span class="math inline">\(f_j\)</span> are convex, and the equality
constraints <span class="math inline">\(h_j\)</span> are affine. <span class="math display">\[\begin{align}
\text{minimize}_{\pmb x} \ \ \ \ &amp;f(\pmb x)\\
s.t. \ \ \ \ &amp;f_i(\pmb x)\leq 0,i=1,2,\cdots,k; \\
&amp;h_j(\pmb x)=0,j=1,2,\cdots,l.
\end{align}\]</span></p>
<h4 id="its-nice-to-be-convex">It's nice to be convex!</h4>
<ul>
<li><span class="math inline">\(\nabla f(\pmb x)=0\)</span> if and only
if <span class="math inline">\(\pmb x\)</span> is a global minimizer of
<span class="math inline">\(f(\pmb x)\)</span> .</li>
<li>If <span class="math inline">\(\pmb x\)</span> is a local minimizer
of a convex optimization problem, it is a global minimizer.</li>
</ul>
<h4 id="optimization-methods">Optimization methods</h4>
<ul>
<li>Gradient descent</li>
<li>Newton's method</li>
<li>Coordinate descent</li>
<li>Lagrangian method</li>
</ul>
<h2 id="lecture3_basicinml">Lecture3_BasicInML</h2>
<h3 id="generalization-ability-泛化能力">Generalization ability
(泛化能力)</h3>
<ul>
<li>A model's ability to generalize to new data</li>
<li>If the model is trained too well, it can fit perfectly the random
fluctuatioins or noise in the training data but it will fail to predict
accurately on new data <img src="/2023/02/17/Machine%20Learning/a99b5dbe4e510c276edb458034cea64.png" srcset="/img/loading.gif" lazyload></li>
</ul>
<h4 id="underfitting-and-overfitting">Underfitting and overfitting</h4>
<ul>
<li><p>Underfitting (欠拟合) occurs when a statistical model or machine
learning algorithm cannot capture the underlying trend of the data. <img src="/2023/02/17/Machine%20Learning/f70b694bdd89dce57b1ed3575e59c22.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>Overfitting (过拟合) occurs when a statistical model describes
random error or noise instead of the underlying relationship. <img src="/2023/02/17/Machine%20Learning/8918735b6e8c752e2d4ae0af01c4e03.png" srcset="/img/loading.gif" lazyload></p></li>
</ul>
<h4 id="basic-terms">Basic terms</h4>
<ul>
<li>A sample is denoted by <span class="math inline">\((\pmb
x_i,y_i)\)</span> where <span class="math inline">\(\pmb x_i\)</span> is
the attribute (feature) vector and <span class="math inline">\(y_i\)</span> is the label (response).</li>
<li>A list of <span class="math inline">\(m\)</span> samples is a
dataset, denoted by <span class="math inline">\(D\)</span> .<span class="math display">\[D=\{(\pmb x_i,y_i):i=1,2,\cdots,m\}\]</span>
<ul>
<li>Training set, denoted by <span class="math inline">\(S\)</span> , is
used to train the model</li>
<li>Test set, denoted by <span class="math inline">\(T\)</span> , is
used to evaluate the performance of the model</li>
</ul></li>
<li>A mapping from the attribute space (特征空间) <span class="math inline">\(\mathcal X\)</span> to the label space (标签空间)
<span class="math inline">\(\mathcal Y\)</span> , denoted by <span class="math inline">\(f\)</span> , is called a hypothesis (假设).<span class="math display">\[f:\mathcal X \rightarrow \mathcal Y\]</span></li>
<li>The set of all possible hypotheses is called hypothesis space
(假设空间), denoted by <span class="math inline">\(F\)</span> .<span class="math display">\[F=\{f_1,f_2,\cdots\}\]</span></li>
</ul>
<h4 id="loss-function">Loss function</h4>
<ul>
<li>Suppose <span class="math inline">\(\hat f(\pmb x)\)</span> is
obtained using the training set <span class="math inline">\(S\)</span></li>
<li>For an unknown sample <span class="math inline">\((\pmb
x_0,y_0)\)</span> , the error between the prediction <span class="math inline">\(\hat f(\pmb x_0)\)</span> and the observed value
<span class="math inline">\(y_0\)</span> is measured by the loss
function (损失函数) <span class="math display">\[ L(\hat f(\pmb
x_0),y_0)\]</span></li>
<li>Examples
<ul>
<li><span class="math inline">\(L(\hat f(\pmb x),y)=\mathbb I(\hat
f(\pmb x)\neq y)\)</span> (binary classification)</li>
<li><span class="math inline">\(L(\hat f(\pmb x),y)=(\hat f(\pmb
x)-y)^2\)</span> (regression)</li>
</ul></li>
</ul>
<h4 id="generalization-error">Generalization error</h4>
<h5 id="generalization-error-泛化误差">Generalization error
(泛化误差)</h5>
<p><span class="math display">\[R(f)=\mathbb E [L(f(\pmb x),y)]\]</span>
- The expectation is taken over the joint distribution of <span class="math inline">\((\pmb x ,y)\)</span></p>
<h5 id="test-error-测试误差">Test error (测试误差)</h5>
<ul>
<li>Given a set of test samples <span class="math inline">\(T=\{(\pmb
x_i,y_i):i=1,2,\cdots,n\}\)</span> , the test error is given by <span class="math display">\[\hat R_T(f)=\cfrac{1}{n}\sum_{i=1}^n L(f(\pmb
x_i),y_i)\]</span></li>
</ul>
<h5 id="training-error-训练误差">Training error (训练误差)</h5>
<ul>
<li>Given a set of training samples <span class="math inline">\(S=\{(\pmb x_i,y_i):i=1,2,\cdots,m\}\)</span> , the
training error is given by <span class="math display">\[\hat
R_S(f)=\cfrac{1}{m}\sum_{i=1}^mL(f(\pmb x_i),y_i)\]</span> #####
Learning objective</li>
<li>Select a hypothesis <span class="math inline">\(f\in F\)</span> with
the smallest <em>generalization error</em>. <span class="math display">\[\min_{f\in F}R(f)=\min_{f\in F}\mathbb E[L(f(\pmb
x),y)]\]</span></li>
<li>However, the true distribution of <span class="math inline">\((\pmb
x,y)\)</span> is usually unknown in practice.</li>
<li>We obtain the hypothesis by minimizing the <em>training error</em>
with the training set <span class="math inline">\(S=\{(\pmb
x_i,y_i):i=1,2,\cdots,m\}\)</span> ,<span class="math display">\[\min_{f\in F}\hat R_S(f)=\min_{f\in
F}\cfrac{1}{m}\sum_{i=1}^m L(f(\pmb x_i),y_i)\]</span></li>
</ul>
<h5 id="training-error-vs-test-error">Training error vs test error</h5>
<p><img src="/2023/02/17/Machine%20Learning/7ba8a7a66050778027f6549fa41cb2a.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="performance-metrics">Performance metrics</h3>
<h4 id="classification">Classification</h4>
<h5 id="performance-metrics-for-classification">Performance metrics for
classification</h5>
<ul>
<li>Consider a binary classifier <span class="math inline">\(\mathcal Y
=\{-,+\}\)</span> <img src="/2023/02/17/Machine%20Learning/10adcb662406252426c360458bf636b.png" srcset="/img/loading.gif" lazyload></li>
</ul>
<h5 id="confusion-matrix">Confusion matrix</h5>
<p><img src="/2023/02/17/Machine%20Learning/8181efdc993f5c23481a08ae4ac524b.png" srcset="/img/loading.gif" lazyload>
- True Positive (真正例) - we predicted "+" and the true class is "+" -
True Negative (真反例) - we predicted "-" and the true class is "-" -
False Positive (假正例) - we predicted "+" and the true class is "-" -
False Negative (假反例) - we predicted "-" and the true class is "+"</p>
<h5 id="accuracy">Accuracy</h5>
<p><span class="math display">\[\text{Accuracy} =
\cfrac{TP+TN}{TP+FN+FP+TN}=\cfrac{\text{Correct
predictions}}{\text{Total data points}}\]</span> - The relationship with
the misclassification error rate (分类错误率), <span class="math display">\[\text{Accuracy}=1-\cfrac{1}{m}\sum_{i=1}^m\mathbb
I (f(\pmb x_i\neq y_i))\]</span> ###### Limitation of Accuracy - A
predictive model may have high accuracy, but be useless. - Suppose the
positive class is only a tiny portion of the observed data. For example,
only 1% of patients has true cancer while other 99% of patients don't
have any cancers. - Consider a "stupid" model that always predicts "no
cancer", what is the accuracy?</p>
<h5 id="precision查准率">Precision(查准率)</h5>
<p><span class="math display">\[\text{Precision}=\cfrac{TP}{TP+FP}=\cfrac{\text{Correctly
predicted positive}}{\text{All predicted positive}}\]</span></p>
<h5 id="recall查全率">Recall(查全率)</h5>
<p><span class="math display">\[\text{Recall}=\cfrac{TP}{TP+FN}=\cfrac{\text{Correctly
predicted positive}}{\text{All real positive}}\]</span></p>
<ul>
<li>Which one is worse, False Positive or False Negative?</li>
<li>It depends!
<ul>
<li>Medical Diagnosis - False Negative</li>
<li>Span Detection - False Positive</li>
</ul></li>
</ul>
<h5 id="f_1-score"><span class="math inline">\(F_1\)</span> score</h5>
<ul>
<li><p>How to compare precision/recall and decide which algorithm is
better?</p></li>
<li><p><span class="math inline">\(F_1\)</span> score: a combined
measure <span class="math display">\[F_1=2\cfrac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}\]</span></p></li>
<li><p>Which metric to use?</p></li>
<li><p>Accuracy</p>
<ul>
<li>The class distribution is balanced</li>
<li>FP and FN costs are similar</li>
</ul></li>
<li><p><span class="math inline">\(F_1\)</span> score</p>
<ul>
<li>The class distribution is unbalanced</li>
<li>FP and FN costs may be different</li>
</ul></li>
<li><p>Recall</p>
<ul>
<li>The cost of FN is much higher than that of FP</li>
<li>e.g. rather get healthy people labeled as sick over leaving a
infected person labeled healthy</li>
</ul></li>
<li><p>Precision</p>
<ul>
<li>The cost of FP is much higher than that of FN</li>
<li>e.g. rather have some span emails in inbox than some regular emails
in your spam box</li>
</ul></li>
</ul>
<h4 id="regression">Regression</h4>
<h5 id="performance-metrics-for-regression">Performance metrics for
regression</h5>
<ul>
<li>The common performance measure for regression is <em>mean squared
error (MSE)</em> .</li>
<li>For a dateset <span class="math inline">\(D=\{(\pmb
x_i,y_i):i=1,2,\cdots,m\}\)</span> , <span class="math display">\[\hat
R_D(\hat f)=\cfrac{1}{m}\sum_{i=1}^m(\hat f(\pmb
x_i)-y_i)^2\]</span></li>
<li>Some other performance measures
<ul>
<li>Root Mean Square Error (RMSE)</li>
<li>Mean Absolute Error (MAE)</li>
<li><span class="math inline">\(R^2\)</span></li>
</ul></li>
</ul>
<h3 id="bias-variance-decomposition">Bias-variance decomposition</h3>
<ul>
<li><p>Given a training set <span class="math inline">\(S=\{(\pmb
x_i,y_i),i=1,2,\cdots,m\}\)</span> such that each sample <span class="math inline">\((\pmb x_i,y_i)\)</span> satisfies the following
relationship <span class="math display">\[y=f(\pmb
x)+\epsilon\]</span></p>
<ul>
<li><span class="math inline">\(\epsilon\)</span> is the noise with mean
zero and variance <span class="math inline">\(\sigma^2\)</span> .</li>
</ul></li>
<li><p>Let <span class="math inline">\(\hat f(\pmb x;S)\)</span> denote
the estimated function that is trained with the set <span class="math inline">\(S\)</span></p></li>
<li><p>For an unseen sample <span class="math inline">\((\pmb
x_0,y_0)\)</span> ,</p>
<ul>
<li>the predicted value using the function trained with <span class="math inline">\(S\)</span> is <span class="math inline">\(\hat
f(\pmb x_0;S)\)</span></li>
<li>the expected predicted value is <span class="math inline">\(\mathbb
E_S[\hat f(\pmb x_0;S)]\)</span></li>
<li>the true value is <span class="math inline">\(f(\pmb
x_0)\)</span></li>
<li>the bias (偏差) of the predicted value is <span class="math display">\[\text{Bias}[\hat f(\pmb x_0;S)]=\mathbb E[\hat
f(\pmb x_0;S)]-f(\pmb x_0)\]</span></li>
<li>the variance (方差) of the predicted value is <span class="math display">\[\text{Var}[\hat f(\pmb x_0;S)]=\mathbb
E\left[(\hat f(\pmb x_0;S)-\mathbb E[\hat f(\pmb
x_0;S)])^2\right]\]</span></li>
<li>the expected squared error, where the expectation is over the random
noise and the training set, is given by <span class="math display">\[E\left[(y_0-\hat f(\pmb
x_0;S))^2\right]=(\text{Bias}[\hat f(\pmb x_0;S)])^2+\text{Var}[\hat
f(\pmb x_0;S)]+\sigma^2\]</span></li>
<li>proof: <span class="math display">\[\begin{align}
  &amp;\mathbb E\left[(y_0-\hat f(\pmb x_0;S))^2\right]\\
  =&amp;\mathbb E\left[(y_0-\textcolor{red}{\mathbb E[\hat f(\pmb
x_0;S)]+\mathbb E[\hat f(\pmb x_0;S)]}-\hat f(\pmb x_0;S))^2\right]\\
  =&amp;\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb
x_0;S))^2+(y_0-\mathbb E[[\hat f(\pmb x_0;S)])^2+2(y_0-\mathbb E[\hat
f(\pmb x_0;S)])(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb
x_0;S))\right]\\
  =&amp;\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb
x_0;S))^2\right]+\mathbb E[(y_0-\mathbb E[\hat f(\pmb
x_0;S)])^2]+\mathbb E[2(y_0-\mathbb E[\hat f(\pmb x_0;S)])(\mathbb
E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))]\\
  =&amp;\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb
x_0;S))^2\right]+\mathbb E [(\textcolor{red}{f(\pmb
x_0)+\epsilon}-\mathbb E[\hat f(\pmb x_0;S)])^2]\\
  =&amp;\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb
x_0;S))^2\right]+\mathbb E[\hat f(\pmb x_0)-\mathbb E[\hat f(\pmb
x_0;S)])^2]+\mathbb E[\epsilon^2]+\mathbb E[2\epsilon(f(\pmb
x_0)-\mathbb E[\hat f(\pmb x_0;S)])]\\
  =&amp;\text{Var}[\hat f(\pmb x_0;S)]+(\text{Bias}[\hat f(\pmb
x_0;S)])^2+\sigma^2
  \end{align}\]</span></li>
</ul></li>
<li><p>The <em>variance</em> represents how much the trained model move
about its mean.</p></li>
<li><p>The <em>bias</em> represents the difference between the expected
prediction of our model and the true value. <img src="/2023/02/17/Machine%20Learning/e4f39df5e0cf02295ed0adf94236c82.png" srcset="/img/loading.gif" lazyload> <img src="/2023/02/17/Machine%20Learning/4010e1c4642202ac977ba16ffc9a0ee.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>The ideal case is that we reduce both the bias and variance to
reduce the total error.</p></li>
<li><p>However, there is a trade-off between the bias and variance.</p>
<ul>
<li>High bias: more features, more complex models, better optimization,
boosting, ...</li>
<li>High variance: more data, regularization, less features, less
complex models, bagging, ... <img src="/2023/02/17/Machine%20Learning/b0f3730daa3d994524a12413137e863.png" srcset="/img/loading.gif" lazyload></li>
</ul></li>
</ul>
<h3 id="model-selection">Model selection</h3>
<p>Fit the data (blue dots) using polynomials with different degrees of
freedom. - How to select the appropriate model with good fit? <img src="/2023/02/17/Machine%20Learning/1a58a26150be5c680563a0eb25d7b8a.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="principle-of-occams-razor-奥卡姆剃刀">Principle of Occam's razor
(奥卡姆剃刀)</h4>
<ul>
<li>Select the hypothesis with the fewest assumptions among all
competing hypotheses that explain known observations equally well. <img src="/2023/02/17/Machine%20Learning/30dc5adbbd599316a0cb17da5726c8c.png" srcset="/img/loading.gif" lazyload></li>
</ul>
<h4 id="regularization">Regularization</h4>
<ul>
<li><p>Regularization(正则化) refers to the process of adding additional
terms to our loss function, often to introduce a <u>preference for
simpler models</u></p></li>
<li><p>It aims to reduce the generalization error but not its training
error.</p></li>
<li><p>Recall the training error <span class="math inline">\(\hat
R_S(f)=\cfrac{1}{m}\displaystyle\sum_{i=1}^mL(f(\pmb x_i),y_i)\)</span>
, we search for the hypothesis that leads to the smallest training error
<span class="math display">\[\min_{f\in F}\hat R_S(f)\]</span></p></li>
<li><p>Minimization problem with regularized loss function <span class="math display">\[\min_{f\in F}\hat R_S(f)+\lambda E(f)\]</span>
<img src="/2023/02/17/Machine%20Learning/f4eec853b3f60729c5dd34dc7b4acc9.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p><span class="math inline">\(L_2\)</span> regularization <span class="math display">\[E(f=\pmb w^T\pmb x)=||\pmb
w||_2^2=\sum_{i=1}^nw_i^2\]</span> <img src="/2023/02/17/Machine%20Learning/0ad30f160f2e7ecab62f7354a1c4616.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p><span class="math inline">\(L_1\)</span> regularization <span class="math display">\[E(f=\pmb w^T\pmb x)=||\pmb
w||_1=\sum_{i=1}^n|w_i|\]</span> <img src="/2023/02/17/Machine%20Learning/a28e0bebc2515f58f224263e5cbedf5.png" srcset="/img/loading.gif" lazyload></p></li>
</ul>
<h4 id="hyperparameter超参数">Hyperparameter(超参数)</h4>
<ul>
<li>The parameter is determined before the learning process
<ul>
<li>Example: the degree of the polynomial, the regularization
coefficient.</li>
</ul></li>
<li>It can not be adopted by the learning algorithm from the training
data</li>
<li>How to find the optimal hyperparameter?
<ul>
<li>Set it to different values</li>
<li>Evaluate the corresponding models</li>
<li>Choose the one that results in the best performance</li>
</ul></li>
</ul>
<h4 id="validation-strategy">Validation strategy</h4>
<h5 id="hold-out-validation">Hold-out validation</h5>
<ul>
<li>Split the training set into two parts: a training set and a
validation set <img src="/2023/02/17/Machine%20Learning/dd55e1c530692fa6810d9b314c15f84.png" srcset="/img/loading.gif" lazyload></li>
</ul>
<h5 id="cross-validation">Cross validation</h5>
<ul>
<li>K-fold cross validation: divide the training set into k equal size
subsets <img src="/2023/02/17/Machine%20Learning/6c170feb2750e49af1c23e63965b608.png" srcset="/img/loading.gif" lazyload></li>
</ul>
<h5 id="leave-one-out-cross-validation">Leave-one-out cross
validation</h5>
<ul>
<li><p>K-fold cross validation where <span class="math inline">\(k=m\)</span> <img src="/2023/02/17/Machine%20Learning/132f7b5bbcbb9caecf14d6a7f6bf2f0.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>Use one observation as the validation set</p></li>
<li><p>Each sample is used once for validation</p></li>
<li><p>It could be vary computationally intensive!</p></li>
</ul>
<h4 id="which-validation-strategy-to-use">Which validation strategy to
use?</h4>
<ul>
<li>Large data set
<ul>
<li>Hold-out validation is simpler testing and computationally
cheaper.</li>
<li>Hold-out strategy is suitable when the amount of data is huge.</li>
</ul></li>
<li>Small data set
<ul>
<li>Cross-validation is useful when the dataset is small.</li>
<li>10-fold cross validation is common, but smaller values of k are
often used when learning takes a lot of time</li>
</ul></li>
</ul>
<h2 id="lecture4_linearmodels">Lecture4_LinearModels</h2>
<h3 id="linear-regression">Linear Regression</h3>
<h4 id="motivating-example">Motivating example</h4>
<ul>
<li>The Advertising data set consists of the <em>sales</em> of that
product in 200 different markets, along with advertising budgets for the
product in each of those markets for three different media: <em>TV,
radio and newspaper</em>. <img src="/2023/02/17/Machine%20Learning/8fce1beb900d9682237c079542b44f5.png" srcset="/img/loading.gif" lazyload></li>
</ul>
<h4 id="simple-linear-regression">Simple linear regression</h4>
<ul>
<li>Assuming approximately a linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> <span class="math display">\[y\approx
b+w\cdot x\]</span></li>
<li>Predicting <span class="math inline">\(\hat y\)</span> based on
<span class="math inline">\(x\)</span> <span class="math display">\[\hat
y=\hat b+\hat w\cdot x\]</span> <img src="/2023/02/17/Machine%20Learning/5192779dc145eba035b997cf1a76fcc.png" srcset="/img/loading.gif" lazyload></li>
</ul>
<h5 id="estimating-the-parameters">Estimating the parameters</h5>
<ul>
<li><p>Mean square error (MSE) <span class="math display">\[\begin{align}
MSE&amp;=\cfrac{1}{m}((y_1-b-w\cdot x_1)^2+(y_2-b-w\cdot
x_2)^2+\cdots+(y_m-b-w\cdot x_m)^2)\\
&amp;=\cfrac{1}{m}\sum_{i=1}^m(y_i-b-w\cdot x_i)^2
\end{align}\]</span> <img src="/2023/02/17/Machine%20Learning/876fcd4c99289d7d14c2494c352de58.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>FInd the linear function with the smallest MSE <span class="math display">\[(\hat w,\hat
b)=\text{arg}\min_{w,b}=\cfrac{1}{m}\sum_{i=1}^m(y_i-b-w\cdot
x_i)^2\]</span></p></li>
<li><p>FOCs: <span class="math display">\[\begin{align}
\cfrac{\partial L(w,b)}{\partial
w}&amp;=\cfrac{1}{m}\sum_{i=1}^m2(y_i-b-w\cdot x_i)(-x_i)=0\\
\cfrac{\partial L(w,b)}{\partial
b}&amp;=\cfrac{1}{m}\sum_{i=1}^m2(y_i-b-w\cdot x_i)(-1)=0
\end{align}\]</span> <span class="math display">\[\begin{align}
\hat w&amp;=\cfrac{\displaystyle\sum_{i=1}^m(x_i-\bar x)(y_i-\bar
y)}{\displaystyle\sum_{i=1}^m(x_i-\bar x)^2},\ \ \ \ \hat b=\bar y-\hat
w\bar x,\\
\bar x&amp;=\cfrac{1}{m}\sum_{i=1}^mx_i, \ \ \ \ \bar
y=\cfrac{1}{m}\sum_{i=1}^my_i
\end{align}\]</span></p></li>
</ul>
<p><img src="/2023/02/17/Machine%20Learning/4cda0b999d6f3752153a5f48f5b45fa.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="multivariate-linear-regression">Multivariate linear
regression</h4>
<ul>
<li>We ignore the other two factors when estimating the
coefficients</li>
<li>How to make predictions given the levels of the three advertising
media?<span class="math display">\[sales=b+w_1\cdot TV+w_2\cdot
radio+w_3\cdot newspaper+\epsilon\]</span> In general, with <span class="math inline">\(n\)</span>-dimension features, <span class="math display">\[\begin{align}
Y&amp;\approx b+w_1\cdot x_1+w_2\cdot x_2+\cdots+w_n\cdot x_n\\
\hat y &amp;=\hat b+\hat w_1\cdot x_1+\hat w_2\cdot x_2+\cdots+\hat
w_n\cdot x_n
\end{align}\]</span> Choose <span class="math inline">\(b,w_1,\cdots,w_n\)</span> to minimize the
following <span class="math display">\[MSE=\cfrac{1}{m}\sum_{i=1}^m(y_i-\hat
y_i)^2=\cfrac{1}{m}\sum_{i=1}^m(y_i-b-\hat w_1\cdot x_{i,1}-\hat
w_2\cdot x_{i,2}-\cdots-\hat w_n\cdot x_{i,n})^2\]</span> <span class="math display">\[\begin{align}
\pmb X&amp;=\begin{bmatrix}
1 &amp;x_{1,1} &amp;\cdots &amp;x_{i,n}\\
\vdots &amp;\vdots &amp;\ddots &amp;\vdots\\
1 &amp;x_{m,1} &amp;\cdots &amp;x_{m,n}
\end{bmatrix}\ \ \ \ m\times (n+1) \text{ matrix with each row a feature
vector}\\
\pmb y&amp;=[y_1,y_2,\cdots,y_m]^T\ \ \ \ m\times 1\text{ vector of
outputs in the training set}\\
\pmb\beta &amp;=[b,w_1,w_2,\cdots,w_n]^T\ \ \ \ (n+1)\times 1\text{
vector of parameters}
\end{align}\]</span></li>
</ul>
<p>Choose <span class="math inline">\(\beta\)</span> to minimize the
following objective <span class="math display">\[MSE=\cfrac{1}{m}(\pmb
y-\pmb X\pmb \beta)^T(\pmb y-\pmb X\pmb\beta)\]</span></p>
<p><span class="math display">\[\min_{\pmb\beta}
L(\pmb\beta)=\cfrac{1}{2}(\pmb y-\pmb X\pmb\beta)^T(\pmb y-\pmb
X\pmb\beta)\]</span> <span class="math display">\[\cfrac{\partial
L(\pmb\beta)}{\partial \pmb\beta}=-\pmb X^T(\pmb y-\pmb
X\pmb\beta)\]</span> To minimize <span class="math inline">\(L(\pmb\beta)\)</span> , we set its derivatives to
zero and obtain the normal equations <span class="math display">\[\cfrac{\partial L(\pmb\beta)}{\partial
\pmb\beta}=-\pmb X^T(\pmb y-\pmb X\pmb\beta)=0\Rightarrow \pmb X^T\pmb
X\hat{\pmb\beta}=\pmb X^T\pmb y\]</span> Suppose <span class="math inline">\(\pmb X^T\pmb X\)</span> is invertible, <span class="math display">\[\begin{align}
\hat{\pmb\beta}&amp;=(\pmb X^T\pmb X)^{-1}\pmb X^T\pmb y\\
\hat y_i&amp;=\pmb x_i^T\hat{\pmb\beta}=\pmb x_i^T(\pmb X^T\pmb
X)^{-1}\pmb X^T\pmb y\\
\hat {\pmb y}&amp;=\pmb X\hat{\pmb\beta}=\pmb X(\pmb X^T\pmb X)^{-1}\pmb
X^T\pmb y
\end{align}\]</span></p>
<h5 id="ridge-regression">Ridge regression</h5>
<ul>
<li>What if <span class="math inline">\(\pmb X^T\pmb X\)</span> is not
invertible? <span class="math display">\[\min_{\pmb\beta}L(\pmb\beta)=(\pmb y-\pmb
X\pmb\beta)^T(\pmb y-\pmb X\pmb\beta)+\lambda||\pmb\beta||_2^2\]</span>
<span class="math display">\[\cfrac{\partial L(\pmb\beta)}{\partial
\pmb\beta}=-2\pmb X^T(\pmb y-\pmb
X\pmb\beta)+2\lambda\pmb\beta\]</span></li>
<li>To minimize <span class="math inline">\(L(\pmb\beta)\)</span> , we
set its derivatives to zero: <span class="math display">\[(\lambda\pmb
I+\pmb X^T\pmb X)\pmb\beta=\pmb X^T\pmb y\]</span></li>
<li>The estimator from Ridge regression is computed as: <span class="math display">\[\hat{\pmb\beta}_\text{ridge}=(\lambda\pmb I+\pmb
X^T\pmb X)^{-1}\pmb X^T\pmb y\Leftarrow (\lambda\pmb I+\pmb X^T\pmb
X)\text{ is invertible for }\lambda&gt; 0\]</span></li>
</ul>
<h4 id="regularized-linear-regression">Regularized linear
regression</h4>
<p><span class="math display">\[(\pmb y-\pmb X\pmb\beta)^T(\pmb y-\pmb
X\pmb\beta)+\lambda E(\pmb\beta^T\pmb x)\]</span> <span class="math inline">\(\lambda\)</span> : Regularization coefficient;
<span class="math inline">\(E(\pmb\beta^T\pmb x)\)</span> :
Regularization term</p>
<ul>
<li>L2 regularization (Ridge regression)
<ul>
<li><span class="math inline">\(E(f=\pmb\beta^T\pmb
x)=||\pmb\beta||_2^2\)</span></li>
<li><span class="math inline">\(\lambda\rightarrow 0,
\hat{\pmb\beta}_\text{ridge}\rightarrow\hat{\pmb\beta}_{OLS}\)</span></li>
<li><span class="math inline">\(\lambda\rightarrow\infty,\hat{\pmb\beta}_\text{ridge}\rightarrow
0\)</span></li>
</ul></li>
<li>L1 regularization (Lasso regression)
<ul>
<li><span class="math inline">\(E(f=\pmb\beta^T\pmb
x)=||\pmb\beta||_1\)</span></li>
<li>Some of coefficient estimates tend to zeros</li>
<li>Variable selection (sparse models)</li>
</ul></li>
</ul>
<h5 id="regularization-1">Regularization</h5>
<p>Ridge vs. Lasso: which one is better?</p>
<ul>
<li>Case 1: A relatively small number of features have substantial
coefficients and the remaining features have parameters that are very
small or equal to zero.
<ul>
<li>Lasso regression</li>
</ul></li>
<li>Case 2: The response is a function of many features, all with
parameters of roughly equal size.
<ul>
<li>Ridge regression</li>
</ul></li>
<li>Neither ridge nor lasso regression would universally dominate the
other</li>
<li>Which approach to use? How to determine <span class="math inline">\(\lambda\)</span> ?
<ul>
<li>Cross validation!</li>
</ul></li>
</ul>
<h4 id="linear-regression-probabilistic-view">Linear regression:
Probabilistic View</h4>
<h5 id="recap-probability-statistics">Recap: Probability &amp;
Statistics</h5>
<h6 id="the-multivariate-gaussian-distribution">The multivariate
Gaussian distribution</h6>
<p>A random vector <span class="math inline">\(X\)</span> is said to
have a multivariate normal (Gaussian) distribution with mean <span class="math inline">\(\mu\in\mathbb R^n\)</span> and covariance matrix
<span class="math inline">\(\Sigma \in S_{++}^n\)</span> <span class="math display">\[f_{X_1,X_2,\cdots,X_n}(x_1,x_2,\cdots,x_n)=\cfrac{1}{(2\pi)^\frac{n}{2}|\Sigma|^\frac{1}{2}}e^{-\frac{1}{2}(\pmb
x-\mu)^T\Sigma^{-1}(\pmb x-\mu)}\]</span> We write this as <span class="math inline">\(X\sim\mathcal N(\mu,\Sigma)\)</span>.</p>
<p><img src="/2023/02/17/Machine%20Learning/bfd5b6d0bc87a6d76c69c391e921870.png" srcset="/img/loading.gif" lazyload></p>
<p>When <span class="math inline">\(n=1\)</span> , it is a normal
(Gaussian) distribution <span class="math inline">\(\mathcal
N(\mu_1,\Sigma_{11})\)</span> . <span class="math display">\[f_{X_1}(x_1)=\cfrac{1}{(2\pi)^\frac{1}{2}|\Sigma_{11}|^\frac{1}{2}}e^{-\frac{1}{2}(x_1-\mu_1)^T\Sigma_{11}^{-1}(x_1-\mu_1)}\]</span>
##### Linear regression: Probabilistic View Assume the response <span class="math inline">\(Y\)</span> is given by a deterministic function
and an additive Gaussian noise. <span class="math display">\[y=\pmb\beta^T\pmb x+\epsilon,\text{ where
}\epsilon\sim N(0,\sigma^2)\]</span> - The linear regression estimator
is the maximum likelihood estimator of the data. - The likelihood
function <span class="math display">\[P(\pmb y|\pmb
X;\pmb\beta)=P(y_1|\pmb x_1;\pmb\beta)P(y_2|\pmb x_2;\pmb\beta)\cdots
P(y_m|\pmb x_m;\pmb\beta)=\prod_{i=1}^mP(y_i|\pmb
x_i;\pmb\beta)\]</span> - The log-likelihood function <span class="math display">\[\log P(\pmb y|\pmb X;\pmb\beta)=\log\prod_{i=1}^m
P(y_i|\pmb x_i;\pmb\beta)=\sum_{i=1}^m\log P(y_i|\pmb
x_i;\pmb\beta)\]</span> - Given dataset <span class="math inline">\((\pmb X,\pmb y)\)</span> , find <span class="math inline">\(\pmb\beta\)</span> that can maximize the
log-likelihood of <span class="math inline">\(\pmb y\)</span> . <span class="math display">\[\hat{\pmb\beta}=\text{arg}\max_{\pmb\beta}\log
P(\pmb y|\pmb X;\pmb\beta)=\text{arg}\max_{\pmb\beta}\sum_{i=1}^m\log
P(y_i|\pmb x_i;\pmb\beta)\]</span></p>
<p><span class="math display">\[\begin{align}
\hat{\pmb\beta}&amp;=\text{arg}\max_{\pmb\beta}\log P(\pmb y|\pmb
X;\pmb\beta)\\
&amp;=\text{arg}\max_{\pmb\beta}\sum_{i=1}^m\log P(y_i|\pmb
x_i;\pmb\beta)\\
&amp;=\text{arg}\max_{\pmb\beta}\sum_{i=1}^m\log\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\cfrac{(y_i-\pmb\beta^T\pmb
x_i)^2}{2\sigma^2}}
\Leftarrow P(y_i|\pmb
x_i;\pmb\beta)=\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\cfrac{(y_i-\pmb\beta^T\pmb
x_i)^2}{2\sigma^2}}\\
&amp;=\text{arg}\max_{\pmb\beta}-\sum_{i=1}^m\cfrac{(y_i-\pmb\beta^T\pmb
x_i)^2}{2\sigma^2}\\
&amp;=\text{arg}\min_{\pmb\beta}\sum_{i=1}^m\cfrac{(y_i-\pmb\beta^T\pmb
x_i)^2}{2\sigma^2}\\
&amp;=\text{arg}\min_{\pmb\beta}\sum_{i=1}^m(y_i-\pmb\beta^T\pmb x_i)^2
\Leftrightarrow MSE=\frac{1}{m}(\pmb y-\pmb X\pmb\beta)^T(\pmb  y-\pmb
X\pmb\beta)
\end{align}\]</span></p>
<h5 id="ridge-regression-probabilistic-view">Ridge regression:
Probabilistic View</h5>
<ul>
<li>Assume the response <span class="math inline">\(Y\)</span> is given
by a deterministic function and an additive Gaussian noise. <span class="math display">\[y=\pmb\beta^T\pmb x+\epsilon,\text{ where
}\epsilon\sim N(0,\sigma^2)\]</span></li>
<li>Suppose <span class="math inline">\(\pmb\beta\)</span> has the prior
<span class="math inline">\(p(\pmb\beta)=\mathcal N(0,\tau^2\pmb
I)\)</span></li>
<li>Ridge regression estimator is a MAP estimator with Gaussian prior
<span class="math display">\[\begin{align}
\hat{\pmb\beta}_{MAP}&amp;=\text{arg}\max\sum_{i=1}^m\log p(y_i|\pmb
x_i;\pmb\beta)+\log p(\pmb\beta)\\
P(y_i|\pmb
x_i;\pmb\beta)=\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\cfrac{(y_i-\pmb\beta^T\pmb
x_i)^2}{2\sigma^2}}
\Rightarrow &amp;=\text{arg}\max\sum_{i=1}^m-\cfrac{(y_i-\pmb\beta^T\pmb
x_i)^2}{2\sigma^2}-\cfrac{1}{2\tau^2}||\pmb\beta||_2^2
\Leftarrow
p(\pmb\beta)=\cfrac{e^{-\cfrac{1}{2\tau^2}\pmb\beta^T\pmb\beta}}{\sqrt{(2\pi\tau^2)^n}}\\
&amp;=\text{arg}\max_{\pmb\beta}\sum_{i=1}^m-(y_i-\pmb\beta^T\pmb
x_i)^2-\lambda||\pmb\beta||_2^2
\Leftarrow \lambda=\cfrac{\sigma^2}{\tau^2}\\
\text{Ridge regression}\Leftrightarrow
&amp;=\text{arg}\min_{\pmb\beta}\sum_{i=1}^m(y_i-\pmb\beta^T\pmb
x_i)^2+\lambda||\pmb\beta||_2^2
\end{align}\]</span> ### Classification</li>
<li>In many situation, the response variable is qualitative.</li>
<li>Classification is a process of predicting qualitative
responses.</li>
<li>Examples
<ul>
<li>Medical diagnosis: predict whether a patient is sick or healthy</li>
<li>Spam detection: predict whether an email is spam or not</li>
<li>Credit card fraud: predict whether a given credit card transaction
is fraud or not</li>
<li>Marketing: predict whether a given user will buy a product or
not</li>
</ul></li>
<li>Classifiers
<ul>
<li>Logistic regression</li>
<li>Support vector machine (SVM)</li>
<li>Naïve Bayesian</li>
<li>…</li>
</ul></li>
</ul>
<h4 id="logistic-regression">Logistic regression</h4>
<h5 id="why-not-linear-regression">Why not linear regression?</h5>
<p>Can we use linear regression to predict the probability of default?
<span class="math display">\[p(\pmb x)=\pmb w^T\pmb x+b\]</span> - The
probability of default could be negative when balances close to 0, and
could be bigger than 1 when the balances are large. - We must model
<span class="math inline">\(p(\pmb x)\)</span> as a function that gives
output between 0 and 1. <img src="/2023/02/17/Machine%20Learning/8238a887080cb52105e2da7c8321fab.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="logistic-function">Logistic function</h5>
<p><span class="math display">\[\sigma(z)=\cfrac{1}{1+e^{-z}}\]</span> -
bounded in (0,1) - <span class="math inline">\(\sigma(z)\rightarrow
1\)</span> when <span class="math inline">\(z \rightarrow
\infty\)</span> - <span class="math inline">\(\sigma(z)\rightarrow
0\)</span> when <span class="math inline">\(z\rightarrow
-\infty\)</span> - <span class="math inline">\(\sigma^\prime(z)=\sigma(z)(1-\sigma(z))\)</span></p>
<h6 id="binary-classification-using-logistic-function">Binary
classification using logistic function</h6>
<p><span class="math display">\[\begin{align}
p(y=1|\pmb x)=\sigma(\pmb w^T\pmb x+b)=\cfrac{1}{1+e^{-(\pmb w^T\pmb
x+b)}}\\
p(y=0|\pmb x)=1-\sigma(\pmb w^T\pmb x+b)=\cfrac{e^{-(\pmb w^T\pmb
x+b)}}{1+e^{-(\pmb w^T\pmb x+b)}}
\end{align}\]</span></p>
<h6 id="interpretation-of-logistic-regression">Interpretation of
logistic regression</h6>
<ul>
<li>Let <span class="math inline">\(p(\pmb x)=\cfrac{1}{1+e^{-(\pmb
w^T\pmb x+b)}}\)</span>, the odds(几率) is given by, <span class="math display">\[odds=\cfrac{p(\pmb x)}{1-p(\pmb x)}=e^{\pmb
w^T\pmb x+b}\]</span></li>
<li>If we take the logarithm on both sides, <span class="math display">\[\log\cfrac{p(\pmb x)}{1-p(\pmb x)}=\pmb w^T\pmb
x+b\]</span></li>
<li>The logit of an event's odds is predicted by a linear model.</li>
<li>One-unit increase in <span class="math inline">\(x_i\)</span>
changes the log-odds by <span class="math inline">\(w_i\)</span> holding
all other features fixed.</li>
</ul>
<h4 id="training-the-logistic-function">Training the logistic
function</h4>
<ul>
<li><p>We use the maximum likelihood estimation (MLE) to estimate <span class="math inline">\(b\)</span> and <span class="math inline">\(\pmb
w\)</span></p></li>
<li><p>Let <span class="math inline">\(\pmb \beta=[b;\pmb w]\)</span> ,
the likelihood function is given by <span class="math display">\[\begin{align}
\mathcal L(\pmb \beta) &amp;= \prod_{i:y_i=1}p(y_i=1|\pmb
x_1;\pmb\beta)\prod_{i^\prime:y_{i^\prime}=0}p(y_{i^\prime}=0|\pmb
x_{i^\prime};\pmb \beta)\\
&amp;=\prod_{i=1}^mp(y_i=1|\pmb x_i;\pmb\beta)^{y_i}p(y_1=0|\pmb
x_i;\pmb\beta)^{1-y_i}
\end{align}\]</span> <span class="math display">\[\begin{align}
p(y=1|\pmb x;\pmb\beta)=\sigma(\pmb\beta^T\pmb x)=\cfrac{1}{1+e^{-\pmb
\beta^T\pmb x}}\\
p(y=0|\pmb x;\pmb\beta)=1-\sigma(\pmb\beta^T\pmb
x)=\cfrac{e^{-\pmb\beta^T\pmb x}}{1+e^{-\pmb\beta^T\pmb x}}
\end{align}\]</span></p></li>
<li><p>We can equivalently minimize the <em>negative</em> log-likelihood
function <span class="math display">\[\begin{align}
\mathcal{l(\pmb\beta)}&amp;=-\log\mathcal{L(\pmb\beta)}\\
&amp;=\sum_{i=1}^m(-y_i\log p(y_i=1|\pmb x_i;\pmb\beta)-(1-y_i)\log
p(y_i=0|\pmb x_i;\pmb\beta))\\
&amp;=\sum_{i=1}^m(-y_i\pmb\beta^T\pmb x_i+\log(1+e^{\pmb\beta^T\pmb
x_i}))
\end{align}\]</span></p></li>
<li><p>Maximum Likelihood Estimation (MLE) <span class="math display">\[\begin{align}
\hat{\pmb\beta}&amp;=\text{arg}\max_{\pmb\beta}\mathcal L(\pmb\beta)\\
&amp;=\text{arg}\min_{\pmb\beta}\mathcal l(\pmb\beta)\\
&amp;=\text{arg}\min_{\pmb\beta}\sum_{i=1}^m(-y_i\pmb\beta^T\pmb
x_i+\log(1+e^{\pmb\beta^T\pmb x_i}))
\end{align}\]</span></p></li>
<li><p>There is no closed-form solution for the above optimization
problem</p></li>
<li><p>Fortunately, <span class="math inline">\(\mathcal
l(\pmb\beta)\)</span> is a convex function, we can apply the
<em>gradient descent method</em> to find the optimal solution.</p></li>
</ul>
<h5 id="gradient-descent-method-梯度下降法">Gradient descent method
(梯度下降法)</h5>
<ul>
<li>Recall: gradient is the direction of fastest increase</li>
<li>Updating rule <span class="math display">\[\pmb
\beta_{new}\leftarrow\pmb \beta_{old}-\eta\nabla\mathcal l(\pmb
\beta)\]</span></li>
<li>Search procedure
<ol type="1">
<li>Choose an initial value for <span class="math inline">\(\pmb\beta\)</span></li>
<li>Update <span class="math inline">\(\pmb\beta\)</span> iteratively
<ol type="1">
<li>Take the derivative of <span class="math inline">\(\mathcal
l(\pmb\beta)\)</span></li>
<li>Move the parameters in the direction of steepest descent</li>
</ol></li>
<li>Stop until <strong>convergence</strong> <img src="/2023/02/17/Machine%20Learning/6360a7faf1d39affd8e0985f07daa4a.png" srcset="/img/loading.gif" lazyload></li>
</ol></li>
</ul>
<h5 id="batch-gradient-descent-批量梯度下降">Batch gradient descent
(批量梯度下降)</h5>
<p><span class="math display">\[\begin{align}
\mathcal l(\pmb\beta)&amp;=\sum_{i=1}^m(-y_i\pmb\beta^T\pmb
x_i+\log(1+e^{\pmb\beta^T\pmb x_i}))\\
\nabla\mathcal l(\pmb\beta)&amp;=-\sum_{i=1}^m\pmb x_i(y_i-p(y_i=1|\pmb
x;\pmb\beta))
\end{align}\]</span> Update <span class="math inline">\(\pmb\beta\)</span> using the whole batch <span class="math display">\[\pmb\beta_{new}\leftarrow\pmb\beta_{old}+\eta\sum_{i=1}^m\pmb
x_i(y_i-p(y=1|\pmb x;\pmb\beta_{old}))\]</span> <img src="/2023/02/17/Machine%20Learning/a07605c556f05131fcb0eed75e8fe92.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="stochastic-gradient-descent-随机梯度下降">Stochastic gradient
descent (随机梯度下降)</h5>
<p><span class="math display">\[\begin{align}
\mathcal l(\pmb\beta)=\sum_{i=1}^m-y_i\pmb\beta^T\pmb
x_i+\log(1+e^{\pmb\beta^T\pmb x_i})=\sum_{i=1}^m\mathcal
l^{(i)}(\pmb\beta)\\
\text{where } \mathcal l^{(i)}(\pmb\beta)=-y_i\pmb\beta^T\pmb
x_i+\log(1+e^{\pmb\beta^T\pmb x_i})\\
\nabla\mathcal l^{(i)}(\pmb\beta)=-\pmb x_i(y_i-p(y_i=1|\pmb
x_i;\pmb\beta))
\end{align}\]</span> Update <span class="math inline">\(\pmb\beta\)</span> using single data sample <span class="math display">\[\pmb\beta_{new}\leftarrow\pmb\beta_{old}+\eta\pmb
x_i(y_i-p(y_i=1|\pmb x_i;\pmb\beta_{old}))\]</span> <img src="/2023/02/17/Machine%20Learning/c04750dc47e5320cec59791c2897602.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="mini-batch-gradient-descent-小批量梯度下降">Mini-batch gradient
descent (小批量梯度下降)</h5>
<ul>
<li>A combination of batch GD and stochastic GD
<ul>
<li>Split the whole dataset into <span class="math inline">\(K\)</span>
mini-batches <span class="math inline">\(\{1,2,3\cdots,K\}\)</span></li>
<li>For each mini-batch <span class="math inline">\(k\)</span>, perform
one-step BGD to minimize <span class="math display">\[\begin{align}
\mathcal l^k(\pmb\beta)&amp;=\sum_{i=1}^{n_k}-y_i\pmb\beta^T\pmb
x_i+\log(1+e^{\pmb\beta^T\pmb x_i})\\
\nabla\mathcal l^k(\pmb\beta)&amp;=-\sum_{i=1}^{n_k}\pmb
x_i(y_i-p(y_i=1|\pmb x_i;\pmb\beta))
\end{align}\]</span> Update <span class="math inline">\(\pmb\beta\)</span> using a mini-batch of data
samples <span class="math display">\[\pmb\beta_{new}\leftarrow\pmb\beta_{old}+\eta\sum_{i=1}^{n_k}\pmb
x_i(y_i-p(y=1|\pmb x_i;\pmb\beta_{old}))\]</span></li>
</ul></li>
</ul>
<h5 id="choice-of-learning-rate-学习速率">Choice of learning rate
(学习速率)</h5>
<p><img src="/2023/02/17/Machine%20Learning/4e3d1c319b8d2121651b340bb9e0379.png" srcset="/img/loading.gif" lazyload>
- Gradient descent would take long time to converge and can be very
slow</p>
<p><img src="/2023/02/17/Machine%20Learning/28249e9b76fddb1d028bad646f5b33e.png" srcset="/img/loading.gif" lazyload>
- May overshoot the minimum - The algorithm may fail to converge, or
even diverge</p>
<p><img src="/2023/02/17/Machine%20Learning/34a695cff223faf29380cb4cd036595.png" srcset="/img/loading.gif" lazyload>
- For sufficiently small <span class="math inline">\(\eta\)</span>, the
loss function should decrease on every iteration. - Plot the loss
function as a function of number of iterations to see if the learning
rate is appropriate</p>
<h4 id="making-prediction">Making prediction</h4>
<h5 id="label-decision-and-thresholds">Label decision and
thresholds</h5>
<ul>
<li><p>Logistic regression provides the probability of one event
belonging to a class or another</p></li>
<li><p>The final label of an instance is decided by setting a threshold
<span class="math inline">\(h\)</span> (typically 0.5) <span class="math display">\[\hat y=\begin{cases}
1&amp;,\ \ \ \ p(y=1|\pmb x)&gt; h\\
0&amp;,\ \ \ \ otherwise
\end{cases}\]</span></p></li>
<li><p>How to choose the threshold? | | Higher threshold | lower
threshold | |---|---|---| | False Negative (FN) | <span class="math inline">\(\uparrow\)</span> | <span class="math inline">\(\downarrow\)</span> | | False Positive (FP) |
<span class="math inline">\(\downarrow\)</span> | <span class="math inline">\(\uparrow\)</span> |</p></li>
<li><p>Don’t get confused by its name! It’s a classification rather than
regression algorithm</p></li>
</ul>
<h5 id="another-evaluation-measure-roc-curve">Another evaluation
measure: ROC curve</h5>
<p>Receiver Operating Characteristic (ROC) curve - ROC curve plots True
Positive rate vs. False Positive rate at different thresholds between 0
and 1 - It shows the trade-off between true-positive rate and
false-positive rate of classification algorithms - True Positive rate
(recall, 真正例率) <span class="math display">\[TPR=\cfrac{TP}{TP+FN}\]</span> - False Positive
rate (假正例率) <span class="math display">\[FPR=\cfrac{FP}{FP+TN}\]</span> <img src="/2023/02/17/Machine%20Learning/6a2e08f093e043bef56e9524d47d15f.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="roc-curve-and-auc-area-under-the-curve">ROC curve and AUC (area
under the curve)</h5>
<p><img src="/2023/02/17/Machine%20Learning/2beb86993fa4ae74f0081ec91c2c703.png" srcset="/img/loading.gif" lazyload>
- In this example, Classifier A is better than B, which is better than
random guessing</p>
<p><img src="/2023/02/17/Machine%20Learning/ab20f3bcdac435adab7fbe44748781b.png" srcset="/img/loading.gif" lazyload>
- Compare multiple models by a single number - Higher AUC will be the
better - However, it does not tell us the performance of the model for a
given threshold setting</p>
<h4 id="regularization-in-logistic-regression">Regularization in
logistic regression</h4>
<ul>
<li>Without regularization, the logistic regression would keep driving
the loss towards 0 in high dimensions.</li>
<li>L1 regularization <span class="math display">\[\mathcal
l(\pmb\beta)+E(\pmb\beta)=\mathcal
l(\pmb\beta)+\cfrac{\lambda}{2}\sum_{i=1}^n|\beta_i|\]</span></li>
<li>L2 regularization <span class="math display">\[\mathcal
l(\pmb\beta)+E(\pmb\beta)=\mathcal
l(\pmb\beta)+\cfrac{\lambda}{2}\sum_{i=1}^n\beta_i^2\]</span></li>
<li>Early stopping, that is, limiting the number of training steps or
the learning rate.</li>
</ul>
<h4 id="multiclass-classification-多分类">Multiclass classification
(多分类)</h4>
<ul>
<li>Email tagging: Work, Friends, Family, Hobby</li>
<li>Medical diagrams: Healthy, Cold, Flu</li>
<li>Weather: Sunny, Windy, Rainy <img src="/2023/02/17/Machine%20Learning/0dbd9f34a7dd01a3cf8fdff2267228b.png" srcset="/img/loading.gif" lazyload></li>
</ul>
<h5 id="multinomial-logistic-regression-多项逻辑回归">Multinomial
logistic regression (多项逻辑回归)</h5>
<ul>
<li>The label can take values from the set <span class="math inline">\(\{1,2,\cdots,K\}\)</span></li>
<li>The multinomial logistic regression model is given as,<span class="math display">\[\begin{align}
P(Y=k|x)&amp;=\cfrac{e^{\pmb\beta^T_k\pmb
x}}{1+\displaystyle\sum_{i=1}^{K-1}e^{\pmb\beta^T_i\pmb x}},\ \ \ \
k=1,2,\cdots,K-1\\
P(Y=K|\pmb
x)&amp;=\cfrac{1}{1+\displaystyle\sum_{i=1}^{K-1}e^{\pmb\beta_i^T\pmb
x}}\\
\end{align}\]</span></li>
<li>The parameters can be estimated by the MLE approach <span class="math display">\[\max_{\beta_k,k=1,2,\cdots,K-1}\sum_{i=1}^m\sum_{k=1}^Ky_i^k\log
P(y_i^k=1|\pmb x_i)\]</span></li>
</ul>
<h5 id="one-vs-rest-ovr">One-vs-Rest (OvR)</h5>
<p><img src="/2023/02/17/Machine%20Learning/3b345e6e23b44d137f2eef677b086a0.png" srcset="/img/loading.gif" lazyload>
##### One-vs-One (OvO) <img src="/2023/02/17/Machine%20Learning/163ee69801c5fd90f0db8434f12c0a0.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="lecture5_featureengineering">Lecture5_FeatureEngineering</h2>
<h3 id="introduction">Introduction</h3>
<h3 id="data-understanding">Data understanding</h3>
<h4 id="structured-versus-unstructured-data">Structured versus
unstructured data</h4>
<h4 id="quantitative-versus-qualitative-data">Quantitative versus
qualitative data</h4>
<h4 id="exploratory-data-analysis-descriptive-statistics-and-data-visualizations">Exploratory
data analysis: descriptive statistics and data visualizations</h4>
<h3 id="data-processing">Data processing</h3>
<h4 id="missing-values">Missing values</h4>
<h4 id="class-imbanlance">Class imbanlance</h4>
<h4 id="feature-scaling-discretization">Feature scaling &amp;
discretization</h4>
<h4 id="feature-encoding-text-data-representation">Feature encoding
&amp; text data representation</h4>
<h2 id="lecture7_neuralnetworks">Lecture7_NeuralNetworks</h2>
<h3 id="overview-of-neural-networks">Overview of neural networks</h3>
<h4 id="structure-of-neurons-in-human-brain">Structure of neurons in
human brain</h4>
<ul>
<li>The human brain can be described as a biological neural network—an
interconnected web of neurons transmitting elaborate patterns of
electrical signals.</li>
<li>Dendrites receive input signals and, based on those inputs, fire an
output signal via an axon.</li>
</ul>
<p><img src="/2023/02/17/Machine%20Learning/d6ac10786053cf27bd83808148d4005.png" srcset="/img/loading.gif" lazyload>
- Dendrite (树突): It receives signals from other neurons - Cell body:
It sums all the incoming signals to generate input - Axon (轴突): When
the sum reaches a threshold value, neuron fires and the signal travels
down the axon to the other neurons</p>
<h4 id="artificial-neurons">Artificial neurons</h4>
<ul>
<li>Artificial neurons (人工神经元) are the basic computing units of
information processing in an artificial neural network
(人工神经网络).</li>
<li>The dendrites carry the signals (inputs <span class="math inline">\(\pmb x\)</span>) to the cell body where they all
get summed (weighted sum of inputs)</li>
<li>Synaptic strength (突触强度) controls the strength of the influence
and direction (weights <span class="math inline">\(\pmb w\)</span>)</li>
<li>Cell body reaches certain threshold and sends a spike to the next
axon (activation function <span class="math inline">\(f\)</span>)</li>
<li>Axons pass signals (outputs <span class="math inline">\(\pmb
y\)</span>) to other neurons (nodes)</li>
</ul>
<h4 id="artificial-neural-networks-ann">Artificial Neural Networks
(ANN)</h4>
<ul>
<li>Artificial neural networks (ANN) are computing systems inspired by
the biological neural networks.</li>
<li>It is comprised of a network of artificial neurons (nodes).</li>
</ul>
<p><img src="/2023/02/17/Machine%20Learning/99505909a8ce803cd2585640c29ea03.png" srcset="/img/loading.gif" lazyload>
- The connections between units do not form a cycle. - The information
moves in only one direction.</p>
<ul>
<li>Artificial neural networks (ANN) is best used
<ul>
<li>For modeling highly nonlinear systems</li>
<li>When the model interpretability is not a key concern</li>
<li>When data is available incrementally and you wish to constantly
update the model</li>
</ul></li>
<li>Applications
<ul>
<li>Image recognition</li>
<li>Speech recognition</li>
<li>Natural language processing</li>
<li>Game AI</li>
<li>...</li>
</ul></li>
</ul>
<h4 id="history-of-neural-networks">History of neural networks</h4>
<ul>
<li><strong>The First wave</strong>
<ul>
<li>1943 McCulloch and Pitts proposed the McCulloch-Pitts neuron model
(M-P神经元)</li>
<li>1958 Rosenblatt introduced the simple single layer networks now
called Perceptrons (感知机).</li>
<li>1969 Minsky and Papert’s book Perceptrons demonstrated the
limitation of single layer perceptron, and almost the whole field went
into hibernation.</li>
</ul></li>
<li><strong>The Second wave</strong>
<ul>
<li>1986 The Back-Propagation learning algorithm (反向传播学习算法) for
Multi-Layer Perceptrons was rediscovered and the whole field took off
again.</li>
</ul></li>
<li><strong>The Third wave</strong>
<ul>
<li>2006 Deep (neural networks) Learning (深度学习) gains
popularity</li>
<li>2012 made significant break-through in many applications
<ul>
<li>AlexNet: Geoffrey Hinton, IIya Sutskever, Alex Krizhevsky</li>
<li>Google Brain: Jeff Dean, Andrew Ng</li>
</ul></li>
</ul></li>
</ul>
<h3 id="perceptron-model">Perceptron model</h3>
<ul>
<li>Rosenblatt’s single layer perceptron, 1958
<ul>
<li>The simplest feedforward neural network. It does not contain any
hidden layers.</li>
<li>It computes the weighted output, and uses a threshold activation
function to output a quantity.</li>
</ul></li>
</ul>
<p><img src="/2023/02/17/Machine%20Learning/31005d52795dd6bf9d6ccb279d59ceb.png" srcset="/img/loading.gif" lazyload>
- <em>Activation function (激活函数)</em> <span class="math display">\[f(z)=\begin{cases}
1 &amp;\text{if } z\geq 0\\
0 &amp;\text{otherwise}
\end{cases}\]</span> - Prediction <span class="math display">\[ \hat
y=f\left(\sum_iw_ix_i+b\right)\]</span></p>
<h4 id="training-the-perceptron-model">Training the Perceptron
model</h4>
<ul>
<li>Perceptron uses iterative algorithm to learn a correct set of
weights</li>
<li>Updating rule <span class="math display">\[\begin{align}
w_i\leftarrow w_i+\Delta w_i\\
\Delta w_i=\eta(y-\hat y)x_i
\end{align}\]</span>
<ul>
<li><span class="math inline">\(\eta\)</span> : Learning rate</li>
<li><span class="math inline">\(y\)</span> : Target output</li>
<li><span class="math inline">\(\hat y\)</span> : Perceptron output</li>
</ul></li>
<li>Rosenblatt proved the convergence of the learning algorithm, if
<ul>
<li>the training data is linearly separable</li>
<li><span class="math inline">\(\eta\)</span> is sufficiently small</li>
</ul></li>
</ul>
<h4 id="representation-of-boolean-functions-布尔函数">Representation of
Boolean functions (布尔函数)</h4>
<p><img src="/2023/02/17/Machine%20Learning/f7eb0da81ed0871c9b951828e1eef00.png" srcset="/img/loading.gif" lazyload>
- AND (<span class="math inline">\(x_1 \wedge x_2\)</span> , 与) - <span class="math inline">\(w_1=w_2=1,b=-1.5\)</span> - OR (<span class="math inline">\(x_1\vee x_2\)</span> , 或) - <span class="math inline">\(w_1=w_2=1,b=-0.5\)</span> - NOT (<span class="math inline">\(\overline{x_1}\)</span> , 非) - <span class="math inline">\(w_1=-0.6,w_2=0,b=0.5\)</span></p>
<h5 id="limitation-of-perceptron-model">Limitation of Perceptron
model</h5>
<p>XOR (<span class="math inline">\(\overline{x_1}\wedge
x_2+\overline{x_2}\vee x_1\)</span> , 异或)</p>
<table>
<thead>
<tr class="header">
<th>Input</th>
<th></th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(x_1\)</span></td>
<td><span class="math inline">\(x_2\)</span></td>
<td><span class="math inline">\(y\)</span></td>
</tr>
<tr class="even">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p><img src="/2023/02/17/Machine%20Learning/44f6ee7c2003c9bfdc6952ad6a82bb8.png" srcset="/img/loading.gif" lazyload>
- XOR is not linearly separable - Some elementary computations such as
XOR cannot be represented by Rosenblatt’s single layer perceptron</p>
<h5 id="solution---add-a-hidden-layer">Solution - Add a hidden
layer</h5>
<p><img src="/2023/02/17/Machine%20Learning/7932e2084b642ada22ad4a78f912adf.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="multi-layer-feedforward-neural-networks">Multi-layer feedforward
neural networks</h3>
<p>Input layer (输入层) - No computation performed - Pass information to
hidden layers Hidden layer (隐藏层) - Intermediate layer between input
and output layers - Activation function apply on those layers - Can be
multiple hidden layers Output layer (输出层) - Transferring information
to outside world <img src="/2023/02/17/Machine%20Learning/cf8051b1925bef0c1100e19ec7ecdc2.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/2023/02/17/Machine%20Learning/46bc95f08fe1f0d64f8f9a4733f00b0.png" srcset="/img/loading.gif" lazyload>
<span class="math display">\[\begin{align}
\alpha_j=\sum_{i=1}^dv_{ij}x_i\\
\beta_l=\sum_{j=1}^qw_{ji}b_j\\
\end{align}\]</span> Forward prediction <span class="math display">\[x=(x_1,x_2,\cdots,x_d)\overset{b_j=f(\alpha_j)=f(\sum_{i=1}^dv_{ij}x_i)}{\longrightarrow}b_j\overset{y_i=f(\beta_i)=f(\sum_{j=1}^qw_{ji}b_j)}{\longrightarrow}y_i\]</span></p>
<p>For the training sample <span class="math inline">\((\pmb x_k,\pmb
y_k)\)</span> , the square error loss is <span class="math display">\[E_k=\cfrac{1}{2}\sum_{t=1}^l(\hat
y_t^k-y_t^k)^2\]</span></p>
<p>The accumulated square error loss of dataset <span class="math inline">\(S=\{(\pmb x_k,\pmb y_k): 1,2,\cdots, m\}\)</span>
is <span class="math display">\[\begin{align}
E&amp;=\cfrac{1}{m}\sum_{k=1}^mE_k\\
&amp;=\cfrac{1}{2m}\sum_{k=1}^m\sum_{t=1}^l(\hat y_t^k-y_t^k)^2
\end{align}\]</span></p>
<h4 id="back-propagation-algorithm">Back-Propagation algorithm</h4>
<p><img src="/2023/02/17/Machine%20Learning/60f7ff4efd734ff604959c09c02b128.png" srcset="/img/loading.gif" lazyload>
<span class="math display">\[\begin{align}
\alpha_j=\sum_{i=1}^dv_{ij}x_i\\
\beta_l=\sum_{j=1}^qw_{jl}b_j
\end{align}\]</span> - Use <strong>gradient descent method</strong> to
train the neural network - Updating rules <span class="math display">\[\begin{align}
\pmb w\leftarrow \pmb w+\Delta \pmb w\\
\pmb v\leftarrow \pmb v+\Delta \pmb v\\
\end{align}\]</span> - Learning <span class="math inline">\(\pmb
w\)</span> <span class="math display">\[\begin{align}
\pmb w&amp;\leftarrow \pmb w+\Delta \pmb w\\
\Delta w_{ji}&amp;=-\eta\cfrac{\partial E_k}{\partial w_{jl}}\\
\cfrac{\partial E_k}{\partial w_{jl}}&amp;=\cfrac{\partial
E_k}{\partial\hat y_l^k}\cdot\cfrac{\partial\hat
y_l^k}{\partial\beta_l}\cdot\cfrac{\partial\beta_l}{\partial w_{jl}}\ \
\ \ \text{(Chain rule)}\\
\\
\cfrac{\partial E_k}{\partial\hat y_l^k}&amp;=(\hat y_l^k-y_l^k)\\
\\
\cfrac{\partial\hat y_l^k}{\partial\beta_l}&amp;=f^\prime(\beta_l)\\
&amp;=f(\beta_l)(1-f(\beta_l))\\
&amp;=\hat y_l^k(1-\hat y_l^k)\\
\\
\cfrac{\partial\beta_l}{\partial w_{jl}}&amp;=b_j\\
\\
\cfrac{\partial E_k}{\partial w_{jl}}&amp;=b_j(\hat y_l^k-y_l^k)\hat
y_l^k(1-\hat y_l^k)\\
\\
\Delta w_{jl}&amp;=-\eta\cfrac{\partial E_k}{\partial w_{jl}}=-\eta
b_j(\hat y_l^k-y_l^k)\hat y_l^k(1-\hat y_l^k)=\eta b_jg_l\\
g_l&amp;=\hat y_l^k(1-\hat y_l^k)(y_l^k-\hat y_l^k)\ \ \ \
\text{(&quot;error&quot; at node $l$)}
\end{align}\]</span></p>
<p><code>Input: learning rate 𝜂 and data set 𝑆 = &#123;(𝒙_𝑘, 𝒚_𝑘, 𝑘=1,2, … ,𝑚&#125;</code>
<code>Initialize: 𝒘, 𝒗</code>
<code>Procedure: Repeat until convergence</code>
<code>For training sample k = 1,2,…,m do</code>
<code>Calculate predictions \hat 𝒚_𝑘 %Feedforward prediction</code>
<code>Calculate 𝑔_𝑡,𝑡=1,2, … 𝑙 %Back-propagate errors</code>
<code>Calculate 𝑒_𝑗,𝑗=1,2, … 𝑞 %Back-propagate errors</code>
<code>Calculate Δ𝒘, Δ𝒗       %Calculate weights gradients</code>
<code>Update 𝒘, 𝒗            %Update weights</code></p>
<p><img src="/2023/02/17/Machine%20Learning/9bc96b20dd2d9a83b34e6770feefa63.png" srcset="/img/loading.gif" lazyload>
##### An calculation example <img src="/2023/02/17/Machine%20Learning/cba0fc4f531383fa7eee44fe02e9de0.png" srcset="/img/loading.gif" lazyload> Assume
that the neurons have a Sigmoid activation function: 1. Perform a
forward pass on the network 2. Perform a reverse pass (training) once
with target = 0.5 and learning rate = 1 3. Perform a further forward
pass and comment on the results</p>
<ol type="i">
<li>Input to the top neuron: <span class="math inline">\((0.35\times0.1)
+ (0.9 \times 0.8) = 0.755\)</span>. <span class="math inline">\(Out_1 =
0.68\)</span> Input to the bottom neuron: <span class="math inline">\((0.9 \times 0.6) + (0.35 \times 0.4) =
0.68\)</span>. <span class="math inline">\(Out_2 = 0.6637\)</span> Input
to the final neuron: <span class="math inline">\((0.3 \times 0.68) +
(0.9 \times 0.6637) = 0.80\)</span>. <span class="math inline">\(Output
= 0.69\)</span></li>
<li>Output error <span class="math inline">\(g = (y − \hat y)(1 − \hat
y)\hat y = (0.5 − 0.69)(1 − 0.69)0.69 = −0.0406\)</span></li>
</ol>
<p>Errors for the hidden layer <span class="math inline">\(e_1 = 𝑔
\times 𝑤_1 \times (1 − out_1) \times out_1 = −0.0406 \times 0.3 \times
(1 − 0.68) \times 0.68 = −0.00265\)</span> <span class="math inline">\(e_2 = g \times w_2 \times (1 − out_2) \times out_2
= −0.0406 \times 0.9 \times (1 − 0.6637) \times 0.6637 =
−0.008156\)</span> Update the weights for the output layer <span class="math inline">\(w_1 \leftarrow w_1 +(\eta \times g \times out_1) =
0.3 + (−0.0406 \times 0.68) = 0.272392\)</span> <span class="math inline">\(w_2 \leftarrow w_2 + (\eta \times g \times out_2)
= 0.9 + (−0.0406 × 0.6637) = 0.87305\)</span></p>
<p>%% Update the weights for the hidden layer 𝑤𝑤3 ← 𝑤𝑤3 + 𝜂𝜂 × 𝑒𝑒1 × 𝑥𝑥1
= 0.1 + −0.00265 × 0.35 = 0.0991 𝑤𝑤4 ← 𝑤𝑤4 + 𝜂𝜂 × 𝑒𝑒1 × 𝑥𝑥2 = 0.8 +
−0.00265 × 0.9 = 0.7976 𝑤𝑤5 ← 𝑤𝑤5 + 𝜂𝜂 × 𝑒𝑒2 × 𝑥𝑥1 = 0.4 + −0.008156 ×
0.35 = 0.3971 𝑤𝑤6 ← 𝑤𝑤6 + 𝜂𝜂 × 𝑒𝑒2 × 𝑥𝑥2 = 0.6 + −0.008156 × 0.9 =
0.5927 (iii) Input to the top neuron: 0.35 × 0.0991 + 0.9 × 0.7976 =
0.7525. Out1 = 0.6797 Input to the bottom neuron: 0.9 × 0.5927 + 0.35 ×
0.3971 = 0.6724. Out2 = 0.662 Input to the final neuron: 0.272392 ×
0.6797 + 0.87305 × 0.662 = 0.7631. Output = 0.682 %%</p>
<h3 id="activation-functions">Activation functions</h3>
<h3 id="issues-of-training-neural-networks">Issues of training neural
networks</h3>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/2022S/" class="category-chain-item">2022S</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" class="print-no-link">#课程笔记</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>机器学习-课程笔记 (TBC)</div>
      <div>http://hiryan23.github.io/2023/02/17/Machine Learning/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Ruiyang He</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年2月17日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2023年6月1日</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/02/19/%E5%AF%B9chatgpt%E7%9A%84%E6%AC%A2%E5%91%BC%E6%98%AF%E4%BA%BA%E5%AF%B9%E5%BC%82%E5%8C%96%E7%9A%84%E5%8F%8D%E6%8A%97%E2%80%94%E2%80%94%E8%AE%BAai%E4%BB%A3%E5%86%99%E8%AE%BA%E6%96%87%E7%9A%84%E8%A7%A3%E6%94%BE%E6%80%A7%E4%B8%8E%E5%BC%80%E6%94%BE%E6%80%A7/" title="对chatgpt的欢呼是人对异化的反抗——论ai代写论文的解放性与开放性">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">对chatgpt的欢呼是人对异化的反抗——论ai代写论文的解放性与开放性</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/02/17/%E5%8D%9A%E5%BC%88%E8%AE%BA/" title="博弈论-课程笔记 (TBC)">
                        <span class="hidden-mobile">博弈论-课程笔记 (TBC)</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      

    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
