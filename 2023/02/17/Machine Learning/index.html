<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>机器学习-课程笔记 | Hiryan的世界</title>
  <meta name="description" content="本笔记整理自安泰经济与管理学院2022年春季学期课程BUSS2505-02机器学习，授课人是李成璋老师。  Lecture1_IntroductionApplicationsMachine Learning is all around us Game AI Deep blue, IBM AlphaGo, Deep Mind Deepstack, CMU &amp; Facebook AI   R">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-课程笔记">
<meta property="og:url" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/index.html">
<meta property="og:site_name" content="Hiryan的世界">
<meta property="og:description" content="本笔记整理自安泰经济与管理学院2022年春季学期课程BUSS2505-02机器学习，授课人是李成璋老师。  Lecture1_IntroductionApplicationsMachine Learning is all around us Game AI Deep blue, IBM AlphaGo, Deep Mind Deepstack, CMU &amp; Facebook AI   R">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hiryan23.github.io/pic/9538373a69fb3836595f01495607834.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/bfec802ec5619a7f6b5ef92fd832d31.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/6cf69ccc0b5e18ab05c080df1515205.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/110a43955525c0ad9a62c039eb3876a.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/dce728e8f2d0819688a148ebaf686f9.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/11af398aea2369112afe0fe46f5d255.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/379ac1b116f3c5612b0e6a1f3df25ea.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/b885ffb38cf90c077ad79abb079e005.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/57c1cdf60aceade4f79afcdcf5781c5.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/b9432d6edc99c82146c5ca340b6fd65.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/8c43075b2094ec2db8c526f1ea28d14.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/a99b5dbe4e510c276edb458034cea64.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/f70b694bdd89dce57b1ed3575e59c22.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/8918735b6e8c752e2d4ae0af01c4e03.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/7ba8a7a66050778027f6549fa41cb2a.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/10adcb662406252426c360458bf636b.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/8181efdc993f5c23481a08ae4ac524b.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/e4f39df5e0cf02295ed0adf94236c82.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/4010e1c4642202ac977ba16ffc9a0ee.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/b0f3730daa3d994524a12413137e863.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/1a58a26150be5c680563a0eb25d7b8a.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/30dc5adbbd599316a0cb17da5726c8c.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/f4eec853b3f60729c5dd34dc7b4acc9.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/0ad30f160f2e7ecab62f7354a1c4616.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/a28e0bebc2515f58f224263e5cbedf5.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/dd55e1c530692fa6810d9b314c15f84.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/6c170feb2750e49af1c23e63965b608.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/132f7b5bbcbb9caecf14d6a7f6bf2f0.png">
<meta property="og:image" content="http://hiryan23.github.io/2023/02/17/Machine%20Learning/8fce1beb900d9682237c079542b44f5.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/5192779dc145eba035b997cf1a76fcc.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/876fcd4c99289d7d14c2494c352de58.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/4cda0b999d6f3752153a5f48f5b45fa.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/8238a887080cb52105e2da7c8321fab.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/d6ac10786053cf27bd83808148d4005.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/99505909a8ce803cd2585640c29ea03.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/31005d52795dd6bf9d6ccb279d59ceb.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/f7eb0da81ed0871c9b951828e1eef00.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/44f6ee7c2003c9bfdc6952ad6a82bb8.png">
<meta property="og:image" content="http://hiryan23.github.io/pic/7932e2084b642ada22ad4a78f912adf.png">
<meta property="article:published_time" content="2023-02-17T15:12:03.271Z">
<meta property="article:modified_time" content="2023-04-04T08:57:59.866Z">
<meta property="article:author" content="Ruiyang He">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hiryan23.github.io/pic/9538373a69fb3836595f01495607834.png">
  <!-- Canonical links -->
  <link rel="canonical" href="http://hiryan23.github.io/2023/02/17/Machine%20Learning/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Hiryan的世界" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" rel="stylesheet">
  
  
  
  
<meta name="generator" content="Hexo 6.3.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Ruiyang He</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">上海交通大学 本科生</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shanghai, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">友链</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">关于</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/hiryan23" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>你永远不该评判自己不了解的事物。</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E5%8D%9A%E5%BC%88%E8%AE%BA/">2022S博弈论</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E5%8F%91%E5%B1%95%E7%BB%8F%E6%B5%8E%E5%AD%A6/">2022S发展经济学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">2022S机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/">2022S计量经济学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E8%B4%A2%E5%8A%A1%E7%AE%A1%E7%90%86/">2022S财务管理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E8%B4%A7%E5%B8%81%E9%87%91%E8%9E%8D%E5%AD%A6/">2022S货币金融学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2022S%E9%87%91%E8%9E%8D%E5%AD%A6%E5%8E%9F%E7%90%86/">2022S金融学原理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/2023S%E9%A9%AC%E5%85%8B%E6%80%9D%E4%B8%BB%E4%B9%89%E5%8E%9F%E7%90%86/">2023S马克思主义原理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0/">分享文章</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%97%A5%E8%AE%B0%E3%80%81%E6%9D%82%E6%84%9F%E4%B8%8E%E8%87%AA%E6%88%91%E6%89%B9%E5%88%A4/">日记、杂感与自我批判</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82/">杂</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E2%80%9C%E6%84%8F%E8%AF%86%E6%B5%81%E2%80%9D/" rel="tag">“意识流”</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E5%B1%95%E7%BB%8F%E6%B5%8E%E5%AD%A6/" rel="tag">发展经济学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E7%8E%B0%E4%BB%A3%E4%B8%BB%E4%B9%89%E5%93%B2%E5%AD%A6/" rel="tag">后现代主义哲学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%8F%E8%A7%82%E9%87%91%E8%9E%8D%E5%AD%A6/" rel="tag">宏观金融学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B0%8F%E8%AF%B4-%E6%95%A3%E6%96%87%E7%AD%89/" rel="tag">小说\散文等</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/" rel="tag">微观经济学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BE%AE%E8%A7%82%E9%87%91%E8%9E%8D%E5%AD%A6/" rel="tag">微观金融学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%BD%E8%BD%A6%E6%8E%A7%E5%88%B6/" rel="tag">汽车控制</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A4%BE%E4%BC%9A%E5%88%86%E6%9E%90/" rel="tag">社会分析</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/" rel="tag">计量经济学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%87%91%E8%9E%8D%E5%AD%A6/" rel="tag">金融学</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A9%AC%E5%8E%9F/" rel="tag">马原</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/%E2%80%9C%E6%84%8F%E8%AF%86%E6%B5%81%E2%80%9D/" style="font-size: 13px;">“意识流”</a> <a href="/tags/%E5%8F%91%E5%B1%95%E7%BB%8F%E6%B5%8E%E5%AD%A6/" style="font-size: 13px;">发展经济学</a> <a href="/tags/%E5%90%8E%E7%8E%B0%E4%BB%A3%E4%B8%BB%E4%B9%89%E5%93%B2%E5%AD%A6/" style="font-size: 13px;">后现代主义哲学</a> <a href="/tags/%E5%AE%8F%E8%A7%82%E9%87%91%E8%9E%8D%E5%AD%A6/" style="font-size: 13px;">宏观金融学</a> <a href="/tags/%E5%B0%8F%E8%AF%B4-%E6%95%A3%E6%96%87%E7%AD%89/" style="font-size: 13px;">小说\散文等</a> <a href="/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/" style="font-size: 13px;">微观经济学</a> <a href="/tags/%E5%BE%AE%E8%A7%82%E9%87%91%E8%9E%8D%E5%AD%A6/" style="font-size: 13px;">微观金融学</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 13px;">机器学习</a> <a href="/tags/%E6%B1%BD%E8%BD%A6%E6%8E%A7%E5%88%B6/" style="font-size: 13px;">汽车控制</a> <a href="/tags/%E7%A4%BE%E4%BC%9A%E5%88%86%E6%9E%90/" style="font-size: 13px;">社会分析</a> <a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 14px;">笔记</a> <a href="/tags/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/" style="font-size: 13px;">计量经济学</a> <a href="/tags/%E9%87%91%E8%9E%8D%E5%AD%A6/" style="font-size: 13px;">金融学</a> <a href="/tags/%E9%A9%AC%E5%8E%9F/" style="font-size: 13px;">马原</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">三月 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a><span class="archive-list-count">13</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%97%A5%E8%AE%B0%E3%80%81%E6%9D%82%E6%84%9F%E4%B8%8E%E8%87%AA%E6%88%91%E6%89%B9%E5%88%A4/">日记、杂感与自我批判</a>
              </p>
              <p class="item-title">
                <a href="/2023/03/14/%E7%94%9F%E7%97%85%E4%BA%86/" class="title">2023.03.14 生病了</a>
              </p>
              <p class="item-date">
                <time datetime="2023-03-14T14:09:52.155Z" itemprop="datePublished">2023-03-14</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%88%86%E4%BA%AB%E6%96%87%E7%AB%A0/">分享文章</a>
              </p>
              <p class="item-title">
                <a href="/2023/02/28/%E6%9C%80%E5%90%8E%E4%B8%80%E5%90%8D/" class="title">最后一名 | 马塞尔·埃梅</a>
              </p>
              <p class="item-date">
                <time datetime="2023-02-28T13:04:05.126Z" itemprop="datePublished">2023-02-28</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/2023S%E9%A9%AC%E5%85%8B%E6%80%9D%E4%B8%BB%E4%B9%89%E5%8E%9F%E7%90%86/">2023S马克思主义原理</a>
              </p>
              <p class="item-title">
                <a href="/2023/02/27/%E9%A9%AC%E5%85%8B%E6%80%9D%E4%B8%BB%E4%B9%89%E5%8E%9F%E7%90%86/" class="title">马克思主义原理-重点梳理</a>
              </p>
              <p class="item-date">
                <time datetime="2023-02-27T06:20:15.125Z" itemprop="datePublished">2023-02-27</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%9D%82/">杂</a>
              </p>
              <p class="item-title">
                <a href="/2023/02/22/%E4%B8%80%E9%81%93can%E6%8A%A5%E6%96%87%E8%AF%BB%E5%8F%96%E7%9A%84%E9%97%AE%E9%A2%98/" class="title">一道can报文读取的问题</a>
              </p>
              <p class="item-date">
                <time datetime="2023-02-22T05:44:19.256Z" itemprop="datePublished">2023-02-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%97%A5%E8%AE%B0%E3%80%81%E6%9D%82%E6%84%9F%E4%B8%8E%E8%87%AA%E6%88%91%E6%89%B9%E5%88%A4/">日记、杂感与自我批判</a>
              </p>
              <p class="item-title">
                <a href="/2023/02/22/%E4%BB%8E%E7%8F%AD%E4%BC%9A%E5%8F%91%E8%A8%80%E5%BC%95%E5%87%BA%E7%9A%84%E6%80%9D%E8%80%83/" class="title">2023.02.22 从班会发言引出的思考</a>
              </p>
              <p class="item-date">
                <time datetime="2023-02-22T00:14:36.308Z" itemprop="datePublished">2023-02-22</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
  <aside class="sidebar sidebar-toc   in  " id="collapseToc" itemscope
    itemtype="http://schema.org/WPSideBar">
    <div class="slimContent">
      <nav id="toc" class="article-toc">
        <h3 class="toc-title">
          文章目录
        </h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture1-Introduction"><span class="toc-text">Lecture1_Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Applications"><span class="toc-text">Applications</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Machine-Learning-is-all-around-us"><span class="toc-text">Machine Learning is all around us</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Economical-impact-of-Machine-Learning"><span class="toc-text">Economical impact of Machine Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Trends-of-Machine-Learning"><span class="toc-text">Trends of Machine Learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition"><span class="toc-text">Definition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#What-is-Machine-Learning"><span class="toc-text">What is Machine Learning?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Programming-vs-Machine-Learning"><span class="toc-text">Programming vs Machine Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Brief-History-of-Machine-Learning"><span class="toc-text">Brief History of Machine Learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Categories"><span class="toc-text">Categories</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Supervised-Learning"><span class="toc-text">Supervised Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Process-of-Supervised-Learning"><span class="toc-text">Process of Supervised Learning</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Unsupervised-Learning"><span class="toc-text">Unsupervised Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Some-Machine-Learning-algorithms"><span class="toc-text">Some Machine Learning algorithms</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture2-MathForML"><span class="toc-text">Lecture2_MathForML</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Notations"><span class="toc-text">Notations</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Probability-amp-Statistics"><span class="toc-text">Probability &amp; Statistics</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sample-space-Omega"><span class="toc-text">Sample space($\Omega$)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Event-E"><span class="toc-text">Event($E$)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Event-space-mathcal-F"><span class="toc-text">Event space($\mathcal F$)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Axioms-of-probability"><span class="toc-text">Axioms of probability</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Random-variable-RV"><span class="toc-text">Random variable (RV)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Probability-distribution"><span class="toc-text">Probability distribution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Joint-distribution"><span class="toc-text">Joint distribution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conditional-probability"><span class="toc-text">Conditional probability</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Product-rule"><span class="toc-text">Product rule</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bayes%E2%80%99-rule"><span class="toc-text">Bayes’ rule</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Marginal-probability"><span class="toc-text">Marginal probability</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Independence"><span class="toc-text">Independence</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Expectation"><span class="toc-text">Expectation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conditional-expectation"><span class="toc-text">Conditional expectation</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Law-of-total-expectation"><span class="toc-text">Law of total expectation</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Variance"><span class="toc-text">Variance</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Covariance"><span class="toc-text">Covariance</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Estimation-of-Parameters"><span class="toc-text">Estimation of Parameters</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Maximum-Likelihood-Estimation-MLE"><span class="toc-text">Maximum Likelihood Estimation (MLE)</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Example-of-MLE"><span class="toc-text">Example of MLE</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Maximum-A-Posteriori-Estimation-MAP"><span class="toc-text">Maximum A Posteriori Estimation (MAP)</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Example-of-MAP"><span class="toc-text">Example of MAP</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-Algebra"><span class="toc-text">Linear Algebra</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Vector"><span class="toc-text">Vector</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Matrix"><span class="toc-text">Matrix</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Vector-algorithmic"><span class="toc-text">Vector algorithmic</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Vector-norm"><span class="toc-text">Vector norm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Matrix-arithmetric"><span class="toc-text">Matrix arithmetric</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transpose"><span class="toc-text">Transpose</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Symmetric-matrix"><span class="toc-text">Symmetric matrix</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inverse-of-a-matrix"><span class="toc-text">Inverse of a matrix</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Semidefinite-matrices"><span class="toc-text">Semidefinite matrices</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Back-to-Probability-amp-Statistics"><span class="toc-text">Back to Probability &amp; Statistics</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Random-vector"><span class="toc-text">Random vector</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Covariance-matrix"><span class="toc-text">Covariance matrix</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Matrix-calculus"><span class="toc-text">Matrix calculus</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimization"><span class="toc-text">Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#What-is-optimization"><span class="toc-text">What is optimization?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Local-minima-and-global-minima"><span class="toc-text">Local minima and global minima</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Convex-set"><span class="toc-text">Convex set</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Convex-Concave-function"><span class="toc-text">Convex (Concave) function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#First-order-convexity-condition"><span class="toc-text">First-order convexity condition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Second-order-convexity-condition"><span class="toc-text">Second-order convexity condition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Examples-of-convex-functions"><span class="toc-text">Examples of convex functions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Convex-optimization-problem"><span class="toc-text">Convex optimization problem</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#It%E2%80%99s-nice-to-be-convex"><span class="toc-text">It’s nice to be convex!</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Optimization-methods"><span class="toc-text">Optimization methods</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture3-BasicInML"><span class="toc-text">Lecture3_BasicInML</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Generalization-ability-%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="toc-text">Generalization ability (泛化能力)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Underfitting-and-overfitting"><span class="toc-text">Underfitting and overfitting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Basic-terms"><span class="toc-text">Basic terms</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Loss-function"><span class="toc-text">Loss function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Generalization-error"><span class="toc-text">Generalization error</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Generalization-error-%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-text">Generalization error (泛化误差)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Test-error-%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE"><span class="toc-text">Test error (测试误差)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Training-error-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE"><span class="toc-text">Training error (训练误差)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Learning-objective"><span class="toc-text">Learning objective</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Training-error-vs-test-error"><span class="toc-text">Training error vs test error</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Performance-metrics"><span class="toc-text">Performance metrics</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Classification"><span class="toc-text">Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Performance-metrics-for-classification"><span class="toc-text">Performance metrics for classification</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Confusion-matrix"><span class="toc-text">Confusion matrix</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Accuracy"><span class="toc-text">Accuracy</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Limitation-of-Accuracy"><span class="toc-text">Limitation of Accuracy</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Precision-%E6%9F%A5%E5%87%86%E7%8E%87"><span class="toc-text">Precision(查准率)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Recall-%E6%9F%A5%E5%85%A8%E7%8E%87"><span class="toc-text">Recall(查全率)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#F-1-score"><span class="toc-text">$F_1$ score</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Regression"><span class="toc-text">Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Performance-metrics-for-regression"><span class="toc-text">Performance metrics for regression</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bias-variance-decomposition"><span class="toc-text">Bias-variance decomposition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-selection"><span class="toc-text">Model selection</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Principle-of-Occam%E2%80%99s-razor-%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80"><span class="toc-text">Principle of Occam’s razor (奥卡姆剃刀)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Regularization"><span class="toc-text">Regularization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hyperparameter-%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-text">Hyperparameter(超参数)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Validation-strategy"><span class="toc-text">Validation strategy</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hold-out-validation"><span class="toc-text">Hold-out validation</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Cross-validation"><span class="toc-text">Cross validation</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Leave-one-out-cross-validation"><span class="toc-text">Leave-one-out cross validation</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Which-validation-strategy-to-use"><span class="toc-text">Which validation strategy to use?</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture4-LinearModels"><span class="toc-text">Lecture4_LinearModels</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-Regression"><span class="toc-text">Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Motivating-example"><span class="toc-text">Motivating example</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Simple-linear-regression"><span class="toc-text">Simple linear regression</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Estimating-the-parameters"><span class="toc-text">Estimating the parameters</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multivariate-linear-regression"><span class="toc-text">Multivariate linear regression</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Ridge-regression"><span class="toc-text">Ridge regression</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Regularized-linear-regression"><span class="toc-text">Regularized linear regression</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Regularization-1"><span class="toc-text">Regularization</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Linear-regression-Probabilistic-View"><span class="toc-text">Linear regression: Probabilistic View</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Recap-Probability-amp-Statistics"><span class="toc-text">Recap: Probability &amp; Statistics</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#The-multivariate-Gaussian-distribution"><span class="toc-text">The multivariate Gaussian distribution</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Linear-regression-Probabilistic-View-1"><span class="toc-text">Linear regression: Probabilistic View</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Ridge-regression-Probabilistic-View"><span class="toc-text">Ridge regression: Probabilistic View</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-1"><span class="toc-text">Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Logistic-regression"><span class="toc-text">Logistic regression</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Why-not-linear-regression"><span class="toc-text">Why not linear regression?</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Logistic-function"><span class="toc-text">Logistic function</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Binary-classification-using-logistic-function"><span class="toc-text">Binary classification using logistic function</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Interpretation-of-logistic-regression"><span class="toc-text">Interpretation of logistic regression</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Training-the-logistic-function"><span class="toc-text">Training the logistic function</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Gradient-descent-method-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-text">Gradient descent method (梯度下降法)</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture5-FeatureEngineering"><span class="toc-text">Lecture5_FeatureEngineering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-understanding"><span class="toc-text">Data understanding</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Structured-versus-unstructured-data"><span class="toc-text">Structured versus unstructured data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Quantitative-versus-qualitative-data"><span class="toc-text">Quantitative versus qualitative data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Exploratory-data-analysis-descriptive-statistics-and-data-visualizations"><span class="toc-text">Exploratory data analysis: descriptive statistics and data visualizations</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-processing"><span class="toc-text">Data processing</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Missing-values"><span class="toc-text">Missing values</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Class-imbanlance"><span class="toc-text">Class imbanlance</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Feature-scaling-amp-discretization"><span class="toc-text">Feature scaling &amp; discretization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Feature-encoding-amp-text-data-representation"><span class="toc-text">Feature encoding &amp; text data representation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture7-NeuralNetworks"><span class="toc-text">Lecture7_NeuralNetworks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Overview-of-neural-networks"><span class="toc-text">Overview of neural networks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Structure-of-neurons-in-human-brain"><span class="toc-text">Structure of neurons in human brain</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Artificial-neurons"><span class="toc-text">Artificial neurons</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Artificial-Neural-Networks-ANN"><span class="toc-text">Artificial Neural Networks (ANN)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#History-of-neural-networks"><span class="toc-text">History of neural networks</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Perceptron-model"><span class="toc-text">Perceptron model</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Training-the-Perceptron-model"><span class="toc-text">Training the Perceptron model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Representation-of-Boolean-functions-%E5%B8%83%E5%B0%94%E5%87%BD%E6%95%B0"><span class="toc-text">Representation of Boolean functions (布尔函数)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Limitation-of-Perceptron-model"><span class="toc-text">Limitation of Perceptron model</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Solution-Add-a-hidden-layer"><span class="toc-text">Solution - Add a hidden layer</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-layer-feedforward-neural-networks"><span class="toc-text">Multi-layer feedforward neural networks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Back-Propagation-algorithm"><span class="toc-text">Back-Propagation algorithm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Activation-functions"><span class="toc-text">Activation functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Issues-of-training-neural-networks"><span class="toc-text">Issues of training neural networks</span></a></li></ol></li></ol>
      </nav>
    </div>
  </aside>
  
<main class="main" role="main">
  <div class="content">
  <article id="post-Machine Learning" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
    
          <h1 class="article-title" itemprop="name">
            机器学习-课程笔记
          </h1>
          
            
      
      <div class="article-meta">
        <span class="article-date">
	<i class="icon icon-calendar"></i>
	<a href="/2023/02/17/Machine%20Learning/" class="article-date">
		<time datetime="2023-02-17T15:12:03.271Z" itemprop="datePublished">
			2023-02-17
		</time>
	</a>
</span>

<span class="article-date">
	<i class="icon icon-calendar-check"></i>
	<a href="/2023/02/17/Machine%20Learning/" class="article-date">
		<time datetime="2023-04-04T08:57:59.866Z" itemprop="dateUpdated">
			2023-04-04
		</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/2022S%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">2022S机器学习</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a>, <a class="article-tag-link-link" href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a>
  </span>


        
	<span class="article-read hidden-xs">
		<i class="icon icon-eye-fill" aria-hidden="true"></i>
		<span id="busuanzi_container_page_pv">
			<span id="busuanzi_value_page_pv"></span>
		</span>
	</span>

	
		
        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2023/02/17/Machine%20Learning/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <blockquote>
<p><em>本笔记整理自安泰经济与管理学院2022年春季学期课程BUSS2505-02机器学习，授课人是李成璋老师。</em></p>
</blockquote>
<h2 id="Lecture1-Introduction"><a href="#Lecture1-Introduction" class="headerlink" title="Lecture1_Introduction"></a>Lecture1_Introduction</h2><h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><h4 id="Machine-Learning-is-all-around-us"><a href="#Machine-Learning-is-all-around-us" class="headerlink" title="Machine Learning is all around us"></a>Machine Learning is all around us</h4><ul>
<li>Game AI<ul>
<li>Deep blue, IBM</li>
<li>AlphaGo, Deep Mind</li>
<li>Deepstack, CMU &amp; Facebook AI</li>
</ul>
</li>
<li>Robot<ul>
<li>SpotMini, BostonDynamics</li>
<li>Big Dog</li>
</ul>
</li>
<li>Image recognition</li>
<li>Self-driving car</li>
<li>Medical Diagnosis</li>
<li>Applications of ML in business settings</li>
<li>Customer segmentation</li>
<li>Applications in Finance</li>
<li>Credit lending &amp; Fraud detection</li>
<li>Personalized recommendation</li>
<li>Dynamic pricing<ul>
<li>Rue La La</li>
</ul>
</li>
<li>Order dispatch for ride-sharing platforms<ul>
<li>DiDi</li>
</ul>
</li>
</ul>
<h4 id="Economical-impact-of-Machine-Learning"><a href="#Economical-impact-of-Machine-Learning" class="headerlink" title="Economical impact of Machine Learning"></a>Economical impact of Machine Learning</h4><ul>
<li>By 2035, AI could double annual global economic growth rates (Accenture)</li>
<li>Global GDP may increase by up to 14% (the equivalent of US$15.7 trillion) by 2030 as a result of the accelerating development and take-up of AI (PwC)</li>
</ul>
<h4 id="Trends-of-Machine-Learning"><a href="#Trends-of-Machine-Learning" class="headerlink" title="Trends of Machine Learning"></a>Trends of Machine Learning</h4><p><img src="/pic/9538373a69fb3836595f01495607834.png" alt="Trends of ML"></p>
<h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><h4 id="What-is-Machine-Learning"><a href="#What-is-Machine-Learning" class="headerlink" title="What is Machine Learning?"></a>What is Machine Learning?</h4><ul>
<li>Field of study that gives computers the ability to learn without being explicitly programmed. - Arthur Samuel, 1959</li>
<li>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. - Tom Mitchell, 1997</li>
</ul>
<h4 id="Programming-vs-Machine-Learning"><a href="#Programming-vs-Machine-Learning" class="headerlink" title="Programming vs Machine Learning"></a>Programming vs Machine Learning</h4><p><img src="/pic/bfec802ec5619a7f6b5ef92fd832d31.png" alt="Programming vs ML"></p>
<h4 id="Brief-History-of-Machine-Learning"><a href="#Brief-History-of-Machine-Learning" class="headerlink" title="Brief History of Machine Learning"></a>Brief History of Machine Learning</h4><ol>
<li>Connectionism, 1950s<ul>
<li>Perception, F. Rosenblatt</li>
</ul>
</li>
<li>Checker game, Arthur Samuel, 1959<ul>
<li>The term Machine Learning is coined.</li>
</ul>
</li>
<li>Symbolism, 1970-1980s<ul>
<li>Decision tree</li>
<li>ID3, Quinlan</li>
<li>Classification and Regression Tree (CART)</li>
</ul>
</li>
<li>Connectionism, 1980-1990s<ul>
<li>Back-Propagation</li>
<li>for Multi-layer Neural Networks</li>
</ul>
</li>
<li>Statistical Learning, 1990s<ul>
<li>Support vector machine</li>
<li>Kernel methods</li>
</ul>
</li>
<li>Connectionism, 2000s<ul>
<li>Deep Learning</li>
</ul>
</li>
</ol>
<h3 id="Categories"><a href="#Categories" class="headerlink" title="Categories"></a>Categories</h3><p>Categories of Machine Learning<br><img src="/pic/6cf69ccc0b5e18ab05c080df1515205.png" alt="Categories of ML"></p>
<h4 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h4><p><img src="/pic/110a43955525c0ad9a62c039eb3876a.png" alt="SL"></p>
<ul>
<li>Aims to <strong>predict on unknown data</strong> using models trained by labeled data</li>
<li>Learning a function that maps the <strong>feature (attribute)</strong> to <strong>label (response)</strong></li>
<li>Classification vs Regression<ul>
<li>Both utilize the training set (known data) to make predictions</li>
<li>The output of classification is categorical (<strong>discrete</strong>) while the output of regression is numerical (<strong>continuous</strong>)</li>
</ul>
</li>
</ul>
<h5 id="Process-of-Supervised-Learning"><a href="#Process-of-Supervised-Learning" class="headerlink" title="Process of Supervised Learning"></a>Process of Supervised Learning</h5><ol>
<li>Split data into training &amp; test sets</li>
<li>Train a model</li>
<li>Make predictions on testing set</li>
<li>Compare predicted and true labels</li>
</ol>
<h4 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h4><p><img src="/pic/dce728e8f2d0819688a148ebaf686f9.png" alt="UL"></p>
<p>Discover the structure and pattern within the unlabeled data.</p>
<ul>
<li>Market Segmentation</li>
<li>Social Network Analysis</li>
</ul>
<h4 id="Some-Machine-Learning-algorithms"><a href="#Some-Machine-Learning-algorithms" class="headerlink" title="Some Machine Learning algorithms"></a>Some Machine Learning algorithms</h4><p><img src="/pic/11af398aea2369112afe0fe46f5d255.png" alt="ML algorithms"></p>
<h2 id="Lecture2-MathForML"><a href="#Lecture2-MathForML" class="headerlink" title="Lecture2_MathForML"></a>Lecture2_MathForML</h2><h4 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h4><ul>
<li>$a \in A$ : $a$ is a member of set A</li>
<li>$||\pmb{v}||$ : the norm of vector $\pmb{v}$</li>
<li>$\pmb{x},\pmb{y},\pmb{z}$ : vector (lower case, bold)</li>
<li>$\pmb{A},\pmb{B}$ : matrix (upper case, bold)</li>
<li>$X$ : random variable (upper case)</li>
<li>$x$ : realizaton of random variable (lower case)</li>
<li>$y= f(\pmb{x})$ : function with muitiple inputs</li>
</ul>
<h3 id="Probability-amp-Statistics"><a href="#Probability-amp-Statistics" class="headerlink" title="Probability &amp; Statistics"></a>Probability &amp; Statistics</h3><h4 id="Sample-space-Omega"><a href="#Sample-space-Omega" class="headerlink" title="Sample space($\Omega$)"></a>Sample space($\Omega$)</h4><p>Set of all possible outcomes of an experiment</p>
<h4 id="Event-E"><a href="#Event-E" class="headerlink" title="Event($E$)"></a>Event($E$)</h4><p>Any subset of outcomes contained in the sample space</p>
<h4 id="Event-space-mathcal-F"><a href="#Event-space-mathcal-F" class="headerlink" title="Event space($\mathcal F$)"></a>Event space($\mathcal F$)</h4><p>The set of all possible events</p>
<h4 id="Axioms-of-probability"><a href="#Axioms-of-probability" class="headerlink" title="Axioms of probability"></a>Axioms of probability</h4><p>The <em>probabililty distribution</em> P is a function that satisfies the following</p>
<ol>
<li>$0 \leq P(E) \leq 1$ for any $E \in \mathcal F$ (Non-negativity)</li>
<li>$P(\Omega)=1$ </li>
<li>$P(E_1 \cup E_2)=P(E_1)+P(E_2)$ if $E_1$ and $E_2$ mutually exclusive events (Additivity)</li>
</ol>
<h4 id="Random-variable-RV"><a href="#Random-variable-RV" class="headerlink" title="Random variable (RV)"></a>Random variable (RV)</h4><p>mapping from sample space to real numbers</p>
<ul>
<li>Probability distribution specifies the probability of observing every possible value of a random variable</li>
<li>Discrete RV has a countable set of possible values: Bernoulli, Poisson, …</li>
<li>Continuous RV can take infinitely many possible values: Uniform, Normal, Exponential, …</li>
</ul>
<h4 id="Probability-distribution"><a href="#Probability-distribution" class="headerlink" title="Probability distribution"></a>Probability distribution</h4><p>Cumulative distribution function (CDF)</p>
<script type="math/tex; mode=display">F_X(x)=P(X \leq x)</script><p>Discrete random variable: probability mass function $p_X(x)$</p>
<script type="math/tex; mode=display">p_X(x)=P(X=x)</script><p>Continuous random variable: probability density function $f_X(x)$</p>
<script type="math/tex; mode=display">f_X(x) = \cfrac{\text{d}F_X(x)}{\text{d}x}</script><h4 id="Joint-distribution"><a href="#Joint-distribution" class="headerlink" title="Joint distribution"></a>Joint distribution</h4><p>Consider two random variables $X$ and $Y$ , the <em>joint cumulative distribution function</em> is defined as</p>
<script type="math/tex; mode=display">F_{XY}(x,y)=P(X \leq x,Y \leq y)</script><p>The joint probability mass function of two discrete variables $X$ , $Y$</p>
<script type="math/tex; mode=display">p_{X,Y}(x,y)=P(X=x,Y=y)</script><p>The joint probability density function of two continuous variables $X$ , $Y$</p>
<script type="math/tex; mode=display">f_{X,Y}(x,y)=\cfrac{\partial^2 F_{XY}(x,y)}{\partial x \partial y}</script><h4 id="Conditional-probability"><a href="#Conditional-probability" class="headerlink" title="Conditional probability"></a>Conditional probability</h4><p>The conditional probability of $X$ given $Y=y$ is defined as, </p>
<script type="math/tex; mode=display">P(X=x|Y=y)=\cfrac{P(X=x,Y=y)}{P(Y=y)}</script><h4 id="Product-rule"><a href="#Product-rule" class="headerlink" title="Product rule"></a>Product rule</h4><script type="math/tex; mode=display">P(X=x,Y=y)=P(X=x|Y=y)P(Y=y)=P(Y=y|X=x)P(X=x)</script><h4 id="Bayes’-rule"><a href="#Bayes’-rule" class="headerlink" title="Bayes’ rule"></a>Bayes’ rule</h4><script type="math/tex; mode=display">P(Y=y|X=x)=\cfrac{P(X=x|Y=y)\cdot P(Y=y)}{P(X=x)}</script><ul>
<li>Application: SPAM email case<ul>
<li>y: labels (SPAM/normal)</li>
<li>x: frequency of keywords    </li>
</ul>
</li>
</ul>
<h4 id="Marginal-probability"><a href="#Marginal-probability" class="headerlink" title="Marginal probability"></a>Marginal probability</h4><p>The probability of event that will occur regardless of conditional events</p>
<script type="math/tex; mode=display">
\begin{align}
P(X=x) &= \sum_{y \in \mathcal{y}}P(X=x,Y=y) \\
       &= \sum_{y \in \mathcal{y}}P(X=x|Y=y)P(Y=y)
\end{align}</script><h4 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h4><p>Consider two events $A$ and $B$ , they are <em>independent</em>  if</p>
<script type="math/tex; mode=display">
P(X=x,Y=y)=P(X=x) \cdot P(Y=y)</script><p>In addition, (if they are independent: )</p>
<script type="math/tex; mode=display">
P(X=x)= \cfrac{P(X=x,Y=y)}{P(Y=y)}=P(X=x | Y=y)</script><h4 id="Expectation"><a href="#Expectation" class="headerlink" title="Expectation"></a>Expectation</h4><p>Expectation (expected value) of a random variable $X$ is computed as</p>
<ul>
<li><u>Discrete RV</u> <script type="math/tex">\mathbb{E} [x] = \sum_x x \cdot p(x)</script></li>
<li><u>Continuous RV</u> <script type="math/tex">\mathbb{E}[X]=\int_x x \cdot f(x) \cdot \text{d} x</script></li>
<li>Expectation of functions <script type="math/tex">\mathbb{E}[h(X)] = \int_x h(x) \cdot  f(x) \cdot \text{d}x</script></li>
<li>Other properties: <script type="math/tex">\begin{align}
\mathbb{E}[aX+b]&=a\mathbb{E}[X]+b \\
\mathbb{E}[X+Y]&=\mathbb{E}[X]+\mathbb{E}[Y]\\
\mathbb E[XY]&=\mathbb E[X]\mathbb E[Y],\ \text{if\ X\ and\ Y\ are uncoorrelated}
\end{align}</script></li>
</ul>
<h4 id="Conditional-expectation"><a href="#Conditional-expectation" class="headerlink" title="Conditional expectation"></a>Conditional expectation</h4><p>The conditional expectation of $X$ with respect to $Y$ is the function</p>
<script type="math/tex; mode=display">\mathbb{E}[X|Y=y]</script><p>Discrete random variable</p>
<script type="math/tex; mode=display">\mathbb{E}[X|Y=y]= \sum_x x\cdot P(X=x|Y=y)</script><p>Continuous random variable</p>
<script type="math/tex; mode=display">\mathbb{E}[X|Y=y]=\int_x x f_{X|Y}(x|y) \text{d}x</script><h5 id="Law-of-total-expectation"><a href="#Law-of-total-expectation" class="headerlink" title="Law of total expectation"></a>Law of total expectation</h5><script type="math/tex; mode=display">\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]]</script><h4 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h4><p>The squared deviation of $X$ from its mean</p>
<script type="math/tex; mode=display">\text{Var}[X]=\mathbb{E}[(X-\mathbb{E}[X])^2]=\mathbb{E}[X^2]=\mathbb{E}[X]^2</script><ul>
<li>Standard variation <script type="math/tex">\sigma = \sqrt{\text{Var}[X]}</script></li>
<li>Other properties <script type="math/tex">\begin{align}
\text{Var}[aX+b]&=a^2 \cdot \text{Var}[x] \\
\text{Var}[X+Y]&=\text{Var}[X]+\text{Var}[Y]\ \text{if X,Y are uncorrelated}
\end{align}</script></li>
</ul>
<h4 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h4><script type="math/tex; mode=display">\text{Cov}(X_1,X_2)=\mathbb{E}[(X_1-\mathbb{E}[X_1])(X_2-\mathbb{E}[X_2])]</script><h4 id="Estimation-of-Parameters"><a href="#Estimation-of-Parameters" class="headerlink" title="Estimation of Parameters"></a>Estimation of Parameters</h4><ul>
<li>Suppose we have random variables $X_1, X_2, \cdots, X_n$ and corresponding observations $x_1, x_2, \cdots, x_n$</li>
<li>We select a parametric model and fit the parameters of the model to the data.</li>
<li>How do we choose the values of the parameters $\theta$ ?</li>
</ul>
<h5 id="Maximum-Likelihood-Estimation-MLE"><a href="#Maximum-Likelihood-Estimation-MLE" class="headerlink" title="Maximum Likelihood Estimation (MLE)"></a>Maximum Likelihood Estimation (MLE)</h5><p>Which $\theta$ makes the observations $x_1,x_2,\cdots,x_n$ most likely?</p>
<ul>
<li>Maximize the likelihood of the observed data <script type="math/tex">\hat{\theta}_{MLE}=\text{arg}\max_\theta \mathcal{L}(\theta)=\text{arg}\max_\theta p(x_1,x_2,\cdots,x_n|\theta)</script></li>
<li>Assume that $x_1,x_2, \cdots,x_n$ are i.i.d., we have <script type="math/tex">\mathcal{L}(\theta)=\prod_{i=1}^n p(x_i|\theta)</script></li>
<li>Take the logarithmic on both sides, we obtain the log-likelihood <script type="math/tex">\log \mathcal{L}(\theta)= \sum_{i=1}^n \log p(x_i|\theta)</script><h6 id="Example-of-MLE"><a href="#Example-of-MLE" class="headerlink" title="Example of MLE"></a>Example of MLE</h6></li>
<li>Imagine a bowl contains a large number of red and white balls. The proportion of the red balls, denoted by $\theta$ , is unknown.</li>
<li>Now we sample balls from this bowl with replacement for $n$ times and observe $x$ red balls out of $n$ balls.</li>
<li>Likelihood function:<script type="math/tex">\begin{align}
L(\theta)=L(x;\theta)=\binom{n}{x}\theta^x(1-\theta)^{n-x} \\
\log L(\theta) = x\log \theta+(n-x)\log(1-\theta) \\
\cfrac{\partial \log L(\theta)}{\partial \theta} = 0 \Rightarrow \hat\theta_{MLE} = \cfrac{x}{n}
\end{align}</script></li>
</ul>
<h5 id="Maximum-A-Posteriori-Estimation-MAP"><a href="#Maximum-A-Posteriori-Estimation-MAP" class="headerlink" title="Maximum A Posteriori Estimation (MAP)"></a>Maximum A Posteriori Estimation (MAP)</h5><p>Which $\theta$ maximizes the posterior $p(\theta | x_1,x_2,\cdots,x_n)$ given the prior $p(\theta)$ ?</p>
<ul>
<li>We assume that the parameter is a random variable, and we specify a prior distribution $p(\theta)$</li>
<li>By Bayes’ rule, we compute the posterior of the parameter <script type="math/tex">p(\theta | x_1,x_2,\cdots,x_n) \propto p(\theta)p(x_1,x_2,\cdots,x_n|\theta)</script></li>
<li>Estimate parameter $\theta$ by maximizing the posterior <script type="math/tex">\hat\theta_{MAP}=\text{arg}\max_\theta p(\theta)p(x_1,x_2,\cdots,x_n|\theta)</script></li>
<li>We take the logarithmic of the posterior, <script type="math/tex">\hat\theta_{MAP}=\text{arg}\max_\theta \log p(\theta)+\sum_{i=1}^n \log p(x_i|\theta)</script><br><em>: MAP: balance MLE and prior knowledge</em></li>
</ul>
<h6 id="Example-of-MAP"><a href="#Example-of-MAP" class="headerlink" title="Example of MAP"></a>Example of MAP</h6><ul>
<li>Imagine a bowl contains a large number of red and white balls. The proportion of the red balls, denoted by $\theta$ , is unknown, but with a Beta prior, $P(\theta) =\cfrac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}$</li>
<li>Now we sample balls from this bowl with replacement for $n$ times and observe $x$ red balls out of $n$ balls.</li>
<li>The posterior function: <script type="math/tex">\begin{align}
p(x|\theta)p(\theta) &= \binom{n}{x}\theta^x(1-\theta)^{n-x}\cfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1} \\
\log p(x|\theta)p(\theta) &= (x+\alpha-1)\log \theta +(n-x+\beta -1)\log(1-\theta) \\
\hat\theta_{MAP}&=\cfrac{x+\alpha-1}{n+\alpha+\beta-2}
\end{align}</script></li>
</ul>
<h3 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a>Linear Algebra</h3><h4 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h4><ul>
<li>A one-dimension array of $n$ values, denoted by $\pmb{x}$ (lower case, bold)</li>
<li>$x_i$ is the $i$-th element of $\pmb{x}$ <script type="math/tex">\pmb{x}=(x_1,x_2,\cdots,x_n)^T= \begin{bmatrix}x_1\\ x_2\\ \vdots \\ x_n \end{bmatrix}</script></li>
</ul>
<h4 id="Matrix"><a href="#Matrix" class="headerlink" title="Matrix"></a>Matrix</h4><ul>
<li>A two-dimension array of $m \times n$ values, denoted by $\pmb{A}$ (upper case, bold)</li>
<li>$m$ is the number of row vector, $n$ is the number of column vectors</li>
<li>$a_{ij}$ is the entry in $i$-th row and $j$-th column <script type="math/tex">\pmb{A}=\begin{bmatrix} a_{11} &\cdots &a_{1n} \\ \vdots &\ddots &\vdots \\ a_{m1} &\cdots &a_{mn} \end{bmatrix}</script></li>
</ul>
<h4 id="Vector-algorithmic"><a href="#Vector-algorithmic" class="headerlink" title="Vector algorithmic"></a>Vector algorithmic</h4><ul>
<li>Scalar multiplication of a vector <script type="math/tex">\pmb{y}=a\pmb{x}=(ax_1,ax_2,\cdots,ax_n)^T</script></li>
<li>Dot product of the vectors $\pmb{x},\pmb{y} \in \mathbb{R}^n$ <script type="math/tex">\pmb{x}^T\pmb{y} = [x_1,x_2,\cdots, x_n]\begin{bmatrix}y_1\\ y_2\\ \vdots\\ y_n\end{bmatrix}=\sum_{i=1}^n x_iy_i \in \mathbb{R}</script></li>
<li>Outer product of the vectors $\pmb{x} \in \mathbb{R}^m, \pmb{y} \in \mathbb{R}^n$ <script type="math/tex">\pmb{x}\pmb{y}^T=\begin{bmatrix}x_1\\ x_2\\ \vdots x_m\end{bmatrix}[y_1,y_2,\cdots,y_n]=\begin{bmatrix}x_1y_1 &\cdots &x_1y_n\\ \vdots &\ddots &\vdots\\ x_my_1 &\cdots &x_my_n\end{bmatrix}</script></li>
</ul>
<h4 id="Vector-norm"><a href="#Vector-norm" class="headerlink" title="Vector norm"></a>Vector norm</h4><p>A norm $||\cdot||$ is a function that satisfies</p>
<ul>
<li>$||\pmb{x}|| \geq 0$ with equality if and only if $\pmb{x}=\pmb{0}$</li>
<li>$||\pmb{x}+\pmb{y}|| \leq ||\pmb{x}|| + ||\pmb{y}||$</li>
<li>$||a\pmb{x}||=|a|||\pmb{x}||$</li>
<li>$\pmb{x}^T\pmb{y}=||\pmb{x}||_2||\pmb{y}||_2\cos(\theta)$</li>
<li>$l_1$ norm $||\pmb{x}||_1=\displaystyle\sum_{i=1}^n|x_i|$</li>
<li>$l_2$ norm $||\pmb{x}||_2=\left(\displaystyle\sum_{i=1}^n|x_i|^2\right)^{\frac{1}{2}}$ </li>
</ul>
<h4 id="Matrix-arithmetric"><a href="#Matrix-arithmetric" class="headerlink" title="Matrix arithmetric"></a>Matrix arithmetric</h4><ul>
<li>Addition of matrices $\pmb{A},\pmb{B} \in \mathbb{R}^{m\times n}$ <script type="math/tex">\pmb{C}=\pmb{A}+\textcolor{red}{\pmb{B}}=\begin{bmatrix} a_{11}+\textcolor{red}{b_{11}} &\cdots &a_{1n}+\textcolor{red}{b_{1n}}\\ \vdots &\ddots &\vdots\\ a_{m1}+\textcolor{red}{b_{m1}} &\cdots &a_{mn}+\textcolor{red}{b_{mn}} \end{bmatrix}</script></li>
<li>Scalar multiplication of a matrix <script type="math/tex">\pmb{B}=\textcolor{red}{d}\cdot \pmb{A}=\begin{bmatrix}\textcolor{red}{d}\cdot a_{11} &\cdots &\textcolor{red}{d}\cdot a_{1n}\\ \vdots &\ddots &\vdots\\ \textcolor{red}{d}\cdot a_{m1} &\cdots &\textcolor{red}{d}\cdot a_{mn}\end{bmatrix}</script></li>
<li>Multiplication of matrices $\pmb{A} \in \mathbb{R}^{m \times n}$ and $\pmb{B} \in \mathbb{R}^{n\times p}$ <script type="math/tex">\pmb{A}\pmb{B}=\pmb{C} \in \mathbb{R}^{m\times p},\ \ \ \ c_{ij}=\sum_{k=1}^na_{ik}b_{kj}</script></li>
<li>Matrix multiplication is <em>associative</em>: $\pmb{A}(\pmb{B}\pmb{C})=(\pmb{AB})\pmb{C}$ </li>
<li>Matrix multiplication is <em>distributive</em>: $\pmb A (\pmb B+\pmb C)=\pmb{AB}+\pmb{AC}$</li>
<li>Matrix multiplication is <em>NOT communicative</em>: $\pmb{AB} \neq \pmb{BA}$</li>
</ul>
<h4 id="Transpose"><a href="#Transpose" class="headerlink" title="Transpose"></a>Transpose</h4><p>Given a matrix $\pmb A \in \mathbb R^{m\times n}$ , its transpose, written by $\pmb A^T \in \mathbb R^{n\times m}$ , is given by <script type="math/tex">(\pmb A^T)_{ij}=(\pmb A)_{ji}</script><br>Some properties</p>
<ul>
<li>$(\pmb{AB})^T=\pmb B^T\pmb A^T$</li>
<li>$(\pmb A^T)^T=\pmb A$</li>
<li>$(\pmb A+\pmb B)^T=\pmb A^T+\pmb B^T$</li>
</ul>
<h4 id="Symmetric-matrix"><a href="#Symmetric-matrix" class="headerlink" title="Symmetric matrix"></a>Symmetric matrix</h4><p>A square matrix is <em>symmetric</em> if $\pmb A =\pmb A^T$.</p>
<h4 id="Inverse-of-a-matrix"><a href="#Inverse-of-a-matrix" class="headerlink" title="Inverse of a matrix"></a>Inverse of a matrix</h4><p>For a matrix $\pmb A\in \mathbb R^{n\times n}$ , if there exists a square matrix $\pmb B \in \mathbb R^{n\times n}$ such that <script type="math/tex">\pmb{BA}=\pmb{AB}=\pmb I</script><br>where $\pmb I$ is the $n$-by-$n$ <em>identity matrix</em>, then $\pmb B$ is the <em>inverse</em> of $\pmb A$.</p>
<ul>
<li>The inverse of $\pmb A$ is denoted by $\pmb A^{-1}$ </li>
<li>A matrix is <em>invertible</em> if it is not <em>singular</em>.</li>
</ul>
<p><u>Solving a linear system</u><br>If $\pmb A$ is square nonsingular matrix, then the solution to the linear system $\pmb{AX}=\pmb b$ is given by <script type="math/tex">\pmb x=\pmb A^{-1}\pmb b</script></p>
<h4 id="Semidefinite-matrices"><a href="#Semidefinite-matrices" class="headerlink" title="Semidefinite matrices"></a>Semidefinite matrices</h4><p>A symmetric matrix $\pmb A \in \mathbb S^{n\times n}$ is </p>
<ul>
<li><em>positive semidefinite</em> if $\pmb x^T \pmb{Ax} \geq 0$ for any $\pmb x \in \mathbb R^n$ and $\pmb x \neq \pmb 0$ , denoted by $\pmb A \succcurlyeq 0$ ;</li>
<li><em>positive definite</em> if $\pmb x^T \pmb{Ax} \textgreater 0$ for any $\pmb x \in \mathbb R^n$ and $\pmb x \neq 0$ , denoted by $\pmb A \succ 0$ ;</li>
<li>negative semidefinite if $-\pmb A$ is positive semidefinite;</li>
<li>negative definite if $-\pmb A$ is positive definite;</li>
<li>indefinite if it is neither positive nor negative definite.</li>
</ul>
<h4 id="Back-to-Probability-amp-Statistics"><a href="#Back-to-Probability-amp-Statistics" class="headerlink" title="Back to Probability &amp; Statistics"></a>Back to Probability &amp; Statistics</h4><h5 id="Random-vector"><a href="#Random-vector" class="headerlink" title="Random vector"></a>Random vector</h5><p>A vector of random variables $X_1,X_2,\cdots,X_n$, denoted by $\pmb X=[X_1,X_2,\cdots,X_n]^T$ </p>
<ul>
<li>$\mathbb E[X]=[\mathbb E[X_1],\mathbb E[X_2],\cdots,\mathbb E[X_n]]^T$</li>
</ul>
<h5 id="Covariance-matrix"><a href="#Covariance-matrix" class="headerlink" title="Covariance matrix"></a>Covariance matrix</h5><script type="math/tex; mode=display">\Sigma = \begin{bmatrix} \text{Cov}[X_1,X_1] &\cdots &\text{Cov}[X_1,X_n]\\ \vdots &\ddots &\vdots\\ \text{Cov}[X_n,X_1] &\cdots &\text{Cov}[X_n,X_n]\end{bmatrix}=\mathbb E[(\pmb X -\mathbb E[X])(\pmb X-\mathbb E[\pmb X])^T]</script><h4 id="Matrix-calculus"><a href="#Matrix-calculus" class="headerlink" title="Matrix calculus"></a>Matrix calculus</h4><p>Consider a function $f:\mathbb R^n \rightarrow \mathbb R$ , the <em>gradient</em> of $f$ is defined as a vector of partial derivatives <script type="math/tex">\nabla f(\pmb x)=\begin{bmatrix}\cfrac{\partial f(\pmb x)}{\partial x_1}\\ \cfrac{\partial f(\pmb x)}{\partial x_2}\\ \vdots\\ \cfrac{\partial f(\pmb x)}{\partial x_n}\end{bmatrix}</script><br>“direction and rate of fastest <strong>increase</strong>“</p>
<ul>
<li>The direction of fastest increase of the function</li>
<li>The magnitude is the rate of increase</li>
</ul>
<p>Consider a function $f: \mathbb R^n \rightarrow \mathbb R$, the <em>Hessian</em> of $f$ is defined as <script type="math/tex">\nabla^2f(\pmb x)=\begin{bmatrix}\cfrac{\partial^2f(\pmb x)}{\partial x_1^2} &\cdots &\cfrac{\partial^2f(\pmb x)}{\partial x_1\partial x_n}\\ \vdots &\ddots &\vdots\\ \cfrac{\partial^2f(\pmb x)}{\partial x_n\partial x_1} &\cdots &\cfrac{\partial^2f(\pmb x)}{\partial x_n^2}\end{bmatrix}</script></p>
<ul>
<li>Hessian is symmetric when $f$ is twice differentiable. <script type="math/tex">\cfrac{\partial^2 f(\pmb x)}{\partial x_i\partial x_j}=\cfrac{\partial^2f(\pmb x)}{\partial x_j\partial x_i}</script><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3></li>
</ul>
<h4 id="What-is-optimization"><a href="#What-is-optimization" class="headerlink" title="What is optimization?"></a>What is optimization?</h4><p>Finding the minimizer of a function subject to constraints:</p>
<script type="math/tex; mode=display">\begin{align}
\text{minimize}_{\pmb x} \ \ \ \ &f(\pmb x) \\
s.t.\ \ \ \ &f_i(\pmb x)\leq 0,i=1,2,\cdots, k;\\
&h_j(\pmb x)=0,j=1,2,\cdots,l.
\end{align}</script><ul>
<li>Mean-variance analysis</li>
<li>Transportation problems</li>
<li>Facility location</li>
<li>Linear regression</li>
<li>Logistic regression</li>
<li>Support vector machine</li>
<li>Neural networks</li>
</ul>
<h4 id="Local-minima-and-global-minima"><a href="#Local-minima-and-global-minima" class="headerlink" title="Local minima and global minima"></a>Local minima and global minima</h4><ul>
<li>Local minima is the solution that optimal within a neighboring set</li>
<li>Global minima is the optimal solution among all possible solutions<br><img src="/pic/379ac1b116f3c5612b0e6a1f3df25ea.png" alt=""></li>
</ul>
<h4 id="Convex-set"><a href="#Convex-set" class="headerlink" title="Convex set"></a>Convex set</h4><p>A set $C\in \mathbb R^n$ is <em>convex</em> if for $\pmb x,\pmb y\in C$ and any $\alpha\in [0,1]$, <script type="math/tex">\alpha \pmb x+(1-\alpha)\pmb y \in C</script><img src="/pic/b885ffb38cf90c077ad79abb079e005.png" alt=""></p>
<p>Examples: $\mathbb R^n$ , norm ball {$\pmb x: ||\pmb x|| \leq r$} for a given $r$ , intersection of convex sets</p>
<h4 id="Convex-Concave-function"><a href="#Convex-Concave-function" class="headerlink" title="Convex (Concave) function"></a>Convex (Concave) function</h4><p>A function $f:\mathbb R^n \rightarrow \mathbb R$ is convex (concave) if for $\pmb x,\pmb y \in \text{dom}(f)$ and any $\alpha \in [0,1]$ , <script type="math/tex">f(\alpha \pmb x+(1-\alpha)\pmb y)\leq(\geq) \alpha f(\pmb x)+(1-\alpha)f(\pmb y)</script><img src="/pic/57c1cdf60aceade4f79afcdcf5781c5.png" alt=""></p>
<h4 id="First-order-convexity-condition"><a href="#First-order-convexity-condition" class="headerlink" title="First-order convexity condition"></a>First-order convexity condition</h4><p>Suppose a function $f:\mathbb R^n \rightarrow \mathbb R$ is differentiable. Then $f$ is convex if and only if for all $\pmb x,\pmb y \in \text{dom}(f)$ <script type="math/tex">f(\pmb y) \geq f(\pmb x) + \nabla f(\pmb x)^T(\pmb y-\pmb x)</script><img src="/pic/b9432d6edc99c82146c5ca340b6fd65.png" alt=""></p>
<h4 id="Second-order-convexity-condition"><a href="#Second-order-convexity-condition" class="headerlink" title="Second-order convexity condition"></a>Second-order convexity condition</h4><p>Suppose a function $f:\mathbb R^n \rightarrow \mathbb R$ is twice differentiable. Then $f$ is convex if and only if for all $\pmb x \in \text{dom}(f)$ <script type="math/tex">\nabla^2f(\pmb x)\succcurlyeq 0</script><img src="/pic/8c43075b2094ec2db8c526f1ea28d14.png" alt=""></p>
<h4 id="Examples-of-convex-functions"><a href="#Examples-of-convex-functions" class="headerlink" title="Examples of convex functions"></a>Examples of convex functions</h4><ul>
<li>Exponential function: $e^{ax}$</li>
<li>Logarithmic function: $\log(x)$ is concave</li>
<li>Affine function: $\pmb a^T\pmb x+b$ is a convex and concave</li>
<li>Least square loss: $||\pmb y-\pmb{X\beta}||_2^2$</li>
<li>$f_1(x)$ is convex for $x\in \text{dom}(f_1)$ and $f_2(x)$ is convex for $x\in \text{dom}(f_2)$ , then $f_1+f_2$ is convex for $x\in \text{dom}(f_1) \cap \text{dom}(f_2)$ </li>
</ul>
<h4 id="Convex-optimization-problem"><a href="#Convex-optimization-problem" class="headerlink" title="Convex optimization problem"></a>Convex optimization problem</h4><p>An optimization problem is convex if its objective is a convex function, the inequality constraints $f_j$ are convex, and the equality constraints $h_j$ are affine. </p>
<script type="math/tex; mode=display">\begin{align}
\text{minimize}_{\pmb x} \ \ \ \ &f(\pmb x)\\
s.t. \ \ \ \ &f_i(\pmb x)\leq 0,i=1,2,\cdots,k; \\
&h_j(\pmb x)=0,j=1,2,\cdots,l.
\end{align}</script><h4 id="It’s-nice-to-be-convex"><a href="#It’s-nice-to-be-convex" class="headerlink" title="It’s nice to be convex!"></a>It’s nice to be convex!</h4><ul>
<li>$\nabla f(\pmb x)=0$ if and only if $\pmb x$ is a global minimizer of $f(\pmb x)$ .</li>
<li>If $\pmb x$ is a local minimizer of a convex optimization problem, it is a global minimizer.</li>
</ul>
<h4 id="Optimization-methods"><a href="#Optimization-methods" class="headerlink" title="Optimization methods"></a>Optimization methods</h4><ul>
<li>Gradient descent</li>
<li>Newton’s method</li>
<li>Coordinate descent</li>
<li>Lagrangian method</li>
</ul>
<h2 id="Lecture3-BasicInML"><a href="#Lecture3-BasicInML" class="headerlink" title="Lecture3_BasicInML"></a>Lecture3_BasicInML</h2><h3 id="Generalization-ability-泛化能力"><a href="#Generalization-ability-泛化能力" class="headerlink" title="Generalization ability (泛化能力)"></a>Generalization ability (泛化能力)</h3><ul>
<li>A model’s ability to generalize to new data</li>
<li>If the model is trained too well, it can fit perfectly the random fluctuatioins or noise in the training data but it will fail to predict accurately on new data<br><img src="/pic/a99b5dbe4e510c276edb458034cea64.png" alt=""></li>
</ul>
<h4 id="Underfitting-and-overfitting"><a href="#Underfitting-and-overfitting" class="headerlink" title="Underfitting and overfitting"></a>Underfitting and overfitting</h4><ul>
<li><p>Underfitting (欠拟合) occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data.<br><img src="/pic/f70b694bdd89dce57b1ed3575e59c22.png" alt=""></p>
</li>
<li><p>Overfitting (过拟合) occurs when a statistical model describes random error or noise instead of the underlying relationship.<br><img src="/pic/8918735b6e8c752e2d4ae0af01c4e03.png" alt=""></p>
</li>
</ul>
<h4 id="Basic-terms"><a href="#Basic-terms" class="headerlink" title="Basic terms"></a>Basic terms</h4><ul>
<li>A sample is denoted by $(\pmb x_i,y_i)$ where $\pmb x_i$ is the attribute (feature) vector and $y_i$ is the label (response).</li>
<li>A list of $m$ samples is a dataset, denoted by $D$ .<script type="math/tex">D=\{(\pmb x_i,y_i):i=1,2,\cdots,m\}</script><ul>
<li>Training set, denoted by $S$ , is used to train the model</li>
<li>Test set, denoted by $T$ , is used to evaluate the performance of the model</li>
</ul>
</li>
<li>A mapping from the attribute space (特征空间) $\mathcal X$ to the label space (标签空间) $\mathcal Y$ , denoted by $f$ , is called a hypothesis (假设).<script type="math/tex">f:\mathcal X \rightarrow \mathcal Y</script></li>
<li>The set of all possible hypotheses is called hypothesis space (假设空间), denoted by $F$ .<script type="math/tex">F=\{f_1,f_2,\cdots\}</script></li>
</ul>
<h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><ul>
<li>Suppose $\hat f(\pmb x)$ is obtained using the training set $S$ </li>
<li>For an unknown sample $(\pmb x_0,y_0)$ , the error between the prediction $\hat f(\pmb x_0)$ and the observed value $y_0$ is measured by the loss function (损失函数) <script type="math/tex">L(\hat f(\pmb x_0),y_0)</script></li>
<li>Examples<ul>
<li>$L(\hat f(\pmb x),y)=\mathbb I(\hat f(\pmb x)\neq y)$ (binary classification)</li>
<li>$L(\hat f(\pmb x),y)=(\hat f(\pmb x)-y)^2$ (regression)</li>
</ul>
</li>
</ul>
<h4 id="Generalization-error"><a href="#Generalization-error" class="headerlink" title="Generalization error"></a>Generalization error</h4><h5 id="Generalization-error-泛化误差"><a href="#Generalization-error-泛化误差" class="headerlink" title="Generalization error (泛化误差)"></a>Generalization error (泛化误差)</h5><script type="math/tex; mode=display">R(f)=\mathbb E [L(f(\pmb x),y)]</script><ul>
<li>The expectation is taken over the joint distribution of $(\pmb x ,y)$ </li>
</ul>
<h5 id="Test-error-测试误差"><a href="#Test-error-测试误差" class="headerlink" title="Test error (测试误差)"></a>Test error (测试误差)</h5><ul>
<li>Given a set of test samples $T=\{(\pmb x_i,y_i):i=1,2,\cdots,n\}$ , the test error is given by <script type="math/tex">\hat R_T(f)=\cfrac{1}{n}\sum_{i=1}^n L(f(\pmb x_i),y_i)</script></li>
</ul>
<h5 id="Training-error-训练误差"><a href="#Training-error-训练误差" class="headerlink" title="Training error (训练误差)"></a>Training error (训练误差)</h5><ul>
<li>Given a set of training samples $S=\{(\pmb x_i,y_i):i=1,2,\cdots,m\}$ , the training error is given by <script type="math/tex">\hat R_S(f)=\cfrac{1}{m}\sum_{i=1}^mL(f(\pmb x_i),y_i)</script><h5 id="Learning-objective"><a href="#Learning-objective" class="headerlink" title="Learning objective"></a>Learning objective</h5></li>
<li>Select a hypothesis $f\in F$ with the smallest <em>generalization error</em>. <script type="math/tex">\min_{f\in F}R(f)=\min_{f\in F}\mathbb E[L(f(\pmb x),y)]</script></li>
<li>However, the true distribution of $(\pmb x,y)$ is usually unknown in practice.</li>
<li>We obtain the hypothesis by minimizing the <em>training error</em> with the training set $S=\{(\pmb x_i,y_i):i=1,2,\cdots,m\}$ ,<script type="math/tex">\min_{f\in F}\hat R_S(f)=\min_{f\in F}\cfrac{1}{m}\sum_{i=1}^m L(f(\pmb x_i),y_i)</script></li>
</ul>
<h5 id="Training-error-vs-test-error"><a href="#Training-error-vs-test-error" class="headerlink" title="Training error vs test error"></a>Training error vs test error</h5><p><img src="/pic/7ba8a7a66050778027f6549fa41cb2a.png" alt=""></p>
<h3 id="Performance-metrics"><a href="#Performance-metrics" class="headerlink" title="Performance metrics"></a>Performance metrics</h3><h4 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h4><h5 id="Performance-metrics-for-classification"><a href="#Performance-metrics-for-classification" class="headerlink" title="Performance metrics for classification"></a>Performance metrics for classification</h5><ul>
<li>Consider a binary classifier $\mathcal Y =\{-,+\}$<br><img src="/pic/10adcb662406252426c360458bf636b.png" alt=""></li>
</ul>
<h5 id="Confusion-matrix"><a href="#Confusion-matrix" class="headerlink" title="Confusion matrix"></a>Confusion matrix</h5><p><img src="/pic/8181efdc993f5c23481a08ae4ac524b.png" alt=""></p>
<ul>
<li>True Positive (真正例) - we predicted “+” and the true class is “+”</li>
<li>True Negative (真反例) - we predicted “-“ and the true class is “-“</li>
<li>False Positive (假正例) - we predicted “+” and the true class is “-“</li>
<li>False Negative (假反例) - we predicted “-“ and the true class is “+”</li>
</ul>
<h5 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h5><script type="math/tex; mode=display">\text{Accuracy} = \cfrac{TP+TN}{TP+FN+FP+TN}=\cfrac{\text{Correct predictions}}{\text{Total data points}}</script><ul>
<li>The relationship with the misclassification error rate (分类错误率), <script type="math/tex">\text{Accuracy}=1-\cfrac{1}{m}\sum_{i=1}^m\mathbb I (f(\pmb x_i\neq y_i))</script><h6 id="Limitation-of-Accuracy"><a href="#Limitation-of-Accuracy" class="headerlink" title="Limitation of Accuracy"></a>Limitation of Accuracy</h6></li>
<li>A predictive model may have high accuracy, but be useless.<ul>
<li>Suppose the positive class is only a tiny portion of the observed data. For example, only 1% of patients has true cancer while other 99% of patients don’t have any cancers.</li>
<li>Consider a “stupid” model that always predicts “no cancer”, what is the accuracy?</li>
</ul>
</li>
</ul>
<h5 id="Precision-查准率"><a href="#Precision-查准率" class="headerlink" title="Precision(查准率)"></a>Precision(查准率)</h5><script type="math/tex; mode=display">\text{Precision}=\cfrac{TP}{TP+FP}=\cfrac{\text{Correctly predicted positive}}{\text{All predicted positive}}</script><h5 id="Recall-查全率"><a href="#Recall-查全率" class="headerlink" title="Recall(查全率)"></a>Recall(查全率)</h5><script type="math/tex; mode=display">\text{Recall}=\cfrac{TP}{TP+FN}=\cfrac{\text{Correctly predicted positive}}{\text{All real positive}}</script><ul>
<li>Which one is worse, False Positive or False Negative?</li>
<li>It depends!<ul>
<li>Medical Diagnosis - False Negative</li>
<li>Span Detection - False Positive</li>
</ul>
</li>
</ul>
<h5 id="F-1-score"><a href="#F-1-score" class="headerlink" title="$F_1$ score"></a>$F_1$ score</h5><ul>
<li>How to compare precision/recall and decide which algorithm is better?</li>
<li><p>$F_1$ score: a combined measure <script type="math/tex">F_1=2\cfrac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}</script></p>
</li>
<li><p>Which metric to use?</p>
</li>
<li>Accuracy<ul>
<li>The class distribution is balanced</li>
<li>FP and FN costs are similar</li>
</ul>
</li>
<li>$F_1$ score<ul>
<li>The class distribution is unbalanced</li>
<li>FP and FN costs may be different</li>
</ul>
</li>
<li>Recall<ul>
<li>The cost of FN is much higher than that of FP</li>
<li>e.g. rather get healthy people labeled as sick over leaving a infected person labeled healthy</li>
</ul>
</li>
<li>Precision<ul>
<li>The cost of FP is much higher than that of FN</li>
<li>e.g. rather have some span emails in inbox than some regular emails in your spam box</li>
</ul>
</li>
</ul>
<h4 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h4><h5 id="Performance-metrics-for-regression"><a href="#Performance-metrics-for-regression" class="headerlink" title="Performance metrics for regression"></a>Performance metrics for regression</h5><ul>
<li>The common performance measure for regression is <em>mean squared error (MSE)</em> .</li>
<li>For a dateset $D=\{(\pmb x_i,y_i):i=1,2,\cdots,m\}$ , <script type="math/tex">\hat R_D(\hat f)=\cfrac{1}{m}\sum_{i=1}^m(\hat f(\pmb x_i)-y_i)^2</script></li>
<li>Some other performance measures<ul>
<li>Root Mean Square Error (RMSE)</li>
<li>Mean Absolute Error (MAE)</li>
<li>$R^2$</li>
</ul>
</li>
</ul>
<h3 id="Bias-variance-decomposition"><a href="#Bias-variance-decomposition" class="headerlink" title="Bias-variance decomposition"></a>Bias-variance decomposition</h3><ul>
<li>Given a training set $S=\{(\pmb x_i,y_i),i=1,2,\cdots,m\}$ such that each sample $(\pmb x_i,y_i)$ satisfies the following relationship <script type="math/tex">y=f(\pmb x)+\epsilon</script><ul>
<li>$\epsilon$ is the noise with mean zero and variance $\sigma^2$ .</li>
</ul>
</li>
<li>Let $\hat f(\pmb x;S)$ denote the estimated function that is trained with the set $S$ </li>
<li><p>For an unseen sample $(\pmb x_0,y_0)$ ,</p>
<ul>
<li>the predicted value using the function trained with $S$ is $\hat f(\pmb x_0;S)$ </li>
<li>the expected predicted value is $\mathbb E_S[\hat f(\pmb x_0;S)]$ </li>
<li>the true value is $f(\pmb x_0)$ </li>
<li>the bias (偏差) of the predicted value is <script type="math/tex">\text{Bias}[\hat f(\pmb x_0;S)]=\mathbb E[\hat f(\pmb x_0;S)]-f(\pmb x_0)</script></li>
<li>the variance (方差) of the predicted value is <script type="math/tex">\text{Var}[\hat f(\pmb x_0;S)]=\mathbb E\left[(\hat f(\pmb x_0;S)-\mathbb E[\hat f(\pmb x_0;S)])^2\right]</script></li>
<li>the expected squared error, where the expectation is over the random noise and the training set, is given by <script type="math/tex">E\left[(y_0-\hat f(\pmb x_0;S))^2\right]=(\text{Bias}[\hat f(\pmb x_0;S)])^2+\text{Var}[\hat f(\pmb x_0;S)]+\sigma^2</script></li>
<li>proof: <script type="math/tex">\begin{align}
&\mathbb E\left[(y_0-\hat f(\pmb x_0;S))^2\right]\\
=&\mathbb E\left[(y_0-\textcolor{red}{\mathbb E[\hat f(\pmb x_0;S)]+\mathbb E[\hat f(\pmb x_0;S)]}-\hat f(\pmb x_0;S))^2\right]\\
=&\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))^2+(y_0-\mathbb E[[\hat f(\pmb x_0;S)])^2+2(y_0-\mathbb E[\hat f(\pmb x_0;S)])(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))\right]\\
=&\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))^2\right]+\mathbb E[(y_0-\mathbb E[\hat f(\pmb x_0;S)])^2]+\mathbb E[2(y_0-\mathbb E[\hat f(\pmb x_0;S)])(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))]\\
=&\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))^2\right]+\mathbb E [(\textcolor{red}{f(\pmb x_0)+\epsilon}-\mathbb E[\hat f(\pmb x_0;S)])^2]\\
=&\mathbb E\left[(\mathbb E[\hat f(\pmb x_0;S)]-\hat f(\pmb x_0;S))^2\right]+\mathbb E[\hat f(\pmb x_0)-\mathbb E[\hat f(\pmb x_0;S)])^2]+\mathbb E[\epsilon^2]+\mathbb E[2\epsilon(f(\pmb x_0)-\mathbb E[\hat f(\pmb x_0;S)])]\\
=&\text{Var}[\hat f(\pmb x_0;S)]+(\text{Bias}[\hat f(\pmb x_0;S)])^2+\sigma^2
\end{align}</script></li>
</ul>
</li>
<li><p>The <em>variance</em> represents how much the trained model move about its mean.</p>
</li>
<li><p>The <em>bias</em> represents the difference between the expected prediction of our model and the true value.<br><img src="/pic/e4f39df5e0cf02295ed0adf94236c82.png" alt=""><br><img src="/pic/4010e1c4642202ac977ba16ffc9a0ee.png" alt=""></p>
</li>
<li><p>The ideal case is that we reduce both the bias and variance to reduce the total error.</p>
</li>
<li>However, there is a trade-off between the bias and variance.<ul>
<li>High bias: more features, more complex models, better optimization, boosting, …</li>
<li>High variance: more data, regularization, less features, less complex models, bagging, …<br><img src="/pic/b0f3730daa3d994524a12413137e863.png" alt=""></li>
</ul>
</li>
</ul>
<h3 id="Model-selection"><a href="#Model-selection" class="headerlink" title="Model selection"></a>Model selection</h3><p>Fit the data (blue dots) using polynomials with different degrees of freedom.</p>
<ul>
<li>How to select the appropriate model with good fit?<br><img src="/pic/1a58a26150be5c680563a0eb25d7b8a.png" alt=""></li>
</ul>
<h4 id="Principle-of-Occam’s-razor-奥卡姆剃刀"><a href="#Principle-of-Occam’s-razor-奥卡姆剃刀" class="headerlink" title="Principle of Occam’s razor (奥卡姆剃刀)"></a>Principle of Occam’s razor (奥卡姆剃刀)</h4><ul>
<li>Select the hypothesis with the fewest assumptions among all competing hypotheses that explain known observations equally well.<br><img src="/pic/30dc5adbbd599316a0cb17da5726c8c.png" alt=""></li>
</ul>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><ul>
<li>Regularization(正则化) refers to the process of adding additional terms to our loss function, often to introduce a <u>preference for simpler models</u></li>
<li>It aims to reduce the generalization error but not its training error.</li>
<li>Recall the training error $\hat R_S(f)=\cfrac{1}{m}\displaystyle\sum_{i=1}^mL(f(\pmb x_i),y_i)$ , we search for the hypothesis that leads to the smallest training error <script type="math/tex">\min_{f\in F}\hat R_S(f)</script></li>
<li><p>Minimization problem with regularized loss function <script type="math/tex">\min_{f\in F}\hat R_S(f)+\lambda E(f)</script><br><img src="/pic/f4eec853b3f60729c5dd34dc7b4acc9.png" alt=""></p>
</li>
<li><p>$L_2$ regularization <script type="math/tex">E(f=\pmb w^T\pmb x)=||\pmb w||_2^2=\sum_{i=1}^nw_i^2</script><br><img src="/pic/0ad30f160f2e7ecab62f7354a1c4616.png" alt=""></p>
</li>
<li><p>$L_1$ regularization <script type="math/tex">E(f=\pmb w^T\pmb x)=||\pmb w||_1=\sum_{i=1}^n|w_i|</script><br><img src="/pic/a28e0bebc2515f58f224263e5cbedf5.png" alt=""></p>
</li>
</ul>
<h4 id="Hyperparameter-超参数"><a href="#Hyperparameter-超参数" class="headerlink" title="Hyperparameter(超参数)"></a>Hyperparameter(超参数)</h4><ul>
<li>The parameter is determined before the learning process<ul>
<li>Example: the degree of the polynomial, the regularization coefficient.</li>
</ul>
</li>
<li>It can not be adopted by the learning algorithm from the training data</li>
<li>How to find the optimal hyperparameter?<ul>
<li>Set it to different values</li>
<li>Evaluate the corresponding models</li>
<li>Choose the one that results in the best performance</li>
</ul>
</li>
</ul>
<h4 id="Validation-strategy"><a href="#Validation-strategy" class="headerlink" title="Validation strategy"></a>Validation strategy</h4><h5 id="Hold-out-validation"><a href="#Hold-out-validation" class="headerlink" title="Hold-out validation"></a>Hold-out validation</h5><ul>
<li>Split the training set into two parts: a training set and a validation set<br><img src="/pic/dd55e1c530692fa6810d9b314c15f84.png" alt=""></li>
</ul>
<h5 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross validation"></a>Cross validation</h5><ul>
<li>K-fold cross validation: divide the training set into k equal size subsets<br><img src="/pic/6c170feb2750e49af1c23e63965b608.png" alt=""></li>
</ul>
<h5 id="Leave-one-out-cross-validation"><a href="#Leave-one-out-cross-validation" class="headerlink" title="Leave-one-out cross validation"></a>Leave-one-out cross validation</h5><ul>
<li><p>K-fold cross validation where $k=m$<br><img src="/pic/132f7b5bbcbb9caecf14d6a7f6bf2f0.png" alt=""></p>
</li>
<li><p>Use one observation as the validation set</p>
</li>
<li>Each sample is used once for validation</li>
<li>It could be vary computationally intensive!</li>
</ul>
<h4 id="Which-validation-strategy-to-use"><a href="#Which-validation-strategy-to-use" class="headerlink" title="Which validation strategy to use?"></a>Which validation strategy to use?</h4><ul>
<li>Large data set<ul>
<li>Hold-out validation is simpler testing and computationally cheaper.</li>
<li>Hold-out strategy is suitable when the amount of data is huge.</li>
</ul>
</li>
<li>Small data set<ul>
<li>Cross-validation is useful when the dataset is small.</li>
<li>10-fold cross validation is common, but smaller values of k are often used when learning takes a lot of time</li>
</ul>
</li>
</ul>
<h2 id="Lecture4-LinearModels"><a href="#Lecture4-LinearModels" class="headerlink" title="Lecture4_LinearModels"></a>Lecture4_LinearModels</h2><h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><h4 id="Motivating-example"><a href="#Motivating-example" class="headerlink" title="Motivating example"></a>Motivating example</h4><ul>
<li>The Advertising data set consists of the <em>sales</em> of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: <em>TV, radio and newspaper</em>.<br><img src="8fce1beb900d9682237c079542b44f5.png" alt=""></li>
</ul>
<h4 id="Simple-linear-regression"><a href="#Simple-linear-regression" class="headerlink" title="Simple linear regression"></a>Simple linear regression</h4><ul>
<li>Assuming approximately a linear relationship between $X$ and $Y$ <script type="math/tex">y\approx b+w\cdot x</script></li>
<li>Predicting $\hat y$ based on $x$ <script type="math/tex">\hat y=\hat b+\hat w\cdot x</script><br><img src="/pic/5192779dc145eba035b997cf1a76fcc.png" alt=""></li>
</ul>
<h5 id="Estimating-the-parameters"><a href="#Estimating-the-parameters" class="headerlink" title="Estimating the parameters"></a>Estimating the parameters</h5><ul>
<li><p>Mean square error (MSE) <script type="math/tex">\begin{align}
MSE&=\cfrac{1}{m}((y_1-b-w\cdot x_1)^2+(y_2-b-w\cdot x_2)^2+\cdots+(y_m-b-w\cdot x_m)^2)\\
&=\cfrac{1}{m}\sum_{i=1}^m(y_i-b-w\cdot x_i)^2
\end{align}</script><br><img src="/pic/876fcd4c99289d7d14c2494c352de58.png" alt=""></p>
</li>
<li><p>FInd the linear function with the smallest MSE <script type="math/tex">(\hat w,\hat b)=\text{arg}\min_{w,b}=\cfrac{1}{m}\sum_{i=1}^m(y_i-b-w\cdot x_i)^2</script></p>
</li>
<li>FOCs: <script type="math/tex">\begin{align}
\cfrac{\partial L(w,b)}{\partial w}&=\cfrac{1}{m}\sum_{i=1}^m2(y_i-b-w\cdot x_i)(-x_i)=0\\
\cfrac{\partial L(w,b)}{\partial b}&=\cfrac{1}{m}\sum_{i=1}^m2(y_i-b-w\cdot x_i)(-1)=0
\end{align}</script><script type="math/tex; mode=display">\begin{align}
\hat w&=\cfrac{\displaystyle\sum_{i=1}^m(x_i-\bar x)(y_i-\bar y)}{\displaystyle\sum_{i=1}^m(x_i-\bar x)^2},\ \ \ \ \hat b=\bar y-\hat w\bar x,\\
\bar x&=\cfrac{1}{m}\sum_{i=1}^mx_i, \ \ \ \ \bar y=\cfrac{1}{m}\sum_{i=1}^my_i
\end{align}</script></li>
</ul>
<p><img src="/pic/4cda0b999d6f3752153a5f48f5b45fa.png" alt=""></p>
<h4 id="Multivariate-linear-regression"><a href="#Multivariate-linear-regression" class="headerlink" title="Multivariate linear regression"></a>Multivariate linear regression</h4><ul>
<li>We ignore the other two factors when estimating the coefficients</li>
<li>How to make predictions given the levels of the three advertising media?<script type="math/tex">sales=b+w_1\cdot TV+w_2\cdot radio+w_3\cdot newspaper+\epsilon</script><br>In general, with $n$-dimension features, <script type="math/tex">\begin{align}
Y&\approx b+w_1\cdot x_1+w_2\cdot x_2+\cdots+w_n\cdot x_n\\
\hat y &=\hat b+\hat w_1\cdot x_1+\hat w_2\cdot x_2+\cdots+\hat w_n\cdot x_n
\end{align}</script><br>Choose $b,w_1,\cdots,w_n$ to minimize the following <script type="math/tex">MSE=\cfrac{1}{m}\sum_{i=1}^m(y_i-\hat y_i)^2=\cfrac{1}{m}\sum_{i=1}^m(y_i-b-\hat w_1\cdot x_{i,1}-\hat w_2\cdot x_{i,2}-\cdots-\hat w_n\cdot x_{i,n})^2</script><script type="math/tex; mode=display">\begin{align}
\pmb X&=\begin{bmatrix}
1 &x_{1,1} &\cdots &x_{i,n}\\
\vdots &\vdots &\ddots &\vdots\\
1 &x_{m,1} &\cdots &x_{m,n}
\end{bmatrix}\ \ \ \ m\times (n+1) \text{ matrix with each row a feature vector}\\
\pmb y&=[y_1,y_2,\cdots,y_m]^T\ \ \ \ m\times 1\text{ vector of outputs in the training set}\\
\pmb\beta &=[b,w_1,w_2,\cdots,w_n]^T\ \ \ \ (n+1)\times 1\text{ vector of parameters}
\end{align}</script></li>
</ul>
<p>Choose $\beta$ to minimize the following objective <script type="math/tex">MSE=\cfrac{1}{m}(\pmb y-\pmb X\pmb \beta)^T(\pmb y-\pmb X\pmb\beta)</script></p>
<script type="math/tex; mode=display">\min_{\pmb\beta} L(\pmb\beta)=\cfrac{1}{2}(\pmb y-\pmb X\pmb\beta)^T(\pmb y-\pmb X\pmb\beta)</script><script type="math/tex; mode=display">\cfrac{\partial L(\pmb\beta)}{\partial \pmb\beta}=-\pmb X^T(\pmb y-\pmb X\pmb\beta)</script><p>To minimize $L(\pmb\beta)$ , we set its derivatives to zero and obtain the normal equations <script type="math/tex">\cfrac{\partial L(\pmb\beta)}{\partial \pmb\beta}=-\pmb X^T(\pmb y-\pmb X\pmb\beta)=0\Rightarrow \pmb X^T\pmb X\hat{\pmb\beta}=\pmb X^T\pmb y</script><br>Suppose $\pmb X^T\pmb X$ is invertible, <script type="math/tex">\begin{align}
\hat{\pmb\beta}&=(\pmb X^T\pmb X)^{-1}\pmb X^T\pmb y\\
\hat y_i&=\pmb x_i^T\hat{\pmb\beta}=\pmb x_i^T(\pmb X^T\pmb X)^{-1}\pmb X^T\pmb y\\
\hat {\pmb y}&=\pmb X\hat{\pmb\beta}=\pmb X(\pmb X^T\pmb X)^{-1}\pmb X^T\pmb y
\end{align}</script></p>
<h5 id="Ridge-regression"><a href="#Ridge-regression" class="headerlink" title="Ridge regression"></a>Ridge regression</h5><ul>
<li>What if $\pmb X^T\pmb X$ is not invertible? <script type="math/tex">\min_{\pmb\beta}L(\pmb\beta)=(\pmb y-\pmb X\pmb\beta)^T(\pmb y-\pmb X\pmb\beta)+\lambda||\pmb\beta||_2^2</script> <script type="math/tex">\cfrac{\partial L(\pmb\beta)}{\partial \pmb\beta}=-2\pmb X^T(\pmb y-\pmb X\pmb\beta)+2\lambda\pmb\beta</script></li>
<li>To minimize $L(\pmb\beta)$ , we set its derivatives to zero: <script type="math/tex">(\lambda\pmb I+\pmb X^T\pmb X)\pmb\beta=\pmb X^T\pmb y</script></li>
<li>The estimator from Ridge regression is computed as: <script type="math/tex">\hat{\pmb\beta}_\text{ridge}=(\lambda\pmb I+\pmb X^T\pmb X)^{-1}\pmb X^T\pmb y\Leftarrow (\lambda\pmb I+\pmb X^T\pmb X)\text{ is invertible for }\lambda\textgreater 0</script></li>
</ul>
<h4 id="Regularized-linear-regression"><a href="#Regularized-linear-regression" class="headerlink" title="Regularized linear regression"></a>Regularized linear regression</h4><script type="math/tex; mode=display">(\pmb y-\pmb X\pmb\beta)^T(\pmb y-\pmb X\pmb\beta)+\lambda E(\pmb\beta^T\pmb x)</script><p>$\lambda$ : Regularization coefficient;<br>$E(\pmb\beta^T\pmb x)$ : Regularization term</p>
<ul>
<li>L2 regularization (Ridge regression)<ul>
<li>$E(f=\pmb\beta^T\pmb x)=||\pmb\beta||_2^2$</li>
<li>$\lambda\rightarrow 0, \hat{\pmb\beta}_\text{ridge}\rightarrow\hat{\pmb\beta}_{OLS}$</li>
<li>$\lambda\rightarrow\infty,\hat{\pmb\beta}_\text{ridge}\rightarrow 0$</li>
</ul>
</li>
<li>L1 regularization (Lasso regression)<ul>
<li>$E(f=\pmb\beta^T\pmb x)=||\pmb\beta||_1$</li>
<li>Some of coefficient estimates tend to zeros</li>
<li>Variable selection (sparse models)</li>
</ul>
</li>
</ul>
<h5 id="Regularization-1"><a href="#Regularization-1" class="headerlink" title="Regularization"></a>Regularization</h5><p>Ridge vs. Lasso: which one is better? </p>
<ul>
<li>Case 1: A relatively small number of features have substantial coefficients and the remaining features have parameters that are very small or equal to zero.<ul>
<li>Lasso regression</li>
</ul>
</li>
<li>Case 2: The response is a function of many features, all with parameters of roughly equal size.<ul>
<li>Ridge regression</li>
</ul>
</li>
<li>Neither ridge nor lasso regression would universally dominate the other</li>
<li>Which approach to use? How to determine $\lambda$ ?<ul>
<li>Cross validation! </li>
</ul>
</li>
</ul>
<h4 id="Linear-regression-Probabilistic-View"><a href="#Linear-regression-Probabilistic-View" class="headerlink" title="Linear regression: Probabilistic View"></a>Linear regression: Probabilistic View</h4><h5 id="Recap-Probability-amp-Statistics"><a href="#Recap-Probability-amp-Statistics" class="headerlink" title="Recap: Probability &amp; Statistics"></a>Recap: Probability &amp; Statistics</h5><h6 id="The-multivariate-Gaussian-distribution"><a href="#The-multivariate-Gaussian-distribution" class="headerlink" title="The multivariate Gaussian distribution"></a>The multivariate Gaussian distribution</h6><p>A random vector $X$ is said to have a multivariate normal (Gaussian) distribution with mean $\mu\in\mathbb R^n$ and covariance matrix $\Sigma \in S_{++}^n$ </p>
<script type="math/tex; mode=display">f_{X_1,X_2,\cdots,X_n}(x_1,x_2,\cdots,x_n)=\cfrac{1}{(2\pi)^\frac{n}{2}|\Sigma|^\frac{1}{2}}e^{-\frac{1}{2}(\pmb x-\mu)^T\Sigma^{-1}(\pmb x-\mu)}$$ We write this as $X\sim\mathcal N(\mu,\Sigma)$. 

![](/pic/bfd5b6d0bc87a6d76c69c391e921870.png)

When $n=1$ , it is a normal (Gaussian) distribution $\mathcal N(\mu_1,\Sigma_{11})$ .
$$f_{X_1}(x_1)=\cfrac{1}{(2\pi)^\frac{1}{2}|\Sigma_{11}|^\frac{1}{2}}e^{-\frac{1}{2}(x_1-\mu_1)^T\Sigma_{11}^{-1}(x_1-\mu_1)}</script><h5 id="Linear-regression-Probabilistic-View-1"><a href="#Linear-regression-Probabilistic-View-1" class="headerlink" title="Linear regression: Probabilistic View"></a>Linear regression: Probabilistic View</h5><p>Assume the response $Y$ is given by a deterministic function and an additive Gaussian noise. <script type="math/tex">y=\pmb\beta^T\pmb x+\epsilon,\text{ where }\epsilon\sim N(0,\sigma^2)</script></p>
<ul>
<li>The linear regression estimator is the maximum likelihood estimator of the data.</li>
<li>The likelihood function <script type="math/tex">P(\pmb y|\pmb X;\pmb\beta)=P(y_1|\pmb x_1;\pmb\beta)P(y_2|\pmb x_2;\pmb\beta)\cdots P(y_m|\pmb x_m;\pmb\beta)=\prod_{i=1}^mP(y_i|\pmb x_i;\pmb\beta)</script></li>
<li>The log-likelihood function <script type="math/tex">\log P(\pmb y|\pmb X;\pmb\beta)=\log\prod_{i=1}^m P(y_i|\pmb x_i;\pmb\beta)=\sum_{i=1}^m\log P(y_i|\pmb x_i;\pmb\beta)</script></li>
<li>Given dataset $(\pmb X,\pmb y)$ , find $\pmb\beta$ that can maximize the log-likelihood of $\pmb y$ . <script type="math/tex">\hat{\pmb\beta}=\text{arg}\max_{\pmb\beta}\log P(\pmb y|\pmb X;\pmb\beta)=\text{arg}\max_{\pmb\beta}\sum_{i=1}^m\log P(y_i|\pmb x_i;\pmb\beta)</script></li>
</ul>
<script type="math/tex; mode=display">\begin{align}
\hat{\pmb\beta}&=\text{arg}\max_{\pmb\beta}\log P(\pmb y|\pmb X;\pmb\beta)\\
&=\text{arg}\max_{\pmb\beta}\sum_{i=1}^m\log P(y_i|\pmb x_i;\pmb\beta)\\
&=\text{arg}\max_{\pmb\beta}\sum_{i=1}^m\log\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}}
\Leftarrow P(y_i|\pmb x_i;\pmb\beta)=\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}}\\
&=\text{arg}\max_{\pmb\beta}-\sum_{i=1}^m\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}\\
&=\text{arg}\min_{\pmb\beta}\sum_{i=1}^m\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}\\
&=\text{arg}\min_{\pmb\beta}\sum_{i=1}^m(y_i-\pmb\beta^T\pmb x_i)^2
\Leftrightarrow MSE=\frac{1}{m}(\pmb y-\pmb X\pmb\beta)^T(\pmb  y-\pmb X\pmb\beta)
\end{align}</script><h5 id="Ridge-regression-Probabilistic-View"><a href="#Ridge-regression-Probabilistic-View" class="headerlink" title="Ridge regression: Probabilistic View"></a>Ridge regression: Probabilistic View</h5><ul>
<li>Assume the response $Y$ is given by a deterministic function and an additive Gaussian noise. <script type="math/tex">y=\pmb\beta^T\pmb x+\epsilon,\text{ where }\epsilon\sim N(0,\sigma^2)</script></li>
<li>Suppose $\pmb\beta$ has the prior $p(\pmb\beta)=\mathcal N(0,\tau^2\pmb I)$</li>
<li>Ridge regression estimator is a MAP estimator with Gaussian prior <script type="math/tex">\begin{align}
\hat{\pmb\beta}_{MAP}&=\text{arg}\max\sum_{i=1}^m\log p(y_i|\pmb x_i;\pmb\beta)+\log p(\pmb\beta)\\
P(y_i|\pmb x_i;\pmb\beta)=\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}}
\Rightarrow &=\text{arg}\max\sum_{i=1}^m-\cfrac{(y_i-\pmb\beta^T\pmb x_i)^2}{2\sigma^2}-\cfrac{1}{2\tau^2}||\pmb\beta||_2^2
\Leftarrow p(\pmb\beta)=\cfrac{e^{-\cfrac{1}{2\tau^2}\pmb\beta^T\pmb\beta}}{\sqrt{(2\pi\tau^2)^n}}\\
&=\text{arg}\max_{\pmb\beta}\sum_{i=1}^m-(y_i-\pmb\beta^T\pmb x_i)^2-\lambda||\pmb\beta||_2^2
\Leftarrow \lambda=\cfrac{\sigma^2}{\tau^2}\\
\text{Ridge regression}\Leftrightarrow &=\text{arg}\min_{\pmb\beta}\sum_{i=1}^m(y_i-\pmb\beta^T\pmb x_i)^2+\lambda||\pmb\beta||_2^2
\end{align}</script><h3 id="Classification-1"><a href="#Classification-1" class="headerlink" title="Classification"></a>Classification</h3></li>
<li>In many situation, the response variable is qualitative. </li>
<li>Classification is a process of predicting qualitative responses.</li>
<li>Examples<ul>
<li>Medical diagnosis: predict whether a patient is sick or healthy</li>
<li>Spam detection: predict whether an email is spam or not</li>
<li>Credit card fraud: predict whether a given credit card transaction is fraud or not</li>
<li>Marketing: predict whether a given user will buy a product or not</li>
</ul>
</li>
<li>Classifiers<ul>
<li>Logistic regression</li>
<li>Support vector machine (SVM)</li>
<li>Naïve Bayesian</li>
<li>…</li>
</ul>
</li>
</ul>
<h4 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h4><h5 id="Why-not-linear-regression"><a href="#Why-not-linear-regression" class="headerlink" title="Why not linear regression?"></a>Why not linear regression?</h5><p>Can we use linear regression to predict the probability of default? <script type="math/tex">p(\pmb x)=\pmb w^T\pmb x+b</script></p>
<ul>
<li>The probability of default could be negative when balances close to 0, and could be bigger than 1 when the balances are large. </li>
<li>We must model $p(\pmb x)$ as a function that gives output between 0 and 1.<br><img src="/pic/8238a887080cb52105e2da7c8321fab.png" alt=""></li>
</ul>
<h5 id="Logistic-function"><a href="#Logistic-function" class="headerlink" title="Logistic function"></a>Logistic function</h5><script type="math/tex; mode=display">\sigma(z)=\cfrac{1}{1+e^{-z}}</script><ul>
<li>bounded in (0,1)</li>
<li>$\sigma(z)\rightarrow 1$ when $z \rightarrow \infty$</li>
<li>$\sigma(z)\rightarrow 0$ when $z\rightarrow -\infty$</li>
<li>$\sigma^\prime(z)=\sigma(z)(1-\sigma(z))$</li>
</ul>
<h6 id="Binary-classification-using-logistic-function"><a href="#Binary-classification-using-logistic-function" class="headerlink" title="Binary classification using logistic function"></a>Binary classification using logistic function</h6><script type="math/tex; mode=display">\begin{align}
p(y=1|\pmb x)=\sigma(\pmb w^T\pmb x+b)=\cfrac{1}{1+e^{-(\pmb w^T\pmb x+b)}}\\
p(y=0|\pmb x)=1-\sigma(\pmb w^T\pmb x+b)=\cfrac{e^{-(\pmb w^T\pmb x+b)}}{1+e^{-(\pmb w^T\pmb x+b)}}
\end{align}</script><h6 id="Interpretation-of-logistic-regression"><a href="#Interpretation-of-logistic-regression" class="headerlink" title="Interpretation of logistic regression"></a>Interpretation of logistic regression</h6><ul>
<li>Let $p(\pmb x)=\cfrac{1}{1+e^{-(\pmb w^T\pmb x+b)}}$, the odds(几率) is given by, <script type="math/tex">odds=\cfrac{p(\pmb x)}{1-p(\pmb x)}=e^{\pmb w^T\pmb x+b}</script></li>
<li>If we take the logarithm on both sides, <script type="math/tex">\log\cfrac{p(\pmb x)}{1-p(\pmb x)}=\pmb w^T\pmb x+b</script></li>
<li>The logit of an event’s odds is predicted by a linear model.</li>
<li>One-unit increase in $x_i$ changes the log-odds by $w_i$ holding all other features fixed.</li>
</ul>
<h4 id="Training-the-logistic-function"><a href="#Training-the-logistic-function" class="headerlink" title="Training the logistic function"></a>Training the logistic function</h4><ul>
<li>We use the maximum likelihood estimation (MLE) to estimate $b$ and $\pmb w$</li>
<li><p>Let $\pmb \beta=[b;\pmb w]$ , the likelihood function is given by <script type="math/tex">\begin{align}
\mathcal L(\pmb \beta) &= \prod_{i:y_i=1}p(y_i=1|\pmb x_1;\pmb\beta)\prod_{i^\prime:y_{i^\prime}=0}p(y_{i^\prime}=0|\pmb x_{i^\prime};\pmb \beta)\\
&=\prod_{i=1}^mp(y_i=1|\pmb x_i;\pmb\beta)^{y_i}p(y_1=0|\pmb x_i;\pmb\beta)^{1-y_i}
\end{align}</script></p>
<script type="math/tex; mode=display">\begin{align}
p(y=1|\pmb x;\pmb\beta)=\sigma(\pmb\beta^T\pmb x)=\cfrac{1}{1+e^{-\pmb \beta^T\pmb x}}\\
p(y=0|\pmb x;\pmb\beta)=1-\sigma(\pmb\beta^T\pmb x)=\cfrac{e^{-\pmb\beta^T\pmb x}}{1+e^{-\pmb\beta^T\pmb x}}
\end{align}</script></li>
<li><p>We can equivalently minimize the <em>negative</em> log-likelihood function <script type="math/tex">\begin{align}
\mathcal{l(\pmb\beta)}&=-\log\mathcal{L(\pmb\beta)}\\
&=\sum_{i=1}^m(-y_i\log p(y_i=1|\pmb x_i;\pmb\beta)-(1-y_i)\log p(y_i=0|\pmb x_i;\pmb\beta))\\
&=\sum_{i=1}^m(-y_i\pmb\beta^T\pmb x_i+\log(1+e^{\pmb\beta^T\pmb x_i}))
\end{align}</script></p>
</li>
<li><p>Maximum Likelihood Estimation (MLE) <script type="math/tex">\begin{align}
\hat{\pmb\beta}&=\text{arg}\max_{\pmb\beta}\mathcal L(\pmb\beta)\\
&=\text{arg}\min_{\pmb\beta}\mathcal l(\pmb\beta)\\
&=\text{arg}\min_{\pmb\beta}\sum_{i=1}^m(-y_i\pmb\beta^T\pmb x_i+\log(1+e^{\pmb\beta^T\pmb x_i}))
\end{align}</script></p>
</li>
<li>There is no closed-form solution for the above optimization problem</li>
<li>Fortunately, $\mathcal l(\pmb\beta)$ is a convex function, we can apply the <em>gradient descent method</em> to find the optimal solution.</li>
</ul>
<h5 id="Gradient-descent-method-梯度下降法"><a href="#Gradient-descent-method-梯度下降法" class="headerlink" title="Gradient descent method (梯度下降法)"></a>Gradient descent method (梯度下降法)</h5><h2 id="Lecture5-FeatureEngineering"><a href="#Lecture5-FeatureEngineering" class="headerlink" title="Lecture5_FeatureEngineering"></a>Lecture5_FeatureEngineering</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h3 id="Data-understanding"><a href="#Data-understanding" class="headerlink" title="Data understanding"></a>Data understanding</h3><h4 id="Structured-versus-unstructured-data"><a href="#Structured-versus-unstructured-data" class="headerlink" title="Structured versus unstructured data"></a>Structured versus unstructured data</h4><h4 id="Quantitative-versus-qualitative-data"><a href="#Quantitative-versus-qualitative-data" class="headerlink" title="Quantitative versus qualitative data"></a>Quantitative versus qualitative data</h4><h4 id="Exploratory-data-analysis-descriptive-statistics-and-data-visualizations"><a href="#Exploratory-data-analysis-descriptive-statistics-and-data-visualizations" class="headerlink" title="Exploratory data analysis: descriptive statistics and data visualizations"></a>Exploratory data analysis: descriptive statistics and data visualizations</h4><h3 id="Data-processing"><a href="#Data-processing" class="headerlink" title="Data processing"></a>Data processing</h3><h4 id="Missing-values"><a href="#Missing-values" class="headerlink" title="Missing values"></a>Missing values</h4><h4 id="Class-imbanlance"><a href="#Class-imbanlance" class="headerlink" title="Class imbanlance"></a>Class imbanlance</h4><h4 id="Feature-scaling-amp-discretization"><a href="#Feature-scaling-amp-discretization" class="headerlink" title="Feature scaling &amp; discretization"></a>Feature scaling &amp; discretization</h4><h4 id="Feature-encoding-amp-text-data-representation"><a href="#Feature-encoding-amp-text-data-representation" class="headerlink" title="Feature encoding &amp; text data representation"></a>Feature encoding &amp; text data representation</h4><h2 id="Lecture7-NeuralNetworks"><a href="#Lecture7-NeuralNetworks" class="headerlink" title="Lecture7_NeuralNetworks"></a>Lecture7_NeuralNetworks</h2><h3 id="Overview-of-neural-networks"><a href="#Overview-of-neural-networks" class="headerlink" title="Overview of neural networks"></a>Overview of neural networks</h3><h4 id="Structure-of-neurons-in-human-brain"><a href="#Structure-of-neurons-in-human-brain" class="headerlink" title="Structure of neurons in human brain"></a>Structure of neurons in human brain</h4><ul>
<li>The human brain can be described as a biological neural network—an interconnected web of neurons transmitting elaborate patterns of electrical signals.</li>
<li>Dendrites receive input signals and, based on those inputs, fire an output signal via an axon.</li>
</ul>
<p><img src="/pic/d6ac10786053cf27bd83808148d4005.png" alt=""></p>
<ul>
<li>Dendrite (树突): It receives signals from other neurons</li>
<li>Cell body: It sums all the incoming signals to generate input</li>
<li>Axon (轴突): When the sum reaches a threshold value, neuron fires and the signal travels down the axon to the other neurons</li>
</ul>
<h4 id="Artificial-neurons"><a href="#Artificial-neurons" class="headerlink" title="Artificial neurons"></a>Artificial neurons</h4><ul>
<li>Artificial neurons (人工神经元) are the basic computing units of information processing in an artificial neural network (人工神经网络).</li>
<li>The dendrites carry the signals (inputs $\pmb x$) to the cell body where they all get summed (weighted sum of inputs)</li>
<li>Synaptic strength (突触强度) controls the strength of the influence and direction (weights $\pmb w$)</li>
<li>Cell body reaches certain threshold and sends a spike to the next axon (activation function $f$)</li>
<li>Axons pass signals (outputs $\pmb y$) to other neurons (nodes)</li>
</ul>
<h4 id="Artificial-Neural-Networks-ANN"><a href="#Artificial-Neural-Networks-ANN" class="headerlink" title="Artificial Neural Networks (ANN)"></a>Artificial Neural Networks (ANN)</h4><ul>
<li>Artificial neural networks (ANN) are computing systems inspired by the biological neural networks.</li>
<li>It is comprised of a network of artificial neurons (nodes).</li>
</ul>
<p><img src="/pic/99505909a8ce803cd2585640c29ea03.png" alt=""></p>
<ul>
<li>The connections between units do not form a cycle.</li>
<li><p>The information moves in only one direction.</p>
</li>
<li><p>Artificial neural networks (ANN) is best used</p>
<ul>
<li>For modeling highly nonlinear systems</li>
<li>When the model interpretability is not a key concern</li>
<li>When data is available incrementally and you wish to constantly update the model</li>
</ul>
</li>
<li>Applications<ul>
<li>Image recognition</li>
<li>Speech recognition</li>
<li>Natural language processing</li>
<li>Game AI</li>
<li>…</li>
</ul>
</li>
</ul>
<h4 id="History-of-neural-networks"><a href="#History-of-neural-networks" class="headerlink" title="History of neural networks"></a>History of neural networks</h4><ul>
<li><strong>The First wave</strong><ul>
<li>1943 McCulloch and Pitts proposed the McCulloch-Pitts neuron model (M-P神经元)</li>
<li>1958 Rosenblatt introduced the simple single layer networks now called Perceptrons (感知机).</li>
<li>1969 Minsky and Papert’s book Perceptrons demonstrated the limitation of single layer perceptron, and almost the whole field went into hibernation.</li>
</ul>
</li>
<li><strong>The Second wave</strong><ul>
<li>1986 The Back-Propagation learning algorithm (反向传播学习算法) for Multi-Layer Perceptrons was rediscovered and the whole field took off again.</li>
</ul>
</li>
<li><strong>The Third wave</strong><ul>
<li>2006 Deep (neural networks) Learning (深度学习) gains popularity</li>
<li>2012 made significant break-through in many applications<ul>
<li>AlexNet: Geoffrey Hinton, IIya Sutskever, Alex Krizhevsky</li>
<li>Google Brain: Jeff Dean, Andrew Ng</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Perceptron-model"><a href="#Perceptron-model" class="headerlink" title="Perceptron model"></a>Perceptron model</h3><ul>
<li>Rosenblatt’s single layer perceptron, 1958<ul>
<li>The simplest feedforward neural network. It does not contain any hidden layers.</li>
<li>It computes the weighted output, and uses a threshold activation function to output a quantity.</li>
</ul>
</li>
</ul>
<p><img src="/pic/31005d52795dd6bf9d6ccb279d59ceb.png" alt=""></p>
<ul>
<li><em>Activation function (激活函数)</em> <script type="math/tex">f(z)=\begin{cases}
1 &\text{if } z\geq 0\\
0 &\text{otherwise}
\end{cases}</script></li>
<li>Prediction <script type="math/tex">\hat y=f\left(\sum_iw_ix_i+b\right)</script></li>
</ul>
<h4 id="Training-the-Perceptron-model"><a href="#Training-the-Perceptron-model" class="headerlink" title="Training the Perceptron model"></a>Training the Perceptron model</h4><ul>
<li>Perceptron uses iterative algorithm to learn a correct set of weights</li>
<li>Updating rule <script type="math/tex">\begin{align}
w_i\leftarrow w_i+\Delta w_i\\
\Delta w_i=\eta(y-\hat y)x_i
\end{align}</script><ul>
<li>$\eta$ : Learning rate</li>
<li>$y$ : Target output</li>
<li>$\hat y$ : Perceptron output</li>
</ul>
</li>
<li>Rosenblatt proved the convergence of the learning algorithm, if<ul>
<li>the training data is linearly separable</li>
<li>$\eta$ is sufficiently small</li>
</ul>
</li>
</ul>
<h4 id="Representation-of-Boolean-functions-布尔函数"><a href="#Representation-of-Boolean-functions-布尔函数" class="headerlink" title="Representation of Boolean functions (布尔函数)"></a>Representation of Boolean functions (布尔函数)</h4><p><img src="/pic/f7eb0da81ed0871c9b951828e1eef00.png" alt=""></p>
<ul>
<li>AND ($x_1 \wedge x_2$ , 与)<ul>
<li>$w_1=w_2=1,b=-1.5$</li>
</ul>
</li>
<li>OR ($x_1\vee x_2$ , 或)<ul>
<li>$w_1=w_2=1,b=-0.5$</li>
</ul>
</li>
<li>NOT ($\overline{x_1}$ , 非)<ul>
<li>$w_1=-0.6,w_2=0,b=0.5$</li>
</ul>
</li>
</ul>
<h5 id="Limitation-of-Perceptron-model"><a href="#Limitation-of-Perceptron-model" class="headerlink" title="Limitation of Perceptron model"></a>Limitation of Perceptron model</h5><p>XOR ($\overline{x_1}\wedge x_2+\overline{x_2}\vee x_1$ , 异或)<br>| Input | |  Output |<br>|—-|—-|—-|<br>| $x_1$ | $x_2$ | $y$ |<br>| 0 | 0 | 0 |<br>| 0 | 1 | 1 |<br>| 1 | 0 | 1 |<br>| 1 | 1 | 0 |</p>
<p><img src="/pic/44f6ee7c2003c9bfdc6952ad6a82bb8.png" alt=""></p>
<ul>
<li>XOR is not linearly separable</li>
<li>Some elementary computations such as XOR cannot be represented by Rosenblatt’s single layer perceptron</li>
</ul>
<h5 id="Solution-Add-a-hidden-layer"><a href="#Solution-Add-a-hidden-layer" class="headerlink" title="Solution - Add a hidden layer"></a>Solution - Add a hidden layer</h5><p><img src="/pic/7932e2084b642ada22ad4a78f912adf.png" alt=""></p>
<h3 id="Multi-layer-feedforward-neural-networks"><a href="#Multi-layer-feedforward-neural-networks" class="headerlink" title="Multi-layer feedforward neural networks"></a>Multi-layer feedforward neural networks</h3><h4 id="Back-Propagation-algorithm"><a href="#Back-Propagation-algorithm" class="headerlink" title="Back-Propagation algorithm"></a>Back-Propagation algorithm</h4><h3 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h3><h3 id="Issues-of-training-neural-networks"><a href="#Issues-of-training-neural-networks" class="headerlink" title="Issues of training neural networks"></a>Issues of training neural networks</h3>
      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://hiryan23.github.io/2023/02/17/Machine%20Learning/" title="机器学习-课程笔记" target="_blank" rel="external">http://hiryan23.github.io/2023/02/17/Machine Learning/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="" target="_blank"><span class="text-dark">Ruiyang He</span><small class="ml-1x">上海交通大学 本科生</small></a></h3>
        <div>生活不止眼前的苟且。</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
    
      <ul class="pager pull-left">
        
          <li class="prev">
            <a href="/2023/02/19/%E5%AF%B9chatgpt%E7%9A%84%E6%AC%A2%E5%91%BC%E6%98%AF%E4%BA%BA%E5%AF%B9%E5%BC%82%E5%8C%96%E7%9A%84%E5%8F%8D%E6%8A%97%E2%80%94%E2%80%94%E8%AE%BAai%E4%BB%A3%E5%86%99%E8%AE%BA%E6%96%87%E7%9A%84%E8%A7%A3%E6%94%BE%E6%80%A7%E4%B8%8E%E5%BC%80%E6%94%BE%E6%80%A7/" title="对chatgpt的欢呼是人对异化的反抗——论ai代写论文的解放性与开放性"><i
                class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
          </li>
          
            
              <li class="next">
                <a href="/2023/02/17/%E5%8D%9A%E5%BC%88%E8%AE%BA/" title="博弈论-课程笔记"><span>
                    下一篇&nbsp;&nbsp;
                  </span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
              </li>
              
                
                  <li class="toggle-toc">
                    <a class="toggle-btn " data-toggle="collapse"
                      href="#collapseToc" aria-expanded="false" title="文章目录" role="button">
                      <span>[&nbsp;</span><span>
                        文章目录
                      </span>
                      <i class="text-collapsed icon icon-anchor"></i>
                      <i class="text-in icon icon-close"></i>
                      <span>]</span>
                    </a>
                  </li>
                  
      </ul>
      
        

            <div class="bar-right">
              
                <div class="share-component" data-sites="qq,wechat"
                  data-mobile-sites="qq,wechat"></div>
                
            </div>
  </div>
</nav>
  


</main>

  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
    
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/hiryan23" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

        <div class="copyright">
            
                &copy; 2023
                    Ruiyang He
                        

                            <div class="publishby">
                                <span id="busuanzi_container_site_pv">
                                    <!--点击<span id="busuanzi_value_site_pv" style="font-family:Courier"></span>次，
                                    -->
                                    访客<span id="busuanzi_value_site_uv" style="font-family:Courier"></span>人
                                </span>
                            </div>

                            <!--
        <div class="publishby">
            Theme by
            <a href="https://github.com/cofess" target="_blank"> cofess </a>
            base on
            <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
        -->
        </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
   window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>

   
<script src="/js/application.js"></script>

      
    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>



         
                  
                           
                              
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



                                 
                                    
                                       
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'kuf03HfUPLpIZjgOwQwJ4d7f-9Nh9j0Va',
    appKey: 'dIdmyF1QYnqtOiB5b9grGYyf',
    placeholder: '不怀疑不能见真理',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script>

     

                                          
                                             

                                                



                                                   <div id="go-top"></div>
                                                   <style type="text/css">
                                                      #go-top {
                                                         width: 40px;
                                                         height: 36px;
                                                         background-color: #5a7db1;
                                                         position: relative;
                                                         border-radius: 2px;
                                                         position: fixed;
                                                         right: 10px;
                                                         bottom: 60px;
                                                         cursor: pointer;
                                                         display: none;
                                                      }

                                                      #go-top:after {
                                                         content: " ";
                                                         position: absolute;
                                                         left: 14px;
                                                         top: 14px;
                                                         border-top: 2px solid #fff;
                                                         border-right: 2px solid #fff;
                                                         width: 12px;
                                                         height: 12px;
                                                         transform: rotate(-45deg);
                                                      }

                                                      #go-top:hover {
                                                         background-color: #1A2433;
                                                      }
                                                   </style>
                                                   <script>
                                                      $(function () {
                                                         var top = $("#go-top");
                                                         $(window).scroll(function () {
                                                            ($(window).scrollTop() > 300) ? top.show(300) : top.hide(200);
                                                            $("#go-top").click(function () {
                                                               $('body,html').animate({ scrollTop: 0 });
                                                               return false();
                                                            })
                                                         });
                                                      });
                                                   </script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>